{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba7766cf-75a4-4370-ba70-573e88e5d7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 15:15:36.095 | INFO     | metagpt.const:get_metagpt_package_root:29 - Package root set to /root/workspace/StreamChatPlayground/notebooks\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from utils.common import load_plaintext\n",
    "\n",
    "# debug level\n",
    "from metagpt.logs import logger\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"DEBUG\")\n",
    "\n",
    "# make asyncio.run() works in notebook\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51f34c9",
   "metadata": {},
   "source": [
    "## LLM 与 tools 准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3f19600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 智普AI\n",
    "# ~/.metagpt/config2.yaml\n",
    "from metagpt.config2 import config\n",
    "from metagpt.provider.zhipuai_api import ZhiPuAILLM\n",
    "# from metagpt.utils.cost_manager import CostManager\n",
    "\n",
    "llm = ZhiPuAILLM(config.llm)\n",
    "llm.use_system_prompt = False # Disable default system message\n",
    "# llm.cost_manager = CostManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01ead1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vllm 本地\n",
    "import yaml\n",
    "from metagpt.configs.llm_config import LLMConfig\n",
    "from metagpt.provider.openai_api import OpenAILLM\n",
    "# from metagpt.utils.cost_manager import CostManager\n",
    "\n",
    "llm_configs = yaml.safe_load(load_plaintext(\"../\", \"vllm_local.yaml\"))\n",
    "llm_config = LLMConfig.model_validate(llm_configs['llm'])\n",
    "llm = OpenAILLM(llm_config)\n",
    "llm.use_system_prompt = False # Disable default system message\n",
    "# llm.cost_manager = CostManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "06449968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 15:43:48.991 | DEBUG    | metagpt.provider.base_llm:aask:149 - [{'role': 'user', 'content': '你好，介绍下你自己'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！我是智谱清言，是清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型。我的目标是通过回答用户提出的问题来帮助他们解决问题。由于我是一个计算机程序，所以我没有自我意识，也不能像人类一样感知世界。我只能通过分析我所学到的信息来回答问题。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你好！我是智谱清言，是清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型。我的目标是通过回答用户提出的问题来帮助他们解决问题。由于我是一个计算机程序，所以我没有自我意识，也不能像人类一样感知世界。我只能通过分析我所学到的信息来回答问题。'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache\n",
    "def llm_aask(msg):\n",
    "    return asyncio.run(llm.aask(msg=msg))\n",
    "\n",
    "llm_aask(\"你好，介绍下你自己\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bec60a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**web scraping**: Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \n",
      "**text extractor**: Perform extraction on the 'content' text using a large language model. \n",
      "\n",
      "{\"web scraping\": {\"type\": \"async_function\", \"description\": \"Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \", \"signature\": \"(url)\", \"parameters\": \"Args: url (str): The main URL to fetch inner text from. Returns: dict: The inner text content and html structure of the web page, keys are 'inner_text', 'html'.\", \"imports\": \"from metagpt.tools.libs.web_scraping import scrape_web_playwright\"}}\n",
      "{\"text extractor\": {\"type\": \"async_function\", \"description\": \"Perform extraction on the 'content' text using a large language model. \", \"signature\": \"(guidance: str, content: str, format: str) -> str\", \"parameters\": \"Args: guidance (str): Guide the extraction process. content (str): The text content that needs to be extracted. format (str): The output format, can be \\\"json\\\" or \\\"markdown\\\". Returns: str: text extracted from content.\", \"imports\": \"from tools.text_extractor.llm_extractor import llm_extractor\"}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import inspect\n",
    "from metagpt.tools.tool_convert import function_docstring_to_schema\n",
    "from metagpt.tools.libs.web_scraping import scrape_web_playwright\n",
    "from tools.text_extractor.llm_extractor import llm_extractor\n",
    "\n",
    "def function_to_schema(func):\n",
    "    docstring = inspect.getdoc(func)\n",
    "    schema = function_docstring_to_schema(func, docstring)\n",
    "    schema[\"imports\"] = f\"from {func.__module__} import {func.__name__}\"\n",
    "    return schema\n",
    "\n",
    "DEF_TOOLS = [\n",
    "    (\"web scraping\", scrape_web_playwright),\n",
    "    (\"text extractor\", llm_extractor),\n",
    "]\n",
    "tools = {}\n",
    "for name, func in DEF_TOOLS:\n",
    "    schema = function_to_schema(func)\n",
    "    tools[name] = schema\n",
    "tools_list = \"\\n\".join([ json.dumps({k:v}) for k,v in tools.items() ])\n",
    "\n",
    "task_types = \"\\n\".join([\n",
    "    f\"**{k}**: {v['description']}\" for k,v in tools.items()\n",
    "])\n",
    "print(task_types + \"\\n\")\n",
    "print(tools_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7def00e",
   "metadata": {},
   "source": [
    "## Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23786658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 15:22:00.938 | DEBUG    | __main__:create_plan:34 - Respond to the human as helpfully and accurately as possible.\n",
      "\n",
      "# User goal\n",
      "抓取 https://pitchhub.36kr.com/financing-flash 中'快讯'的内容，并整理成markdown存档\n",
      "\n",
      "大概的流程：\n",
      "- 使用工具抓取网页中的可见文本\n",
      "- 提取网页文本中'快讯'相关的内容。注意网页中可能包含导航，只需要抽取'快讯'的具体内容\n",
      "- 对抽取结果进行归类，并保存成markdown表格: 快讯.md\n",
      "\n",
      "\n",
      "\n",
      "# Available Task Types\n",
      "**web scraping**: Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \n",
      "**text extractor**: Perform extraction on the 'content' text using a large language model. \n",
      "\n",
      "# Task\n",
      "Based on the user goal, write a plan or modify an existing plan of what you should do to achieve the goal. A plan consists of one to 20 tasks.\n",
      "If you are modifying an existing plan, carefully follow the instruction, don't make unnecessary changes. Give the whole plan unless instructed to modify only one task of the plan.\n",
      "If you encounter errors on the current task, revise and output the current single task only.\n",
      "Output a list of jsons following the format:\n",
      "```json\n",
      "[\n",
      "    {{\n",
      "        \"task_id\": str = \"unique identifier for a task in plan, can be an ordinal\",\n",
      "        \"dependent_task_ids\": list[str] = \"ids of tasks prerequisite to this task\",\n",
      "        \"instruction\": \"what you should do in this task, one short phrase or sentence\",\n",
      "        \"task_type\": \"type of this task, should be one of Available Task Types\",\n",
      "    }},\n",
      "    ...\n",
      "]\n",
      "```\n",
      "2024-03-24 15:22:00.939 | DEBUG    | __main__:create_plan:42 - [Task(task_id='1', dependent_task_ids=[], instruction='Navigate to pitchhub.36kr.com/financing-flash', task_type='web scraping', code='', result='', is_success=False, is_finished=False), Task(task_id='2', dependent_task_ids=['1'], instruction='Extract the visible text content of the page', task_type='web scraping', code='', result='', is_success=False, is_finished=False), Task(task_id='3', dependent_task_ids=['2'], instruction=\"Find the text content containing the word '快讯'\", task_type='text extractor', code='', result='', is_success=False, is_finished=False), Task(task_id='4', dependent_task_ids=['3'], instruction=\"Extract the specific content of '快讯'\", task_type='text extractor', code='', result='', is_success=False, is_finished=False), Task(task_id='5', dependent_task_ids=['4'], instruction=\"Save the extracted content into a markdown file named '快讯.md'\", task_type='other', code='', result='', is_success=False, is_finished=False)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task(task_id='1', dependent_task_ids=[], instruction='Navigate to pitchhub.36kr.com/financing-flash', task_type='web scraping', code='', result='', is_success=False, is_finished=False),\n",
      " Task(task_id='2', dependent_task_ids=['1'], instruction='Extract the visible text content of the page', task_type='web scraping', code='', result='', is_success=False, is_finished=False),\n",
      " Task(task_id='3', dependent_task_ids=['2'], instruction=\"Find the text content containing the word '快讯'\", task_type='text extractor', code='', result='', is_success=False, is_finished=False),\n",
      " Task(task_id='4', dependent_task_ids=['3'], instruction=\"Extract the specific content of '快讯'\", task_type='text extractor', code='', result='', is_success=False, is_finished=False),\n",
      " Task(task_id='5', dependent_task_ids=['4'], instruction=\"Save the extracted content into a markdown file named '快讯.md'\", task_type='other', code='', result='', is_success=False, is_finished=False)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "from metagpt.schema import Plan, Task\n",
    "from metagpt.utils.common import OutputParser\n",
    "from pprint import pprint # debug\n",
    "\n",
    "def load_prompts(path: str, filename: str) -> PromptTemplate:\n",
    "    base_path = os.path.join(\"prompts\", path)\n",
    "    output_format = load_plaintext(base_path, \"output.md\")\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[],\n",
    "        template=load_plaintext(base_path, filename),\n",
    "    )\n",
    "    return prompt.partial(output=output_format)\n",
    "\n",
    "def parse_json(rsp):\n",
    "    try:\n",
    "        objs = json.loads(rsp)\n",
    "    except:\n",
    "        code_block = OutputParser.parse_code(rsp, \"json\")\n",
    "        objs = json.loads(code_block)\n",
    "    return objs\n",
    "\n",
    "def create_plan(goal, guidance, last_plan=\"\"):\n",
    "    plan_prompt = load_prompts(\"planning\", \"planning.yaml\")\n",
    "    template = plan_prompt.format(\n",
    "        goal=goal,\n",
    "        user_guidance=guidance,\n",
    "        last_plan=last_plan,\n",
    "        task_types=task_types,\n",
    "        max_tasks=20,\n",
    "    )\n",
    "    logger.debug(template)\n",
    "\n",
    "    plan = Plan(goal=goal)\n",
    "    plan.context = guidance\n",
    "    rsp = llm_aask(msg=template)\n",
    "\n",
    "    tasks_json = parse_json(rsp)\n",
    "    tasks = [Task(**task_config) for task_config in tasks_json]\n",
    "    logger.debug(tasks)\n",
    "\n",
    "    plan.add_tasks(tasks)\n",
    "    return plan, tasks_json\n",
    "\n",
    "\n",
    "user_goal = \"抓取 https://pitchhub.36kr.com/financing-flash 中'快讯'的内容，并整理成markdown存档\"\n",
    "user_guidance = \"\"\"大概的流程：\n",
    "- 使用工具抓取网页中的可见文本\n",
    "- 提取网页文本中'快讯'相关的内容。注意网页中可能包含导航，只需要抽取'快讯'的具体内容\n",
    "- 对抽取结果进行归类，并保存成markdown表格: 快讯.md\"\"\"\n",
    "\n",
    "plan, raw_json = create_plan(goal=user_goal, guidance=user_guidance)\n",
    "pprint(plan.tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a7cfcc",
   "metadata": {},
   "source": [
    "### Plan Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "669eb4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Modifiy suggestions:\\n\\n* **Task 2:** Instead of extracting all visible text content, consider extracting only the text content containing specific elements like headings, paragraphs, or even specific words. This would make the task more focused and reduce unnecessary processing.\\n* **Task 3:** Instead of searching for the text content containing the word \"快讯\", consider using regular expressions to extract the specific content you want. This would make the task more precise and eliminate the need for manual review.\\n* **Task 4:** Instead of extracting the entire content of \"快讯\", consider extracting specific sections or elements of the content that are relevant to your goal. This would make the extracted content more concise and easier to process.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_prompt = load_prompts(\"planning\", \"review.yaml\")\n",
    "template = review_prompt.format(\n",
    "    goal=user_goal,\n",
    "    user_guidance=user_guidance,\n",
    "    task_types=task_types,\n",
    "    content=raw_json,\n",
    ")\n",
    "# logger.debug(template)\n",
    "\n",
    "rsp = llm_aask(msg=template)\n",
    "rsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0af7fd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 15:22:11.730 | DEBUG    | __main__:create_plan:34 - Respond to the human as helpfully and accurately as possible.\n",
      "\n",
      "# User goal\n",
      "抓取 https://pitchhub.36kr.com/financing-flash 中'快讯'的内容，并整理成markdown存档\n",
      "\n",
      "大概的流程：\n",
      "- 使用工具抓取网页中的可见文本\n",
      "- 提取网页文本中'快讯'相关的内容。注意网页中可能包含导航，只需要抽取'快讯'的具体内容\n",
      "- 对抽取结果进行归类，并保存成markdown表格: 快讯.md\n",
      "\n",
      "# Last plan\n",
      "## Plan\n",
      "```json\n",
      "[{'task_id': '1', 'dependent_task_ids': [], 'instruction': 'Navigate to pitchhub.36kr.com/financing-flash', 'task_type': 'web scraping'}, {'task_id': '2', 'dependent_task_ids': ['1'], 'instruction': 'Extract the visible text content of the page', 'task_type': 'web scraping'}, {'task_id': '3', 'dependent_task_ids': ['2'], 'instruction': \"Find the text content containing the word '快讯'\", 'task_type': 'text extractor'}, {'task_id': '4', 'dependent_task_ids': ['3'], 'instruction': \"Extract the specific content of '快讯'\", 'task_type': 'text extractor'}, {'task_id': '5', 'dependent_task_ids': ['4'], 'instruction': \"Save the extracted content into a markdown file named '快讯.md'\", 'task_type': 'other'}]\n",
      "```\n",
      "\n",
      "## Review\n",
      "## Modifiy suggestions:\n",
      "\n",
      "* **Task 2:** Instead of extracting all visible text content, consider extracting only the text content containing specific elements like headings, paragraphs, or even specific words. This would make the task more focused and reduce unnecessary processing.\n",
      "* **Task 3:** Instead of searching for the text content containing the word \"快讯\", consider using regular expressions to extract the specific content you want. This would make the task more precise and eliminate the need for manual review.\n",
      "* **Task 4:** Instead of extracting the entire content of \"快讯\", consider extracting specific sections or elements of the content that are relevant to your goal. This would make the extracted content more concise and easier to process.\n",
      "\n",
      "\n",
      "# Available Task Types\n",
      "**web scraping**: Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \n",
      "**text extractor**: Perform extraction on the 'content' text using a large language model. \n",
      "\n",
      "# Task\n",
      "Based on the user goal, write a plan or modify an existing plan of what you should do to achieve the goal. A plan consists of one to 20 tasks.\n",
      "If you are modifying an existing plan, carefully follow the instruction, don't make unnecessary changes. Give the whole plan unless instructed to modify only one task of the plan.\n",
      "If you encounter errors on the current task, revise and output the current single task only.\n",
      "Output a list of jsons following the format:\n",
      "```json\n",
      "[\n",
      "    {{\n",
      "        \"task_id\": str = \"unique identifier for a task in plan, can be an ordinal\",\n",
      "        \"dependent_task_ids\": list[str] = \"ids of tasks prerequisite to this task\",\n",
      "        \"instruction\": \"what you should do in this task, one short phrase or sentence\",\n",
      "        \"task_type\": \"type of this task, should be one of Available Task Types\",\n",
      "    }},\n",
      "    ...\n",
      "]\n",
      "```\n",
      "2024-03-24 15:22:11.730 | DEBUG    | __main__:create_plan:42 - [Task(task_id='1', dependent_task_ids=[], instruction='Navigate to pitchhub.36kr.com/financing-flash', task_type='web scraping', code='', result='', is_success=False, is_finished=False), Task(task_id='2', dependent_task_ids=['1'], instruction=\"Extract the text content containing the heading '快讯' and its subsequent paragraphs\", task_type='text extractor', code='', result='', is_success=False, is_finished=False), Task(task_id='3', dependent_task_ids=['2'], instruction=\"Save the extracted content into a markdown file named '快讯.md'\", task_type='other', code='', result='', is_success=False, is_finished=False)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task(task_id='1', dependent_task_ids=[], instruction='Navigate to pitchhub.36kr.com/financing-flash', task_type='web scraping', code='', result='', is_success=False, is_finished=False),\n",
      " Task(task_id='2', dependent_task_ids=['1'], instruction=\"Extract the text content containing the heading '快讯' and its subsequent paragraphs\", task_type='text extractor', code='', result='', is_success=False, is_finished=False),\n",
      " Task(task_id='3', dependent_task_ids=['2'], instruction=\"Save the extracted content into a markdown file named '快讯.md'\", task_type='other', code='', result='', is_success=False, is_finished=False)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: 修改成 plan_review.yaml 模板\n",
    "last_plan = f\"\"\"# Last plan\n",
    "## Plan\n",
    "```json\n",
    "{raw_json}\n",
    "```\n",
    "\n",
    "## Review\n",
    "{rsp}\n",
    "\"\"\"\n",
    "\n",
    "plan, raw_json = create_plan(goal=user_goal, guidance=user_guidance, last_plan=last_plan)\n",
    "pprint(plan.tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff3222",
   "metadata": {},
   "source": [
    "## Tasks execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da32d977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">1 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">asyncio</span><span style=\"background-color: #2f1e2e\">                                                                                                 </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">2 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">nest_asyncio</span><span style=\"background-color: #2f1e2e\">                                                                                            </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">3 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">nest_asyncio</span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">.</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">apply()</span><span style=\"background-color: #2f1e2e\">                                                                                           </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">4 </span><span style=\"background-color: #2f1e2e\">                                                                                                               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m1 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46masyncio\u001b[0m\u001b[48;2;47;30;46m                                                                                                 \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m2 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mnest_asyncio\u001b[0m\u001b[48;2;47;30;46m                                                                                            \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m3 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mnest_asyncio\u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m.\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mapply\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                                           \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m4 \u001b[0m\u001b[48;2;47;30;46m                                                                                                               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('', True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from metagpt.actions.di.execute_nb_code import ExecuteNbCode\n",
    "\n",
    "pre_execute = \"\"\"import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\"\"\"\n",
    "\n",
    "execute_code = ExecuteNbCode()\n",
    "await execute_code.run(pre_execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "819293bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 15:44:14.054 | DEBUG    | __main__:execute_task:9 - As an AI Engineer, you need to help user to achieve their goal step by step in a continuous Jupyter notebook.\n",
      "\n",
      "## Current Task\n",
      "Navigate to pitchhub.36kr.com/financing-flash\n",
      "\n",
      "## Task Guidance\n",
      "Write complete code for 'Current Task'. And avoid duplicating code from 'Finished Tasks', such as repeated import of packages, reading data, etc.\n",
      "Specifically, \n",
      "\n",
      "\n",
      "\n",
      "# Tool Info\n",
      "\n",
      "## Capabilities\n",
      "- You can utilize pre-defined tools in any code lines from 'Available Tools' in the form of Python class or function.\n",
      "- You can freely combine the use of any other public packages, like sklearn, numpy, pandas, etc..\n",
      "\n",
      "## Available Tools:\n",
      "Each tool is described in JSON format. When you call a tool, import the tool from imports first.\n",
      "{\"web scraping\": {\"type\": \"async_function\", \"description\": \"Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \", \"signature\": \"(url)\", \"parameters\": \"Args: url (str): The main URL to fetch inner text from. Returns: dict: The inner text content and html structure of the web page, keys are 'inner_text', 'html'.\", \"imports\": \"from metagpt.tools.libs.web_scraping import scrape_web_playwright\"}}\n",
      "{\"text extractor\": {\"type\": \"async_function\", \"description\": \"Perform extraction on the 'content' text using a large language model. \", \"signature\": \"(guidance: str, content: str, format: str) -> str\", \"parameters\": \"Args: guidance (str): Guide the extraction process. content (str): The text content that needs to be extracted. format (str): The output format, can be \\\"json\\\" or \\\"markdown\\\". Returns: str: text extracted from content.\", \"imports\": \"from tools.text_extractor.llm_extractor import llm_extractor\"}}\n",
      "\n",
      "# Constraints\n",
      "- Take on Current Task if it is in Plan Status, otherwise, tackle User Requirement directly.\n",
      "- Ensure the output new code is executable in the same Jupyter notebook as the previous executed code.\n",
      "- Always prioritize using pre-defined tools for the same functionality.\n",
      "\n",
      "# Output\n",
      "While some concise thoughts are helpful, code is absolutely required. Always output one and only one code block in your response. Output code in the following format:\n",
      "```python\n",
      "your code\n",
      "```\n",
      "\n",
      "Remember!! Since it is a notebook environment, don't use asyncio.run. Instead, use await if you need to call an async function.\n",
      "2024-03-24 15:44:14.055 | DEBUG    | metagpt.provider.base_llm:aask:149 - [{'role': 'user', 'content': 'As an AI Engineer, you need to help user to achieve their goal step by step in a continuous Jupyter notebook.\\n\\n## Current Task\\nNavigate to pitchhub.36kr.com/financing-flash\\n\\n## Task Guidance\\nWrite complete code for \\'Current Task\\'. And avoid duplicating code from \\'Finished Tasks\\', such as repeated import of packages, reading data, etc.\\nSpecifically, \\n\\n\\n\\n# Tool Info\\n\\n## Capabilities\\n- You can utilize pre-defined tools in any code lines from \\'Available Tools\\' in the form of Python class or function.\\n- You can freely combine the use of any other public packages, like sklearn, numpy, pandas, etc..\\n\\n## Available Tools:\\nEach tool is described in JSON format. When you call a tool, import the tool from imports first.\\n{\"web scraping\": {\"type\": \"async_function\", \"description\": \"Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \", \"signature\": \"(url)\", \"parameters\": \"Args: url (str): The main URL to fetch inner text from. Returns: dict: The inner text content and html structure of the web page, keys are \\'inner_text\\', \\'html\\'.\", \"imports\": \"from metagpt.tools.libs.web_scraping import scrape_web_playwright\"}}\\n{\"text extractor\": {\"type\": \"async_function\", \"description\": \"Perform extraction on the \\'content\\' text using a large language model. \", \"signature\": \"(guidance: str, content: str, format: str) -> str\", \"parameters\": \"Args: guidance (str): Guide the extraction process. content (str): The text content that needs to be extracted. format (str): The output format, can be \\\\\"json\\\\\" or \\\\\"markdown\\\\\". Returns: str: text extracted from content.\", \"imports\": \"from tools.text_extractor.llm_extractor import llm_extractor\"}}\\n\\n# Constraints\\n- Take on Current Task if it is in Plan Status, otherwise, tackle User Requirement directly.\\n- Ensure the output new code is executable in the same Jupyter notebook as the previous executed code.\\n- Always prioritize using pre-defined tools for the same functionality.\\n\\n# Output\\nWhile some concise thoughts are helpful, code is absolutely required. Always output one and only one code block in your response. Output code in the following format:\\n```python\\nyour code\\n```\\n\\nRemember!! Since it is a notebook environment, don\\'t use asyncio.run. Instead, use await if you need to call an async function.'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To navigate to the specified URL and scrape its content, we will use the pre-defined tool `web scraping`. Here is the code that you can use in your Jupyter notebook to achieve the current task:\n",
      "\n",
      "```python\n",
      "from metagpt.tools.libs.web_scraping import scrape_web_playwright\n",
      "\n",
      "# Define the URL to scrape\n",
      "url = 'https://pitchhub.36kr.com/financing-flash'\n",
      "\n",
      "# Asynchronously scrape the web page\n",
      "scraped_data = await scrape_web_playwright(url)\n",
      "\n",
      "# Output the scraped data\n",
      "print(scraped_data)\n",
      "```\n",
      "\n",
      "This code will fetch the inner text and HTML structure of the web page at the given URL without duplicating any imports or previous tasks. Remember to run this code in a J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 15:44:19.761 | DEBUG    | __main__:execute_task:12 - To navigate to the specified URL and scrape its content, we will use the pre-defined tool `web scraping`. Here is the code that you can use in your Jupyter notebook to achieve the current task:\n",
      "\n",
      "```python\n",
      "from metagpt.tools.libs.web_scraping import scrape_web_playwright\n",
      "\n",
      "# Define the URL to scrape\n",
      "url = 'https://pitchhub.36kr.com/financing-flash'\n",
      "\n",
      "# Asynchronously scrape the web page\n",
      "scraped_data = await scrape_web_playwright(url)\n",
      "\n",
      "# Output the scraped data\n",
      "print(scraped_data)\n",
      "```\n",
      "\n",
      "This code will fetch the inner text and HTML structure of the web page at the given URL without duplicating any imports or previous tasks. Remember to run this code in a Jupyter notebook cell that is capable of handling asynchronous operations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upyter notebook cell that is capable of handling asynchronous operations.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 1 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">from</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">metagpt.tools.libs.web_scraping</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> scrape_web_playwright</span><span style=\"background-color: #2f1e2e\">                                             </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 2 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 3 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Define the URL to scrape</span><span style=\"background-color: #2f1e2e\">                                                                                    </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 4 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">url </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">'https://pitchhub.36kr.com/financing-flash'</span><span style=\"background-color: #2f1e2e\">                                                             </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 5 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 6 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Asynchronously scrape the web page</span><span style=\"background-color: #2f1e2e\">                                                                          </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 7 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">scraped_data </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">await</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> scrape_web_playwright(url)</span><span style=\"background-color: #2f1e2e\">                                                               </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 8 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 9 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Output the scraped data</span><span style=\"background-color: #2f1e2e\">                                                                                     </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">10 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">print(scraped_data)</span><span style=\"background-color: #2f1e2e\">                                                                                           </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">11 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 1 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mfrom\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mmetagpt\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mtools\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mlibs\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mweb_scraping\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mscrape_web_playwright\u001b[0m\u001b[48;2;47;30;46m                                             \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 2 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 3 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Define the URL to scrape\u001b[0m\u001b[48;2;47;30;46m                                                                                    \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 4 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m'\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46mhttps://pitchhub.36kr.com/financing-flash\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m'\u001b[0m\u001b[48;2;47;30;46m                                                             \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 5 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 6 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Asynchronously scrape the web page\u001b[0m\u001b[48;2;47;30;46m                                                                          \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 7 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mscraped_data\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46mawait\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mscrape_web_playwright\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                               \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 8 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 9 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Output the scraped data\u001b[0m\u001b[48;2;47;30;46m                                                                                     \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m10 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mprint\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mscraped_data\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                                           \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m11 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"from metagpt.tools.libs.web_scraping import scrape_web_playwright\\n\\n# Define the URL to scrape\\nurl = 'https://pitchhub.36kr.com/financing-flash'\\n\\n# Asynchronously scrape the web page\\nscraped_data = await scrape_web_playwright(url)\\n\\n# Output the scraped data\\nprint(scraped_data)\\n\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def execute_task(plan: Plan, plan_status=\"\", task_guidance=\"\"):\n",
    "    codegen_prompt = load_prompts(\"task_codegen\", \"task_codegen.yaml\")\n",
    "    template = codegen_prompt.format(\n",
    "        plan_status=plan_status,\n",
    "        current_task=plan.current_task.instruction,\n",
    "        task_guidance=task_guidance,\n",
    "        tools=tools_list,\n",
    "    )\n",
    "    logger.debug(template)\n",
    "\n",
    "    rsp = llm_aask(msg=template)\n",
    "    logger.debug(rsp)\n",
    "\n",
    "    code_block = OutputParser.parse_code(rsp, \"python\")\n",
    "    execute_code._display(code_block, language=\"python\")\n",
    "    return code_block\n",
    "\n",
    "code = execute_task(plan)\n",
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1cc75ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 1 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">from</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">metagpt.tools.libs.web_scraping</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> scrape_web_playwright</span><span style=\"background-color: #2f1e2e\">                                             </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 2 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 3 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Define the URL to scrape</span><span style=\"background-color: #2f1e2e\">                                                                                    </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 4 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">url </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">'https://pitchhub.36kr.com/financing-flash'</span><span style=\"background-color: #2f1e2e\">                                                             </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 5 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 6 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Asynchronously scrape the web page</span><span style=\"background-color: #2f1e2e\">                                                                          </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 7 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">scraped_data </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">await</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> scrape_web_playwright(url)</span><span style=\"background-color: #2f1e2e\">                                                               </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 8 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 9 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Output the scraped data</span><span style=\"background-color: #2f1e2e\">                                                                                     </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">10 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">print(scraped_data)</span><span style=\"background-color: #2f1e2e\">                                                                                           </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">11 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 1 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mfrom\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mmetagpt\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mtools\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mlibs\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mweb_scraping\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mscrape_web_playwright\u001b[0m\u001b[48;2;47;30;46m                                             \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 2 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 3 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Define the URL to scrape\u001b[0m\u001b[48;2;47;30;46m                                                                                    \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 4 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m'\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46mhttps://pitchhub.36kr.com/financing-flash\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m'\u001b[0m\u001b[48;2;47;30;46m                                                             \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 5 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 6 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Asynchronously scrape the web page\u001b[0m\u001b[48;2;47;30;46m                                                                          \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 7 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mscraped_data\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46mawait\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mscrape_web_playwright\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                               \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 8 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 9 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Output the scraped data\u001b[0m\u001b[48;2;47;30;46m                                                                                     \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m10 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mprint\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mscraped_data\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                                           \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m11 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " \"{'inner_text': '首页\\\\n融资快报\\\\n融资事件\\\\n项目库\\\\n机构库\\\\n项目集\\\\n定向对接\\\\n融通创新\\\\n公司/项目名/投资机构/赛道\\\\n\\\\xa0\\\\n返回36氪\\\\n登录\\\\n融资快报\\\\n文章\\\\nPocketHealth获3300万美元B轮融资，研发医学影像共享平台 | 海外New Things\\\\n该公司推出了Report Reader，用于解释患者报告中的复杂医学术语，提高患者对医疗信息的理解。\\\\n2分钟前\\\\n海若镜\\\\n文章\\\\nClasp Therapeutics获1.5亿美元A轮融资，研发针对肿瘤的精准免疫疗法 | 海外New Things\\\\n重新引导T细胞杀死癌细胞，同时保护全身的健康细胞。\\\\n4分钟前\\\\n海若镜\\\\n文章\\\\nEngrail Therapeutics获1.57亿美元B轮融资，研发针对焦虑症等的疗法 | 海外New Things\\\\nEngrail应用精密化学和药理学，研发治疗焦虑症、抑郁症、创伤后应激障碍和罕见神经退行性疾病等具有重大未满...\\\\n8分钟前\\\\n海若镜\\\\n快讯\\\\n汽车智能化产业企业“博泰车联网”再获约15亿元股权融资\\\\n36氪获悉，汽车智能化产业企业“博泰车联网”近日成功斩获15亿元新一轮融资，将重点投入新一代智能化融域控产品的研发，并全面加速智驾域控的全栈技术创新。同时，公司计划深化舱驾融合解决方案及高性能中央计算平台，以上海为轴心，联动浙江、安徽、江苏，构建长三角一体化的智能制造与供应链新体系，区域布局未来灯塔工厂。\\\\n博泰车联网\\\\n战略融资\\\\n上海市\\\\n2009年成立\\\\n智能网联产品与技术服务提供商\\\\n6小时前\\\\n快讯\\\\n深圳“进化动力”完成C轮融资\\\\n36氪广东获悉，据“紫荆汇富”微信公众号消息，深圳进化动力数码科技有限公司（下称“进化动力”）完成C轮融资，投资方为紫荆汇富、知成基金、天瑞丰年、一桥基金。本轮融资资金将用于建设新生产线并加大布局赣州龙南地区。“进化动力”成立于2015年，公司提供端侧训练-学习芯片及系统平台，为供应链金融、保险、新商业、生鲜零售、智慧农贸、智慧城市、移动机器人等领域打造芯片产品及系统解决方案。原文链接\\\\n进化动力\\\\nB轮\\\\n广东省\\\\n2015年成立\\\\n商业视觉智能芯片及平台提供商\\\\n2024-03-22\\\\n文章\\\\n成立半年融资破亿，线控底盘厂商「坐标系」再获北极光领投5000万元融资｜36氪首发\\\\n定位Tier0.5，切入高性能EMB系统\\\\n2024-03-22\\\\n肖千平\\\\n快讯\\\\n联舌工坊完成近亿元A轮融资\\\\n近期，联舌工坊完成了近亿元的A轮融资，投资方包括联创股份，汇达投资，领润科技等。 联舌工坊是一个预制菜品牌，成立于2019年，致力于发展B端餐饮市场，为餐饮连锁企业做到产品研发，一站式出品要求。（界面新闻）原文链接\\\\n联舌工坊\\\\n天使轮\\\\n上海市\\\\n2019年成立\\\\n预制菜品牌\\\\n2024-03-22\\\\n快讯\\\\n“小格智能”完成Pre-A+轮融资\\\\n36氪获悉，近日，“小格智能”宣布完成Pre-A+轮融资，由真成投资独家投资。本轮资金将主要用于扩大城市占有率，核心技术研发与创新，提升数字化服务能力，继续加大对数据驱动服务的投入，将移动互联网及物联网技术应用到商用洗碗机租赁领域。\\\\n小格智能\\\\nPre-A轮\\\\n江苏省\\\\n2021年成立\\\\n餐厅后厨智能设备研发产销商\\\\n2024-03-22\\\\n文章\\\\n洞察科技完成数千万元天使轮融资，加速推进固态纳米孔在生物大分子实时检测和基因测序领域的大规模商用\\\\n广州洞察科技完成数千万融资，发展纳米孔测序。\\\\n2024-03-22\\\\n多维资本\\\\n快讯\\\\n“恒宇医疗”获数千万元融资\\\\n36氪获悉，“恒宇医疗”宣布完成数千万元融资，本轮由老股东天士力资本领投，海河基金与海泰资本共同设立的生物医药基金跟投，浩悦资本连续担任独家财务顾问。本轮融资将助力研发、生产、注册，并重点加速推进已上市产品的商业化推广。\\\\n恒宇医疗\\\\nD轮\\\\n天津市\\\\n2016年成立\\\\nOCT影像设备技术研发\\\\n2024-03-22\\\\n快讯\\\\n小竹财税获1000万元种子轮融资\\\\n近日，小竹财税完成了新一轮融资1000万元，投资方未透露。据悉，小竹财税是安徽小竹信息技术有限公司的品牌，公司成立于2021年11月，其是一家财税科技服务商，旗下小竹财税官网汇集问题库、小竹问答、税收法规、会计法规、小竹资讯、小竹学院、模板专区、小竹企服等板块。\\\\n2024-03-21\\\\n快讯\\\\n交泰智能完成数百万元种子轮融资\\\\n近日，安徽交泰智能技术有限公司（简称“交泰智能”）完成数百万元种子轮融资，合肥高投领投，以推动其在制造业及新能源行业设备资产智能管理技术的研发与产业化进程。公开资料显示，交泰智能成立于2021年12月，位于安徽合肥。\\\\n2024-03-21\\\\n快\")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result, success = await execute_code.run(code)\n",
    "success, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9d0cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = f\"\"\"### Error\n",
    "```\n",
    "{result}\n",
    "```\n",
    "\n",
    "### Previous code\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "code = execute_task(plan, plan_status=status)\n",
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b5cca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
