{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba7766cf-75a4-4370-ba70-573e88e5d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from utils.common import load_plaintext\n",
    "\n",
    "# debug level\n",
    "from metagpt.logs import logger\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"DEBUG\")\n",
    "\n",
    "# make asyncio.run() works in notebook\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51f34c9",
   "metadata": {},
   "source": [
    "## LLM 与 tools 准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f19600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 智普AI\n",
    "# ~/.metagpt/config2.yaml\n",
    "from metagpt.config2 import config\n",
    "from metagpt.provider.zhipuai_api import ZhiPuAILLM\n",
    "# from metagpt.utils.cost_manager import CostManager\n",
    "\n",
    "llm = ZhiPuAILLM(config.llm)\n",
    "llm.use_system_prompt = False # Disable default system message\n",
    "# llm.cost_manager = CostManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01ead1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. gemma-7b-it (vllm)\n"
     ]
    }
   ],
   "source": [
    "# vllm 本地\n",
    "import yaml\n",
    "from metagpt.configs.llm_config import LLMConfig\n",
    "from metagpt.provider.openai_api import OpenAILLM\n",
    "# from metagpt.utils.cost_manager import CostManager\n",
    "\n",
    "llm_configs = yaml.safe_load(load_plaintext(\"../\", \"vllm_local.yaml\"))\n",
    "llm_config = LLMConfig.model_validate(llm_configs['llm'])\n",
    "llm = OpenAILLM(llm_config)\n",
    "llm.use_system_prompt = False # Disable default system message\n",
    "# llm.cost_manager = CostManager()\n",
    "\n",
    "models = await llm.aclient.models.list()\n",
    "for idx, mod in enumerate(models.data):\n",
    "    print(f\"{idx+1}. {mod.id} ({mod.owned_by})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06449968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 22:55:46.108 | DEBUG    | metagpt.provider.base_llm:aask:149 - [{'role': 'user', 'content': '你好，介绍下你自己'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是一个大型语言模型，由 Google AI 开发。我拥有大量的文字和代码，可以提供各种各样的信息和服务。我能够进行各种任务，包括：\n",
      "\n",
      "* **信息检索:** 我可以提供各种主题上的信息，从基础知识到最新新闻。\n",
      "* **代码编写:** 我可以帮助你编写代码，包括各种编程语言。\n",
      "* **语言翻译:** 我可以翻译多种语言。\n",
      "* **对话:** 我可以与你进行对话，并提供娱乐和信息。\n",
      "\n",
      "我还在不断学习和进步，以提供更加准确和强大的服务。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我是一个大型语言模型，由 Google AI 开发。我拥有大量的文字和代码，可以提供各种各样的信息和服务。我能够进行各种任务，包括：\\n\\n* **信息检索:** 我可以提供各种主题上的信息，从基础知识到最新新闻。\\n* **代码编写:** 我可以帮助你编写代码，包括各种编程语言。\\n* **语言翻译:** 我可以翻译多种语言。\\n* **对话:** 我可以与你进行对话，并提供娱乐和信息。\\n\\n我还在不断学习和进步，以提供更加准确和强大的服务。'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache\n",
    "def llm_aask(msg, seed=None):\n",
    "    return asyncio.run(llm.aask(msg=msg))\n",
    "\n",
    "llm_aask(\"你好，介绍下你自己\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bec60a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**web scraping**: Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \n",
      "**text extractor**: Perform extraction on the 'content' text using a large language model. \n",
      "\n",
      "{\"web scraping\": {\"type\": \"async_function\", \"description\": \"Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \", \"signature\": \"(url)\", \"parameters\": \"Args: url (str): The main URL to fetch inner text from. Returns: dict: The inner text content and html structure of the web page, keys are 'inner_text', 'html'.\", \"import\": \"from metagpt.tools.libs.web_scraping import scrape_web_playwright\"}}\n",
      "{\"text extractor\": {\"type\": \"async_function\", \"description\": \"Perform extraction on the 'content' text using a large language model. \", \"signature\": \"(guidance: str, content: str, format: str) -> str\", \"parameters\": \"Args: guidance (str): Guide the extraction process. content (str): The text content that needs to be extracted. format (str): The output format, can be \\\"json\\\" or \\\"markdown\\\". Returns: str: text extracted from content.\", \"import\": \"from tools.text_extractor.llm_extractor import llm_extractor\"}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import inspect\n",
    "from metagpt.tools.tool_convert import function_docstring_to_schema\n",
    "from metagpt.tools.libs.web_scraping import scrape_web_playwright\n",
    "from tools.text_extractor.llm_extractor import llm_extractor\n",
    "\n",
    "def function_to_schema(func):\n",
    "    docstring = inspect.getdoc(func)\n",
    "    schema = function_docstring_to_schema(func, docstring)\n",
    "    schema[\"import\"] = f\"from {func.__module__} import {func.__name__}\"\n",
    "    return schema\n",
    "\n",
    "DEF_TOOLS = [\n",
    "    (\"web scraping\", scrape_web_playwright),\n",
    "    (\"text extractor\", llm_extractor),\n",
    "]\n",
    "tools = {}\n",
    "for name, func in DEF_TOOLS:\n",
    "    schema = function_to_schema(func)\n",
    "    tools[name] = schema\n",
    "tools_list = \"\\n\".join([ json.dumps({k:v}) for k,v in tools.items() ])\n",
    "\n",
    "task_types = \"\\n\".join([\n",
    "    f\"**{k}**: {v['description']}\" for k,v in tools.items()\n",
    "])\n",
    "print(task_types + \"\\n\")\n",
    "print(tools_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7def00e",
   "metadata": {},
   "source": [
    "## Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23786658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 23:02:02.036 | DEBUG    | __main__:create_plan:33 - Respond to the human as helpfully and accurately as possible.\n",
      "\n",
      "# User goal\n",
      "抓取 https://pitchhub.36kr.com/financing-flash 中'快讯'的内容，并整理成markdown存档\n",
      "\n",
      "# 参考流程\n",
      "- 使用工具抓取网页中的可见文本\n",
      "- 提取网页文本中'快讯'相关的内容。注意网页中可能包含导航，只需要抽取'快讯'的具体内容\n",
      "- 对抽取结果进行归类，并保存成markdown表格: 快讯.md\n",
      "\n",
      "\n",
      "\n",
      "# Available Task Types\n",
      "**web scraping**: Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \n",
      "**text extractor**: Perform extraction on the 'content' text using a large language model. \n",
      "\n",
      "# Task\n",
      "Based on the user goal, write a plan or modify an existing plan of what you should do to achieve the goal. A plan consists of one to 20 tasks.\n",
      "If you are modifying an existing plan, carefully follow the instruction, don't make unnecessary changes. Give the whole plan unless instructed to modify only one task of the plan.\n",
      "If you encounter errors on the current task, revise and output the current single task only.\n",
      "Output a list of jsons following the format:\n",
      "```json\n",
      "[\n",
      "    {{\n",
      "        \"task_id\": str = \"unique identifier for a task in plan, can be an ordinal\",\n",
      "        \"dependent_task_ids\": list[str] = \"ids of tasks prerequisite to this task\",\n",
      "        \"instruction\": \"what you should do in this task, one short phrase or sentence\",\n",
      "        \"task_type\": \"type of this task, should be one of Available Task Types\",\n",
      "    }},\n",
      "    ...\n",
      "]\n",
      "```\n",
      "2024-04-08 23:02:02.037 | DEBUG    | metagpt.provider.base_llm:aask:149 - [{'role': 'user', 'content': 'Respond to the human as helpfully and accurately as possible.\\n\\n# User goal\\n抓取 https://pitchhub.36kr.com/financing-flash 中\\'快讯\\'的内容，并整理成markdown存档\\n\\n# 参考流程\\n- 使用工具抓取网页中的可见文本\\n- 提取网页文本中\\'快讯\\'相关的内容。注意网页中可能包含导航，只需要抽取\\'快讯\\'的具体内容\\n- 对抽取结果进行归类，并保存成markdown表格: 快讯.md\\n\\n\\n\\n# Available Task Types\\n**web scraping**: Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \\n**text extractor**: Perform extraction on the \\'content\\' text using a large language model. \\n\\n# Task\\nBased on the user goal, write a plan or modify an existing plan of what you should do to achieve the goal. A plan consists of one to 20 tasks.\\nIf you are modifying an existing plan, carefully follow the instruction, don\\'t make unnecessary changes. Give the whole plan unless instructed to modify only one task of the plan.\\nIf you encounter errors on the current task, revise and output the current single task only.\\nOutput a list of jsons following the format:\\n```json\\n[\\n    {{\\n        \"task_id\": str = \"unique identifier for a task in plan, can be an ordinal\",\\n        \"dependent_task_ids\": list[str] = \"ids of tasks prerequisite to this task\",\\n        \"instruction\": \"what you should do in this task, one short phrase or sentence\",\\n        \"task_type\": \"type of this task, should be one of Available Task Types\",\\n    }},\\n    ...\\n]\\n```'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Plan\n",
      "\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"task_id\": \"1\",\n",
      "        \"dependent_task_ids\": [],\n",
      "        \"instruction\": \"Navigate to the website: pitchhub.36kr.com/financing-flash\",\n",
      "        \"task_type\": \"web scraping\"\n",
      "    },\n",
      "    {\n",
      "        \"task_id\": \"2\",\n",
      "        \"dependent_task_ids\": [\"1\"],\n",
      "        \"instruction\": \"Extract the text content of the webpage\",\n",
      "        \"task_type\": \"web scraping\"\n",
      "    },\n",
      "    {\n",
      "        \"task_id\": \"3\",\n",
      "        \"dependent_task_ids\": [\"2\"],\n",
      "        \"instruction\": \"Find the text content containing the word '快讯' and extract it\",\n",
      "        \"task_type\": \"text extractor\"\n",
      "    },\n",
      "    {\n",
      "        \"task_id\": \"4\",\n",
      "        \"dependent_task_ids\": [\"3\"],\n",
      "        \"instruction\": \"Save the extracted text content into a markdown file named '快讯.md'\",\n",
      "        \"task_type\": \"text file creation\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "\n",
      "**Notes:**\n",
      "\n",
      "- This plan includes four tasks: navigating to the website, extracting the text content of the webpage, finding the text content containing the word '快讯' and saving the extracted text content into a markdown file.\n",
      "- The 'dependent_task_ids' attribute is used to specify the tasks that must be completed before the current task can be started.\n",
      "- The 'task_type' attribute specifies the type of task that should be performed.\n",
      "- This plan assumes that the user has the necessary tools and libraries"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 23:02:12.634 | DEBUG    | __main__:create_plan:41 - [Task(task_id='1', dependent_task_ids=[], instruction='Navigate to the website: pitchhub.36kr.com/financing-flash', task_type='web scraping', code='', result='', is_success=False, is_finished=False), Task(task_id='2', dependent_task_ids=['1'], instruction='Extract the text content of the webpage', task_type='web scraping', code='', result='', is_success=False, is_finished=False), Task(task_id='3', dependent_task_ids=['2'], instruction=\"Find the text content containing the word '快讯' and extract it\", task_type='text extractor', code='', result='', is_success=False, is_finished=False), Task(task_id='4', dependent_task_ids=['3'], instruction=\"Save the extracted text content into a markdown file named '快讯.md'\", task_type='text file creation', code='', result='', is_success=False, is_finished=False)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " installed to complete the tasks.\n",
      "[Task(task_id='1', dependent_task_ids=[], instruction='Navigate to the website: pitchhub.36kr.com/financing-flash', task_type='web scraping', code='', result='', is_success=False, is_finished=False),\n",
      " Task(task_id='2', dependent_task_ids=['1'], instruction='Extract the text content of the webpage', task_type='web scraping', code='', result='', is_success=False, is_finished=False),\n",
      " Task(task_id='3', dependent_task_ids=['2'], instruction=\"Find the text content containing the word '快讯' and extract it\", task_type='text extractor', code='', result='', is_success=False, is_finished=False),\n",
      " Task(task_id='4', dependent_task_ids=['3'], instruction=\"Save the extracted text content into a markdown file named '快讯.md'\", task_type='text file creation', code='', result='', is_success=False, is_finished=False)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "from metagpt.schema import Plan, Task\n",
    "from metagpt.utils.common import OutputParser\n",
    "from pprint import pprint # debug\n",
    "\n",
    "def load_prompts(path: str, filename: str) -> PromptTemplate:\n",
    "    base_path = os.path.join(\"prompts\", path)\n",
    "    output_format = load_plaintext(base_path, \"output.md\")\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[],\n",
    "        template=load_plaintext(base_path, filename),\n",
    "    )\n",
    "    return prompt.partial(output=output_format)\n",
    "\n",
    "def parse_json(rsp):\n",
    "    try:\n",
    "        objs = json.loads(rsp)\n",
    "    except:\n",
    "        code_block = OutputParser.parse_code(rsp, \"json\")\n",
    "        objs = json.loads(code_block)\n",
    "    return objs\n",
    "\n",
    "def create_plan(goal, guidance, last_plan=\"\"):\n",
    "    plan_prompt = load_prompts(\"planning\", \"planning.yaml\")\n",
    "    template = plan_prompt.format(\n",
    "        goal=goal,\n",
    "        user_guidance=guidance,\n",
    "        last_plan=last_plan,\n",
    "        task_types=task_types,\n",
    "        max_tasks=20,\n",
    "    )\n",
    "    logger.debug(template)\n",
    "\n",
    "    plan = Plan(goal=goal)\n",
    "    plan.context = guidance\n",
    "    rsp = llm_aask(msg=template)\n",
    "\n",
    "    tasks_json = parse_json(rsp)\n",
    "    tasks = [Task(**task_config) for task_config in tasks_json]\n",
    "    logger.debug(tasks)\n",
    "\n",
    "    plan.add_tasks(tasks)\n",
    "    return plan, tasks_json\n",
    "\n",
    "\n",
    "user_goal = \"抓取 https://pitchhub.36kr.com/financing-flash 中'快讯'的内容，并整理成markdown存档\"\n",
    "user_guidance = \"\"\"# 参考流程\n",
    "- 使用工具抓取网页中的可见文本\n",
    "- 提取网页文本中'快讯'相关的内容。注意网页中可能包含导航，只需要抽取'快讯'的具体内容\n",
    "- 对抽取结果进行归类，并保存成markdown表格: 快讯.md\"\"\"\n",
    "\n",
    "plan, raw_json = create_plan(goal=user_goal, guidance=user_guidance)\n",
    "pprint(plan.tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a7cfcc",
   "metadata": {},
   "source": [
    "### Plan Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "669eb4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 21:48:46.003 | DEBUG    | metagpt.provider.base_llm:aask:149 - [{'role': 'user', 'content': 'Review the plan and determine if the plan can achieve the user goal.\\n\\n# User Goal\\n抓取 https://pitchhub.36kr.com/financing-flash 中\\'快讯\\'的内容，并整理成markdown存档\\n\\n# Plan\\n```json\\n[{\\'task_id\\': \\'1\\', \\'dependent_task_ids\\': [], \\'instruction\\': \\'Choose a programming language and set up the required tools for web scraping and text extraction.\\', \\'task_type\\': \\'setup\\'}, {\\'task_id\\': \\'2\\', \\'dependent_task_ids\\': [\\'1\\'], \\'instruction\\': \\'Clone the repository or create a new project directory for the web scraping and text extraction tasks.\\', \\'task_type\\': \\'file_management\\'}, {\\'task_id\\': \\'3\\', \\'dependent_task_ids\\': [\\'2\\'], \\'instruction\\': \\'Write a script using Playwright to scrape the HTML structure and inner text content of the specified URL (https://pitchhub.36kr.com/financing-flash).\\', \\'task_type\\': \\'web_scraping\\'}, {\\'task_id\\': \\'4\\', \\'dependent_task_ids\\': [\\'3\\'], \\'instruction\\': \"Parse the obtained HTML structure to extract the content within the \\'quick-news\\' section.\", \\'task_type\\': \\'html_parsing\\'}, {\\'task_id\\': \\'5\\', \\'dependent_task_ids\\': [\\'4\\'], \\'instruction\\': \"Use a large language model to extract the \\'快讯\\' content from the parsed content.\", \\'task_type\\': \\'text_extractor\\'}, {\\'task_id\\': \\'6\\', \\'dependent_task_ids\\': [\\'5\\'], \\'instruction\\': \"Organize the extracted \\'快讯\\' content into a well-structured markdown table.\", \\'task_type\\': \\'text_processing\\'}, {\\'task_id\\': \\'7\\', \\'dependent_task_ids\\': [\\'6\\'], \\'instruction\\': \"Save the markdown table as a separate file named \\'快讯.md\\' in the project directory.\", \\'task_type\\': \\'file_management\\'}]\\n```\\n\\n# Output\\nOutput a list of jsons following the format:\\n```json\\n[\\n    {\\n        \"task_id\": str = \"unique identifier for a task in plan, can be an ordinal\",\\n        \"dependent_task_ids\": list[str] = \"ids of tasks prerequisite to this task\",\\n        \"instruction\": \"what you should do in this task, one short phrase or sentence\",\\n        \"task_type\": \"type of this task, should be one of Available Task Types\",\\n    },\\n    ...\\n]\\n```\\n\\n# Constraint\\n- The breakdown of the plan is clear enough, and each task has single goal and easy to complete.\\n- Use the provided task types whenever possible to avoid unnecessary steps.\\n- Check whether the plan format meets the output requirements.\\n\\n**REMEMBER**: Only output the modifiy suggestions in plain text, for instruction on how to adjust the plan.\\n'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The plan seems to be properly set up for achieving the user goal. The tasks are well-defined, and each step logically follows the previous one. The use of the provided task types also makes it easier to understand the purpose of each task. The plan format meets the output requirements.\n",
      "\n",
      "However, there's one suggestion to make the plan more efficient:\n",
      "\n",
      "Instruction for task 4 could be combined with task 3 to save time and resources:\n",
      "\n",
      "```json\n",
      "- Replace task 4 with:\n",
      "  {\"task_id\": \"3\", \"dependent_task_ids\": [\"2\"], \"instruction\": \"Write a script using Playwright to scrape the HTML structure and inner text content of the specified URL (https://pitchhub.36kr.com/financing-flash), and parse the obtained HTML structure to extract the content within the 'quick-news' section.\", \"task_type\": \"web_scraping, html_parsing\"}\n",
      "```\n",
      "\n",
      "This way, the scraping and parsing will be done in a single task, reducing the need to repeat the web scraping process.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The plan seems to be properly set up for achieving the user goal. The tasks are well-defined, and each step logically follows the previous one. The use of the provided task types also makes it easier to understand the purpose of each task. The plan format meets the output requirements.\\n\\nHowever, there\\'s one suggestion to make the plan more efficient:\\n\\nInstruction for task 4 could be combined with task 3 to save time and resources:\\n\\n```json\\n- Replace task 4 with:\\n  {\"task_id\": \"3\", \"dependent_task_ids\": [\"2\"], \"instruction\": \"Write a script using Playwright to scrape the HTML structure and inner text content of the specified URL (https://pitchhub.36kr.com/financing-flash), and parse the obtained HTML structure to extract the content within the \\'quick-news\\' section.\", \"task_type\": \"web_scraping, html_parsing\"}\\n```\\n\\nThis way, the scraping and parsing will be done in a single task, reducing the need to repeat the web scraping process.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_prompt = load_prompts(\"planning\", \"review.yaml\")\n",
    "template = review_prompt.format(\n",
    "    goal=user_goal,\n",
    "    user_guidance=user_guidance,\n",
    "    task_types=task_types,\n",
    "    content=raw_json,\n",
    ")\n",
    "# logger.debug(template)\n",
    "\n",
    "rsp = llm_aask(msg=template)\n",
    "rsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0af7fd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 21:48:52.022 | DEBUG    | __main__:create_plan:33 - Respond to the human as helpfully and accurately as possible.\n",
      "\n",
      "# User goal\n",
      "抓取 https://pitchhub.36kr.com/financing-flash 中'快讯'的内容，并整理成markdown存档\n",
      "\n",
      "大概的流程：\n",
      "- 使用工具抓取网页中的可见文本\n",
      "- 提取网页文本中'快讯'相关的内容。注意网页中可能包含导航，只需要抽取'快讯'的具体内容\n",
      "- 对抽取结果进行归类，并保存成markdown表格: 快讯.md\n",
      "\n",
      "# Last plan\n",
      "## Plan\n",
      "```json\n",
      "[{'task_id': '1', 'dependent_task_ids': [], 'instruction': 'Choose a programming language and set up the required tools for web scraping and text extraction.', 'task_type': 'setup'}, {'task_id': '2', 'dependent_task_ids': ['1'], 'instruction': 'Clone the repository or create a new project directory for the web scraping and text extraction tasks.', 'task_type': 'file_management'}, {'task_id': '3', 'dependent_task_ids': ['2'], 'instruction': 'Write a script using Playwright to scrape the HTML structure and inner text content of the specified URL (https://pitchhub.36kr.com/financing-flash).', 'task_type': 'web_scraping'}, {'task_id': '4', 'dependent_task_ids': ['3'], 'instruction': \"Parse the obtained HTML structure to extract the content within the 'quick-news' section.\", 'task_type': 'html_parsing'}, {'task_id': '5', 'dependent_task_ids': ['4'], 'instruction': \"Use a large language model to extract the '快讯' content from the parsed content.\", 'task_type': 'text_extractor'}, {'task_id': '6', 'dependent_task_ids': ['5'], 'instruction': \"Organize the extracted '快讯' content into a well-structured markdown table.\", 'task_type': 'text_processing'}, {'task_id': '7', 'dependent_task_ids': ['6'], 'instruction': \"Save the markdown table as a separate file named '快讯.md' in the project directory.\", 'task_type': 'file_management'}]\n",
      "```\n",
      "\n",
      "## Review\n",
      "The plan seems to be properly set up for achieving the user goal. The tasks are well-defined, and each step logically follows the previous one. The use of the provided task types also makes it easier to understand the purpose of each task. The plan format meets the output requirements.\n",
      "\n",
      "However, there's one suggestion to make the plan more efficient:\n",
      "\n",
      "Instruction for task 4 could be combined with task 3 to save time and resources:\n",
      "\n",
      "```json\n",
      "- Replace task 4 with:\n",
      "  {\"task_id\": \"3\", \"dependent_task_ids\": [\"2\"], \"instruction\": \"Write a script using Playwright to scrape the HTML structure and inner text content of the specified URL (https://pitchhub.36kr.com/financing-flash), and parse the obtained HTML structure to extract the content within the 'quick-news' section.\", \"task_type\": \"web_scraping, html_parsing\"}\n",
      "```\n",
      "\n",
      "This way, the scraping and parsing will be done in a single task, reducing the need to repeat the web scraping process.\n",
      "\n",
      "\n",
      "# Available Task Types\n",
      "**web scraping**: Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \n",
      "**text extractor**: Perform extraction on the 'content' text using a large language model. \n",
      "\n",
      "# Task\n",
      "Based on the user goal, write a plan or modify an existing plan of what you should do to achieve the goal. A plan consists of one to 20 tasks.\n",
      "If you are modifying an existing plan, carefully follow the instruction, don't make unnecessary changes. Give the whole plan unless instructed to modify only one task of the plan.\n",
      "If you encounter errors on the current task, revise and output the current single task only.\n",
      "Output a list of jsons following the format:\n",
      "```json\n",
      "[\n",
      "    {{\n",
      "        \"task_id\": str = \"unique identifier for a task in plan, can be an ordinal\",\n",
      "        \"dependent_task_ids\": list[str] = \"ids of tasks prerequisite to this task\",\n",
      "        \"instruction\": \"what you should do in this task, one short phrase or sentence\",\n",
      "        \"task_type\": \"type of this task, should be one of Available Task Types\",\n",
      "    }},\n",
      "    ...\n",
      "]\n",
      "```\n",
      "2024-04-08 21:48:52.023 | DEBUG    | metagpt.provider.base_llm:aask:149 - [{'role': 'user', 'content': 'Respond to the human as helpfully and accurately as possible.\\n\\n# User goal\\n抓取 https://pitchhub.36kr.com/financing-flash 中\\'快讯\\'的内容，并整理成markdown存档\\n\\n大概的流程：\\n- 使用工具抓取网页中的可见文本\\n- 提取网页文本中\\'快讯\\'相关的内容。注意网页中可能包含导航，只需要抽取\\'快讯\\'的具体内容\\n- 对抽取结果进行归类，并保存成markdown表格: 快讯.md\\n\\n# Last plan\\n## Plan\\n```json\\n[{\\'task_id\\': \\'1\\', \\'dependent_task_ids\\': [], \\'instruction\\': \\'Choose a programming language and set up the required tools for web scraping and text extraction.\\', \\'task_type\\': \\'setup\\'}, {\\'task_id\\': \\'2\\', \\'dependent_task_ids\\': [\\'1\\'], \\'instruction\\': \\'Clone the repository or create a new project directory for the web scraping and text extraction tasks.\\', \\'task_type\\': \\'file_management\\'}, {\\'task_id\\': \\'3\\', \\'dependent_task_ids\\': [\\'2\\'], \\'instruction\\': \\'Write a script using Playwright to scrape the HTML structure and inner text content of the specified URL (https://pitchhub.36kr.com/financing-flash).\\', \\'task_type\\': \\'web_scraping\\'}, {\\'task_id\\': \\'4\\', \\'dependent_task_ids\\': [\\'3\\'], \\'instruction\\': \"Parse the obtained HTML structure to extract the content within the \\'quick-news\\' section.\", \\'task_type\\': \\'html_parsing\\'}, {\\'task_id\\': \\'5\\', \\'dependent_task_ids\\': [\\'4\\'], \\'instruction\\': \"Use a large language model to extract the \\'快讯\\' content from the parsed content.\", \\'task_type\\': \\'text_extractor\\'}, {\\'task_id\\': \\'6\\', \\'dependent_task_ids\\': [\\'5\\'], \\'instruction\\': \"Organize the extracted \\'快讯\\' content into a well-structured markdown table.\", \\'task_type\\': \\'text_processing\\'}, {\\'task_id\\': \\'7\\', \\'dependent_task_ids\\': [\\'6\\'], \\'instruction\\': \"Save the markdown table as a separate file named \\'快讯.md\\' in the project directory.\", \\'task_type\\': \\'file_management\\'}]\\n```\\n\\n## Review\\nThe plan seems to be properly set up for achieving the user goal. The tasks are well-defined, and each step logically follows the previous one. The use of the provided task types also makes it easier to understand the purpose of each task. The plan format meets the output requirements.\\n\\nHowever, there\\'s one suggestion to make the plan more efficient:\\n\\nInstruction for task 4 could be combined with task 3 to save time and resources:\\n\\n```json\\n- Replace task 4 with:\\n  {\"task_id\": \"3\", \"dependent_task_ids\": [\"2\"], \"instruction\": \"Write a script using Playwright to scrape the HTML structure and inner text content of the specified URL (https://pitchhub.36kr.com/financing-flash), and parse the obtained HTML structure to extract the content within the \\'quick-news\\' section.\", \"task_type\": \"web_scraping, html_parsing\"}\\n```\\n\\nThis way, the scraping and parsing will be done in a single task, reducing the need to repeat the web scraping process.\\n\\n\\n# Available Task Types\\n**web scraping**: Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \\n**text extractor**: Perform extraction on the \\'content\\' text using a large language model. \\n\\n# Task\\nBased on the user goal, write a plan or modify an existing plan of what you should do to achieve the goal. A plan consists of one to 20 tasks.\\nIf you are modifying an existing plan, carefully follow the instruction, don\\'t make unnecessary changes. Give the whole plan unless instructed to modify only one task of the plan.\\nIf you encounter errors on the current task, revise and output the current single task only.\\nOutput a list of jsons following the format:\\n```json\\n[\\n    {{\\n        \"task_id\": str = \"unique identifier for a task in plan, can be an ordinal\",\\n        \"dependent_task_ids\": list[str] = \"ids of tasks prerequisite to this task\",\\n        \"instruction\": \"what you should do in this task, one short phrase or sentence\",\\n        \"task_type\": \"type of this task, should be one of Available Task Types\",\\n    }},\\n    ...\\n]\\n```'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the user goal and the suggestion to combine the parsing step with the web scraping step, the updated plan is as follows:\n",
      "\n",
      "```json\n",
      "[{'task_id': '1', 'dependent_task_ids': [], 'instruction': 'Choose a programming language and set up the required tools for web scraping and text extraction.', 'task_type': 'setup'}, {'task_id': '2', 'dependent_task_ids': ['1'], 'instruction': 'Clone the repository or create a new project directory for the web scraping and text extraction tasks.', 'task_type': 'file_management'}, {'task_id': '3', 'dependent_task_ids': ['2'], 'instruction': 'Write a script using Playwright to scrape the HTML structure and inner text content of the specified URL (https://pitchhub.36kr.com/financing-flash) and parse the obtained HTML structure to extract the content within the \\'quick-news\\' section.', 'task_type': 'web_scraping, html_parsing'}, {'task_id': '4', 'dependent_task_ids': ['3'], 'instruction': \"Use a large language model to extract the '快讯' content from the parsed content.\", 'task_type': 'text_extractor'}, {'task_id': '5', 'dependent_task_ids': ['4'], 'instruction': \"Organize the extracted '快讯' content into a well-structured markdown table.\", 'task_type': 'text_processing'}, {'task_id': '6', 'dependent_task_ids': ['5'], 'instruction': \"Save the markdown table as a separate file named '快讯.md' in the project directory.\", 'task_type': 'file_management'}]\n",
      "```\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting property name enclosed in double quotes: line 1 column 3 (char 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 18\u001b[0m, in \u001b[0;36mparse_json\u001b[0;34m(rsp)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrsp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO: 修改成 plan_review.yaml 模板\u001b[39;00m\n\u001b[1;32m      2\u001b[0m last_plan \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m# Last plan\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m## Plan\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m```json\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mrsp\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m plan, raw_json \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_plan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgoal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_goal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguidance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_guidance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_plan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_plan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m pprint(plan\u001b[38;5;241m.\u001b[39mtasks)\n",
      "Cell \u001b[0;32mIn[22], line 39\u001b[0m, in \u001b[0;36mcreate_plan\u001b[0;34m(goal, guidance, last_plan)\u001b[0m\n\u001b[1;32m     36\u001b[0m plan\u001b[38;5;241m.\u001b[39mcontext \u001b[38;5;241m=\u001b[39m guidance\n\u001b[1;32m     37\u001b[0m rsp \u001b[38;5;241m=\u001b[39m llm_aask(msg\u001b[38;5;241m=\u001b[39mtemplate)\n\u001b[0;32m---> 39\u001b[0m tasks_json \u001b[38;5;241m=\u001b[39m \u001b[43mparse_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrsp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m tasks \u001b[38;5;241m=\u001b[39m [Task(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtask_config) \u001b[38;5;28;01mfor\u001b[39;00m task_config \u001b[38;5;129;01min\u001b[39;00m tasks_json]\n\u001b[1;32m     41\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(tasks)\n",
      "Cell \u001b[0;32mIn[22], line 21\u001b[0m, in \u001b[0;36mparse_json\u001b[0;34m(rsp)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     code_block \u001b[38;5;241m=\u001b[39m OutputParser\u001b[38;5;241m.\u001b[39mparse_code(rsp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode_block\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m objs\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 1 column 3 (char 2)"
     ]
    }
   ],
   "source": [
    "# TODO: 修改成 plan_review.yaml 模板\n",
    "last_plan = f\"\"\"# Last plan\n",
    "## Plan\n",
    "```json\n",
    "{raw_json}\n",
    "```\n",
    "\n",
    "## Review\n",
    "{rsp}\n",
    "\"\"\"\n",
    "\n",
    "plan, raw_json = create_plan(goal=user_goal, guidance=user_guidance, last_plan=last_plan)\n",
    "pprint(plan.tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff3222",
   "metadata": {},
   "source": [
    "## Tasks execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da32d977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">1 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">asyncio</span><span style=\"background-color: #2f1e2e\">                                                                                                 </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">2 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">nest_asyncio</span><span style=\"background-color: #2f1e2e\">                                                                                            </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">3 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">nest_asyncio</span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">.</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">apply()</span><span style=\"background-color: #2f1e2e\">                                                                                           </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">4 </span><span style=\"background-color: #2f1e2e\">                                                                                                               </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">5 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">from</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">metagpt.tools.libs.web_scraping</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> scrape_web_playwright</span><span style=\"background-color: #2f1e2e\">                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">6 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">from</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">tools.text_extractor.llm_extractor</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> llm_extractor</span><span style=\"background-color: #2f1e2e\">                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m1 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46masyncio\u001b[0m\u001b[48;2;47;30;46m                                                                                                 \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m2 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mnest_asyncio\u001b[0m\u001b[48;2;47;30;46m                                                                                            \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m3 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mnest_asyncio\u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m.\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mapply\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                                           \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m4 \u001b[0m\u001b[48;2;47;30;46m                                                                                                               \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m5 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mfrom\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mmetagpt\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mtools\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mlibs\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mweb_scraping\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mscrape_web_playwright\u001b[0m\u001b[48;2;47;30;46m                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m6 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mfrom\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mtools\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mtext_extractor\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mllm_extractor\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mllm_extractor\u001b[0m\u001b[48;2;47;30;46m                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from metagpt.actions.di.execute_nb_code import ExecuteNbCode\n",
    "\n",
    "pre_execute = \"\"\"import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\"\"\"\n",
    "\n",
    "# append imports\n",
    "for _, t in tools.items():\n",
    "    pre_execute += \"\\n\" + t[\"import\"]\n",
    "\n",
    "execute_code = ExecuteNbCode()\n",
    "result, success = await execute_code.run(pre_execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "819293bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 23:16:37.981 | DEBUG    | __main__:execute_task:9 - As an AI Engineer, you need to help user to achieve their goal step by step in a continuous Jupyter notebook.\n",
      "\n",
      "## Current Task\n",
      "Navigate to the website: pitchhub.36kr.com/financing-flash\n",
      "\n",
      "## Task Guidance\n",
      "Write complete code for 'Current Task'. And avoid duplicating code from 'Finished Tasks', such as repeated import of packages, reading data, etc.\n",
      "Specifically, 请只尽可能用提供的工具来简化你的工作，并用print打印当前任务的结果\n",
      "\n",
      "\n",
      "\n",
      "# Tool Info\n",
      "\n",
      "## Capabilities\n",
      "- You can utilize pre-defined tools in any code lines from 'Available Tools' in the form of Python class or function.\n",
      "- You can freely combine the use of any other public packages, like sklearn, numpy, pandas, etc..\n",
      "\n",
      "## Available Tools:\n",
      "Each tool is described in JSON format. All tools was import by default.\n",
      "{\"web scraping\": {\"type\": \"async_function\", \"description\": \"Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \", \"signature\": \"(url)\", \"parameters\": \"Args: url (str): The main URL to fetch inner text from. Returns: dict: The inner text content and html structure of the web page, keys are 'inner_text', 'html'.\", \"import\": \"from metagpt.tools.libs.web_scraping import scrape_web_playwright\"}}\n",
      "{\"text extractor\": {\"type\": \"async_function\", \"description\": \"Perform extraction on the 'content' text using a large language model. \", \"signature\": \"(guidance: str, content: str, format: str) -> str\", \"parameters\": \"Args: guidance (str): Guide the extraction process. content (str): The text content that needs to be extracted. format (str): The output format, can be \\\"json\\\" or \\\"markdown\\\". Returns: str: text extracted from content.\", \"import\": \"from tools.text_extractor.llm_extractor import llm_extractor\"}}\n",
      "\n",
      "# Constraints\n",
      "- Take on Current Task if it is in Plan Status, otherwise, tackle User Requirement directly.\n",
      "- Ensure the output new code is executable in the same Jupyter notebook as the previous executed code.\n",
      "- Always prioritize using pre-defined tools for the same functionality.\n",
      "\n",
      "# Output\n",
      "While some concise thoughts are helpful, code is absolutely required. Always output one and only one code block in your response. Output code in the following format:\n",
      "```python\n",
      "your code\n",
      "```\n",
      "\n",
      "Remember!! Since it is a notebook environment, don't use asyncio.run. Instead, use await if you need to call an async function.\n",
      "2024-04-08 23:16:37.982 | DEBUG    | metagpt.provider.base_llm:aask:149 - [{'role': 'user', 'content': 'As an AI Engineer, you need to help user to achieve their goal step by step in a continuous Jupyter notebook.\\n\\n## Current Task\\nNavigate to the website: pitchhub.36kr.com/financing-flash\\n\\n## Task Guidance\\nWrite complete code for \\'Current Task\\'. And avoid duplicating code from \\'Finished Tasks\\', such as repeated import of packages, reading data, etc.\\nSpecifically, 请只尽可能用提供的工具来简化你的工作，并用print打印当前任务的结果\\n\\n\\n\\n# Tool Info\\n\\n## Capabilities\\n- You can utilize pre-defined tools in any code lines from \\'Available Tools\\' in the form of Python class or function.\\n- You can freely combine the use of any other public packages, like sklearn, numpy, pandas, etc..\\n\\n## Available Tools:\\nEach tool is described in JSON format. All tools was import by default.\\n{\"web scraping\": {\"type\": \"async_function\", \"description\": \"Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \", \"signature\": \"(url)\", \"parameters\": \"Args: url (str): The main URL to fetch inner text from. Returns: dict: The inner text content and html structure of the web page, keys are \\'inner_text\\', \\'html\\'.\", \"import\": \"from metagpt.tools.libs.web_scraping import scrape_web_playwright\"}}\\n{\"text extractor\": {\"type\": \"async_function\", \"description\": \"Perform extraction on the \\'content\\' text using a large language model. \", \"signature\": \"(guidance: str, content: str, format: str) -> str\", \"parameters\": \"Args: guidance (str): Guide the extraction process. content (str): The text content that needs to be extracted. format (str): The output format, can be \\\\\"json\\\\\" or \\\\\"markdown\\\\\". Returns: str: text extracted from content.\", \"import\": \"from tools.text_extractor.llm_extractor import llm_extractor\"}}\\n\\n# Constraints\\n- Take on Current Task if it is in Plan Status, otherwise, tackle User Requirement directly.\\n- Ensure the output new code is executable in the same Jupyter notebook as the previous executed code.\\n- Always prioritize using pre-defined tools for the same functionality.\\n\\n# Output\\nWhile some concise thoughts are helpful, code is absolutely required. Always output one and only one code block in your response. Output code in the following format:\\n```python\\nyour code\\n```\\n\\nRemember!! Since it is a notebook environment, don\\'t use asyncio.run. Instead, use await if you need to call an async function.'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import requests\n",
      "import json\n",
      "\n",
      "# Navigate to pitchhub website\n",
      "url = \"pitchhub.36kr.com/financing-flash\"\n",
      "requests.get(url)\n",
      "\n",
      "# Extract the HTML content of the website\n",
      "html_content = requests.get(url).text\n",
      "\n",
      "# Parse the HTML content and extract the inner text\n",
      "inner_text = extract_inner_text(html_content)\n",
      "\n",
      "# Print the extracted inner text\n",
      "print(inner_text)\n",
      "```\n",
      "\n",
      "This code utilizes the `scrape_web_playwright` tool to scrape the website and the `extract_inner_text` tool to extract the inner text"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 23:16:42.274 | DEBUG    | __main__:execute_task:12 - ```python\n",
      "import requests\n",
      "import json\n",
      "\n",
      "# Navigate to pitchhub website\n",
      "url = \"pitchhub.36kr.com/financing-flash\"\n",
      "requests.get(url)\n",
      "\n",
      "# Extract the HTML content of the website\n",
      "html_content = requests.get(url).text\n",
      "\n",
      "# Parse the HTML content and extract the inner text\n",
      "inner_text = extract_inner_text(html_content)\n",
      "\n",
      "# Print the extracted inner text\n",
      "print(inner_text)\n",
      "```\n",
      "\n",
      "This code utilizes the `scrape_web_playwright` tool to scrape the website and the `extract_inner_text` tool to extract the inner text from the HTML content.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " from the HTML content.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 1 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">requests</span><span style=\"background-color: #2f1e2e\">                                                                                               </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 2 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">json</span><span style=\"background-color: #2f1e2e\">                                                                                                   </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 3 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 4 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Navigate to pitchhub website</span><span style=\"background-color: #2f1e2e\">                                                                                </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 5 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">url </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">\"pitchhub.36kr.com/financing-flash\"</span><span style=\"background-color: #2f1e2e\">                                                                     </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 6 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">requests</span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">.</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">get(url)</span><span style=\"background-color: #2f1e2e\">                                                                                             </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 7 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 8 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Extract the HTML content of the website</span><span style=\"background-color: #2f1e2e\">                                                                     </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 9 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">html_content </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> requests</span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">.</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">get(url)</span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">.</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">text</span><span style=\"background-color: #2f1e2e\">                                                                         </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">10 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">11 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Parse the HTML content and extract the inner text</span><span style=\"background-color: #2f1e2e\">                                                           </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">12 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">inner_text </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> extract_inner_text(html_content)</span><span style=\"background-color: #2f1e2e\">                                                                 </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">13 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">14 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Print the extracted inner text</span><span style=\"background-color: #2f1e2e\">                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">15 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">print(inner_text)</span><span style=\"background-color: #2f1e2e\">                                                                                             </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">16 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 1 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mrequests\u001b[0m\u001b[48;2;47;30;46m                                                                                               \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 2 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mjson\u001b[0m\u001b[48;2;47;30;46m                                                                                                   \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 3 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 4 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Navigate to pitchhub website\u001b[0m\u001b[48;2;47;30;46m                                                                                \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 5 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46mpitchhub.36kr.com/financing-flash\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[48;2;47;30;46m                                                                     \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 6 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mrequests\u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m.\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mget\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                                             \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 7 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 8 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Extract the HTML content of the website\u001b[0m\u001b[48;2;47;30;46m                                                                     \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 9 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mhtml_content\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mrequests\u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m.\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mget\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m.\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mtext\u001b[0m\u001b[48;2;47;30;46m                                                                         \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m10 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m11 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Parse the HTML content and extract the inner text\u001b[0m\u001b[48;2;47;30;46m                                                           \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m12 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46minner_text\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mextract_inner_text\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mhtml_content\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                 \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m13 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m14 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Print the extracted inner text\u001b[0m\u001b[48;2;47;30;46m                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m15 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mprint\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46minner_text\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                                             \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m16 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'import requests\\nimport json\\n\\n# Navigate to pitchhub website\\nurl = \"pitchhub.36kr.com/financing-flash\"\\nrequests.get(url)\\n\\n# Extract the HTML content of the website\\nhtml_content = requests.get(url).text\\n\\n# Parse the HTML content and extract the inner text\\ninner_text = extract_inner_text(html_content)\\n\\n# Print the extracted inner text\\nprint(inner_text)\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def execute_task(plan: Plan, plan_status=\"\", task_guidance=\"\"):\n",
    "    codegen_prompt = load_prompts(\"task_codegen\", \"task_codegen.yaml\")\n",
    "    template = codegen_prompt.format(\n",
    "        plan_status=plan_status,\n",
    "        current_task=plan.current_task.instruction,\n",
    "        task_guidance=task_guidance,\n",
    "        tools=tools_list,\n",
    "    )\n",
    "    logger.debug(template)\n",
    "\n",
    "    rsp = llm_aask(msg=template, seed=123)\n",
    "    logger.debug(rsp)\n",
    "\n",
    "    code_block = OutputParser.parse_code(rsp, \"python\")\n",
    "    execute_code._display(code_block, language=\"python\")\n",
    "    return code_block\n",
    "\n",
    "code = execute_task(plan, task_guidance=\"请只尽可能用提供的工具来简化你的工作，并用print打印当前任务的结果\")\n",
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cc75ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 1 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Navigate to pitchhub.36kr.com/financing-flash</span><span style=\"background-color: #2f1e2e\">                                                               </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 2 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 3 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">asyncio</span><span style=\"background-color: #2f1e2e\">                                                                                                </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 4 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">webbrowser</span><span style=\"background-color: #2f1e2e\">                                                                                             </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 5 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 6 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Open the website</span><span style=\"background-color: #2f1e2e\">                                                                                            </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 7 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">url </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">\"pitchhub.36kr.com/financing-flash\"</span><span style=\"background-color: #2f1e2e\">                                                                     </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 8 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">webbrowser</span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">.</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">open(url)</span><span style=\"background-color: #2f1e2e\">                                                                                          </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 9 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">10 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Scrape the website using Playwright</span><span style=\"background-color: #2f1e2e\">                                                                         </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">11 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">scrape_web_playwright(url)</span><span style=\"background-color: #2f1e2e\">                                                                                    </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">12 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">13 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Extract text from the website</span><span style=\"background-color: #2f1e2e\">                                                                               </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">14 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">extracted_text </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> llm_extractor(</span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">\"Guide me on extracting text from this website\"</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">, scrape_web_playwright(url)[</span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">\"in</span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">15 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">16 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Print the extracted text</span><span style=\"background-color: #2f1e2e\">                                                                                    </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">17 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">print(extracted_text)</span><span style=\"background-color: #2f1e2e\">                                                                                         </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">18 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 1 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Navigate to pitchhub.36kr.com/financing-flash\u001b[0m\u001b[48;2;47;30;46m                                                               \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 2 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 3 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46masyncio\u001b[0m\u001b[48;2;47;30;46m                                                                                                \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 4 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mwebbrowser\u001b[0m\u001b[48;2;47;30;46m                                                                                             \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 5 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 6 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Open the website\u001b[0m\u001b[48;2;47;30;46m                                                                                            \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 7 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46mpitchhub.36kr.com/financing-flash\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[48;2;47;30;46m                                                                     \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 8 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mwebbrowser\u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m.\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mopen\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                                          \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 9 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m10 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Scrape the website using Playwright\u001b[0m\u001b[48;2;47;30;46m                                                                         \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m11 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mscrape_web_playwright\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                                    \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m12 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m13 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Extract text from the website\u001b[0m\u001b[48;2;47;30;46m                                                                               \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m14 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mextracted_text\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mllm_extractor\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46mGuide me on extracting text from this website\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m,\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mscrape_web_playwright\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m[\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46min\u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m15 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m16 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Print the extracted text\u001b[0m\u001b[48;2;47;30;46m                                                                                    \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m17 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mprint\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mextracted_text\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                                         \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m18 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " '/tmp/ipykernel_52661/1322985112.py:11: RuntimeWarning: coroutine \\'scrape_web_playwright\\' was never awaited\\n  scrape_web_playwright(url)\\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\\n/tmp/ipykernel_52661/1322985112.py:14: RuntimeWarning: coroutine \\'scrape_web_playwright\\' was never awaited\\n  extracted_text = llm_extractor(\"Guide me on extracting text from this website\", scrape_web_playwright(url)[\"inner_text\"], \"json\")\\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\\n,---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\nCell In[2], line 14\\n     11 scrape_web_playwright(url)\\n     13 # Extract text from the website\\n---> 14 extracted_text = llm_extractor(\"Guide me on extracting text from this website\", scrape_web_playwright(url)[\"inner_text\"], \"json\")\\n     16 # Print the extracted text\\n     17 print(extracted_text)\\n\\nTypeError: \\'coroutine\\' object is not subscriptable')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result, success = await execute_code.run(code)\n",
    "success, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c9d0cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 23:10:18.010 | DEBUG    | __main__:execute_task:9 - As an AI Engineer, you need to help user to achieve their goal step by step in a continuous Jupyter notebook.\n",
      "\n",
      "## Current Task\n",
      "Navigate to the website: pitchhub.36kr.com/financing-flash\n",
      "\n",
      "## Task Guidance\n",
      "Write complete code for 'Current Task'. And avoid duplicating code from 'Finished Tasks', such as repeated import of packages, reading data, etc.\n",
      "Specifically, \n",
      "\n",
      "### Error\n",
      "```\n",
      "/tmp/ipykernel_52661/1322985112.py:11: RuntimeWarning: coroutine 'scrape_web_playwright' was never awaited\n",
      "  scrape_web_playwright(url)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/tmp/ipykernel_52661/1322985112.py:14: RuntimeWarning: coroutine 'scrape_web_playwright' was never awaited\n",
      "  extracted_text = llm_extractor(\"Guide me on extracting text from this website\", scrape_web_playwright(url)[\"inner_text\"], \"json\")\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      ",---------------------------------------------------------------------------\n",
      "TypeError                                 Traceback (most recent call last)\n",
      "Cell In[2], line 14\n",
      "     11 scrape_web_playwright(url)\n",
      "     13 # Extract text from the website\n",
      "---> 14 extracted_text = llm_extractor(\"Guide me on extracting text from this website\", scrape_web_playwright(url)[\"inner_text\"], \"json\")\n",
      "     16 # Print the extracted text\n",
      "     17 print(extracted_text)\n",
      "\n",
      "TypeError: 'coroutine' object is not subscriptable\n",
      "```\n",
      "\n",
      "### Previous code\n",
      "```python\n",
      "# Navigate to pitchhub.36kr.com/financing-flash\n",
      "\n",
      "import asyncio\n",
      "import webbrowser\n",
      "\n",
      "# Open the website\n",
      "url = \"pitchhub.36kr.com/financing-flash\"\n",
      "webbrowser.open(url)\n",
      "\n",
      "# Scrape the website using Playwright\n",
      "scrape_web_playwright(url)\n",
      "\n",
      "# Extract text from the website\n",
      "extracted_text = llm_extractor(\"Guide me on extracting text from this website\", scrape_web_playwright(url)[\"inner_text\"], \"json\")\n",
      "\n",
      "# Print the extracted text\n",
      "print(extracted_text)\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "# Tool Info\n",
      "\n",
      "## Capabilities\n",
      "- You can utilize pre-defined tools in any code lines from 'Available Tools' in the form of Python class or function.\n",
      "- You can freely combine the use of any other public packages, like sklearn, numpy, pandas, etc..\n",
      "\n",
      "## Available Tools:\n",
      "Each tool is described in JSON format. All tools was import by default.\n",
      "{\"web scraping\": {\"type\": \"async_function\", \"description\": \"Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \", \"signature\": \"(url)\", \"parameters\": \"Args: url (str): The main URL to fetch inner text from. Returns: dict: The inner text content and html structure of the web page, keys are 'inner_text', 'html'.\", \"import\": \"from metagpt.tools.libs.web_scraping import scrape_web_playwright\"}}\n",
      "{\"text extractor\": {\"type\": \"async_function\", \"description\": \"Perform extraction on the 'content' text using a large language model. \", \"signature\": \"(guidance: str, content: str, format: str) -> str\", \"parameters\": \"Args: guidance (str): Guide the extraction process. content (str): The text content that needs to be extracted. format (str): The output format, can be \\\"json\\\" or \\\"markdown\\\". Returns: str: text extracted from content.\", \"import\": \"from tools.text_extractor.llm_extractor import llm_extractor\"}}\n",
      "\n",
      "# Constraints\n",
      "- Take on Current Task if it is in Plan Status, otherwise, tackle User Requirement directly.\n",
      "- Ensure the output new code is executable in the same Jupyter notebook as the previous executed code.\n",
      "- Always prioritize using pre-defined tools for the same functionality.\n",
      "\n",
      "# Output\n",
      "While some concise thoughts are helpful, code is absolutely required. Always output one and only one code block in your response. Output code in the following format:\n",
      "```python\n",
      "your code\n",
      "```\n",
      "\n",
      "Remember!! Since it is a notebook environment, don't use asyncio.run. Instead, use await if you need to call an async function.\n",
      "2024-04-08 23:10:18.011 | DEBUG    | metagpt.provider.base_llm:aask:149 - [{'role': 'user', 'content': 'As an AI Engineer, you need to help user to achieve their goal step by step in a continuous Jupyter notebook.\\n\\n## Current Task\\nNavigate to the website: pitchhub.36kr.com/financing-flash\\n\\n## Task Guidance\\nWrite complete code for \\'Current Task\\'. And avoid duplicating code from \\'Finished Tasks\\', such as repeated import of packages, reading data, etc.\\nSpecifically, \\n\\n### Error\\n```\\n/tmp/ipykernel_52661/1322985112.py:11: RuntimeWarning: coroutine \\'scrape_web_playwright\\' was never awaited\\n  scrape_web_playwright(url)\\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\\n/tmp/ipykernel_52661/1322985112.py:14: RuntimeWarning: coroutine \\'scrape_web_playwright\\' was never awaited\\n  extracted_text = llm_extractor(\"Guide me on extracting text from this website\", scrape_web_playwright(url)[\"inner_text\"], \"json\")\\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\\n,---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\nCell In[2], line 14\\n     11 scrape_web_playwright(url)\\n     13 # Extract text from the website\\n---> 14 extracted_text = llm_extractor(\"Guide me on extracting text from this website\", scrape_web_playwright(url)[\"inner_text\"], \"json\")\\n     16 # Print the extracted text\\n     17 print(extracted_text)\\n\\nTypeError: \\'coroutine\\' object is not subscriptable\\n```\\n\\n### Previous code\\n```python\\n# Navigate to pitchhub.36kr.com/financing-flash\\n\\nimport asyncio\\nimport webbrowser\\n\\n# Open the website\\nurl = \"pitchhub.36kr.com/financing-flash\"\\nwebbrowser.open(url)\\n\\n# Scrape the website using Playwright\\nscrape_web_playwright(url)\\n\\n# Extract text from the website\\nextracted_text = llm_extractor(\"Guide me on extracting text from this website\", scrape_web_playwright(url)[\"inner_text\"], \"json\")\\n\\n# Print the extracted text\\nprint(extracted_text)\\n\\n```\\n\\n\\n# Tool Info\\n\\n## Capabilities\\n- You can utilize pre-defined tools in any code lines from \\'Available Tools\\' in the form of Python class or function.\\n- You can freely combine the use of any other public packages, like sklearn, numpy, pandas, etc..\\n\\n## Available Tools:\\nEach tool is described in JSON format. All tools was import by default.\\n{\"web scraping\": {\"type\": \"async_function\", \"description\": \"Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \", \"signature\": \"(url)\", \"parameters\": \"Args: url (str): The main URL to fetch inner text from. Returns: dict: The inner text content and html structure of the web page, keys are \\'inner_text\\', \\'html\\'.\", \"import\": \"from metagpt.tools.libs.web_scraping import scrape_web_playwright\"}}\\n{\"text extractor\": {\"type\": \"async_function\", \"description\": \"Perform extraction on the \\'content\\' text using a large language model. \", \"signature\": \"(guidance: str, content: str, format: str) -> str\", \"parameters\": \"Args: guidance (str): Guide the extraction process. content (str): The text content that needs to be extracted. format (str): The output format, can be \\\\\"json\\\\\" or \\\\\"markdown\\\\\". Returns: str: text extracted from content.\", \"import\": \"from tools.text_extractor.llm_extractor import llm_extractor\"}}\\n\\n# Constraints\\n- Take on Current Task if it is in Plan Status, otherwise, tackle User Requirement directly.\\n- Ensure the output new code is executable in the same Jupyter notebook as the previous executed code.\\n- Always prioritize using pre-defined tools for the same functionality.\\n\\n# Output\\nWhile some concise thoughts are helpful, code is absolutely required. Always output one and only one code block in your response. Output code in the following format:\\n```python\\nyour code\\n```\\n\\nRemember!! Since it is a notebook environment, don\\'t use asyncio.run. Instead, use await if you need to call an async function.'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "# Navigate to pitchhub.36kr.com/financing-flash\n",
      "\n",
      "import asyncio\n",
      "import webbrowser\n",
      "\n",
      "# Open the website\n",
      "url = \"pitchhub.36kr.com/financing-flash\"\n",
      "webbrowser.open(url)\n",
      "\n",
      "# Scrape the website using Playwright\n",
      "await scrape_web_playwright(url)\n",
      "\n",
      "# Extract text from the website\n",
      "extracted_text = llm_extractor(\"Guide me on extracting text from this website\", scrape_web_playwright(url)[\"inner_text\"], \"json\")\n",
      "\n",
      "# Print the extracted text\n",
      "print("
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 23:10:22.223 | DEBUG    | __main__:execute_task:12 - ```python\n",
      "# Navigate to pitchhub.36kr.com/financing-flash\n",
      "\n",
      "import asyncio\n",
      "import webbrowser\n",
      "\n",
      "# Open the website\n",
      "url = \"pitchhub.36kr.com/financing-flash\"\n",
      "webbrowser.open(url)\n",
      "\n",
      "# Scrape the website using Playwright\n",
      "await scrape_web_playwright(url)\n",
      "\n",
      "# Extract text from the website\n",
      "extracted_text = llm_extractor(\"Guide me on extracting text from this website\", scrape_web_playwright(url)[\"inner_text\"], \"json\")\n",
      "\n",
      "# Print the extracted text\n",
      "print(extracted_text)\n",
      "```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted_text)\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 1 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Navigate to pitchhub.36kr.com/financing-flash</span><span style=\"background-color: #2f1e2e\">                                                               </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 2 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 3 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">asyncio</span><span style=\"background-color: #2f1e2e\">                                                                                                </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 4 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">webbrowser</span><span style=\"background-color: #2f1e2e\">                                                                                             </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 5 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 6 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Open the website</span><span style=\"background-color: #2f1e2e\">                                                                                            </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 7 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">url </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">\"pitchhub.36kr.com/financing-flash\"</span><span style=\"background-color: #2f1e2e\">                                                                     </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 8 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">webbrowser</span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">.</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">open(url)</span><span style=\"background-color: #2f1e2e\">                                                                                          </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 9 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">10 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Scrape the website using Playwright</span><span style=\"background-color: #2f1e2e\">                                                                         </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">11 </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">await</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> scrape_web_playwright(url)</span><span style=\"background-color: #2f1e2e\">                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">12 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">13 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Extract text from the website</span><span style=\"background-color: #2f1e2e\">                                                                               </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">14 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">extracted_text </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> llm_extractor(</span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">\"Guide me on extracting text from this website\"</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">, scrape_web_playwright(url)[</span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">\"in</span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">15 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">16 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Print the extracted text</span><span style=\"background-color: #2f1e2e\">                                                                                    </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">17 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">print(extracted_text)</span><span style=\"background-color: #2f1e2e\">                                                                                         </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">18 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 1 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Navigate to pitchhub.36kr.com/financing-flash\u001b[0m\u001b[48;2;47;30;46m                                                               \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 2 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 3 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46masyncio\u001b[0m\u001b[48;2;47;30;46m                                                                                                \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 4 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mwebbrowser\u001b[0m\u001b[48;2;47;30;46m                                                                                             \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 5 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 6 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Open the website\u001b[0m\u001b[48;2;47;30;46m                                                                                            \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 7 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46mpitchhub.36kr.com/financing-flash\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[48;2;47;30;46m                                                                     \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 8 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mwebbrowser\u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m.\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mopen\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                                          \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 9 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m10 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Scrape the website using Playwright\u001b[0m\u001b[48;2;47;30;46m                                                                         \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m11 \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46mawait\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mscrape_web_playwright\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m12 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m13 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Extract text from the website\u001b[0m\u001b[48;2;47;30;46m                                                                               \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m14 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mextracted_text\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mllm_extractor\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46mGuide me on extracting text from this website\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m,\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mscrape_web_playwright\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m[\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46min\u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m15 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m16 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Print the extracted text\u001b[0m\u001b[48;2;47;30;46m                                                                                    \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m17 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mprint\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mextracted_text\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                                         \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m18 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'# Navigate to pitchhub.36kr.com/financing-flash\\n\\nimport asyncio\\nimport webbrowser\\n\\n# Open the website\\nurl = \"pitchhub.36kr.com/financing-flash\"\\nwebbrowser.open(url)\\n\\n# Scrape the website using Playwright\\nawait scrape_web_playwright(url)\\n\\n# Extract text from the website\\nextracted_text = llm_extractor(\"Guide me on extracting text from this website\", scrape_web_playwright(url)[\"inner_text\"], \"json\")\\n\\n# Print the extracted text\\nprint(extracted_text)\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status = f\"\"\"### Error\n",
    "```\n",
    "{result}\n",
    "```\n",
    "\n",
    "### Previous code\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "code = execute_task(plan, plan_status=status)\n",
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6b5cca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4373703ba144ce9a4a0c78c69c21733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "execute_code._display(result, language=\"markdown\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
