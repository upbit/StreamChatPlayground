{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba7766cf-75a4-4370-ba70-573e88e5d7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 19:23:24.313 | INFO     | metagpt.const:get_metagpt_package_root:29 - Package root set to /Users/deryzhou/Downloads/StreamChatPlayground/notebooks\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from utils.common import load_plaintext\n",
    "\n",
    "# debug level\n",
    "from metagpt.logs import logger\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"DEBUG\")\n",
    "\n",
    "# make asyncio.run() works in notebook\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51f34c9",
   "metadata": {},
   "source": [
    "## LLM 与 tools 准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06449968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 19:23:26.922 | DEBUG    | metagpt.provider.base_llm:aask:126 - [{'role': 'user', 'content': '你好'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好👋！我是人工智能助手智谱清言，可以叫我小智🤖，很高兴见到你，欢迎问我任何问题。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你好👋！我是人工智能助手智谱清言，可以叫我小智🤖，很高兴见到你，欢迎问我任何问题。'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ~/.metagpt/config2.yaml\n",
    "from metagpt.config2 import config\n",
    "from metagpt.provider.zhipuai_api import ZhiPuAILLM\n",
    "# from metagpt.utils.cost_manager import CostManager\n",
    "\n",
    "llm = ZhiPuAILLM(config.llm)\n",
    "llm.use_system_prompt = False # Disable default system message\n",
    "# llm.cost_manager = CostManager()\n",
    "\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache\n",
    "def llm_aask(msg):\n",
    "    return asyncio.run(llm.aask(msg=msg))\n",
    "\n",
    "llm_aask(\"你好\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bec60a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**web scraping**: Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \n",
      "**text extractor**: Perform extraction on the 'content' text using a large language model. \n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from metagpt.tools.tool_convert import function_docstring_to_schema\n",
    "from metagpt.tools.libs.web_scraping import scrape_web_playwright\n",
    "from tools.text_extractor.llm_extractor import llm_extractor\n",
    "\n",
    "def function_to_schema(func):\n",
    "    docstring = inspect.getdoc(func)\n",
    "    schema = function_docstring_to_schema(func, docstring)\n",
    "    schema[\"imports\"] = f\"from {func.__module__} import {func.__name__}\"\n",
    "    return schema\n",
    "\n",
    "DEF_TOOLS = [\n",
    "    (\"web scraping\", scrape_web_playwright),\n",
    "    (\"text extractor\", llm_extractor),\n",
    "]\n",
    "tools = {}\n",
    "for name, func in DEF_TOOLS:\n",
    "    schema = function_to_schema(func)\n",
    "    tools[name] = schema\n",
    "\n",
    "task_types = \"\\n\".join([\n",
    "    f\"**{k}**: {v['description']}\" for k,v in tools.items()\n",
    "])\n",
    "print(task_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7def00e",
   "metadata": {},
   "source": [
    "## Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23786658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 19:31:16.821 | DEBUG    | __main__:create_plan:33 - Respond to the human as helpfully and accurately as possible.\n",
      "\n",
      "# User goal\n",
      "抓取 https://pitchhub.36kr.com/financing-flash 中'快讯'的内容，并整理成markdown存档\n",
      "\n",
      "大概的流程：\n",
      "- 使用工具抓取网页中的可见文本\n",
      "- 提取网页文本中'快讯'相关的内容。注意网页中可能包含导航，只需要抽取'快讯'的具体内容\n",
      "- 对抽取结果进行归类，并保存成markdown表格: 快讯.md\n",
      "\n",
      "# Available Task Types:\n",
      "**web scraping**: Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \n",
      "**text extractor**: Perform extraction on the 'content' text using a large language model. \n",
      "\n",
      "# Task:\n",
      "Based on the user goal, write a plan or modify an existing plan of what you should do to achieve the goal. A plan consists of one to 20 tasks.\n",
      "If you are modifying an existing plan, carefully follow the instruction, don't make unnecessary changes. Give the whole plan unless instructed to modify only one task of the plan.\n",
      "If you encounter errors on the current task, revise and output the current single task only.\n",
      "Output a list of jsons following the format:\n",
      "```json\n",
      "[\n",
      "    {{\n",
      "        \"task_id\": str = \"unique identifier for a task in plan, can be an ordinal\",\n",
      "        \"dependent_task_ids\": list[str] = \"ids of tasks prerequisite to this task\",\n",
      "        \"instruction\": \"what you should do in this task, one short phrase or sentence\",\n",
      "        \"task_type\": \"type of this task, should be one of Available Task Types\",\n",
      "    }},\n",
      "    ...\n",
      "]\n",
      "```\n",
      "2024-03-21 19:31:16.822 | DEBUG    | __main__:create_plan:40 - [Task(task_id='1', dependent_task_ids=[], instruction='Scrape the HTML and inner text content of the webpage.', task_type='web scraping', code='', result='', is_success=False, is_finished=False), Task(task_id='2', dependent_task_ids=['1'], instruction='Extract the visible text content from the scraped HTML.', task_type='web scraping', code='', result='', is_success=False, is_finished=False), Task(task_id='3', dependent_task_ids=['2'], instruction=\"Identify and extract sections of the text containing the keyword '快讯'.\", task_type='text extractor', code='', result='', is_success=False, is_finished=False), Task(task_id='4', dependent_task_ids=['3'], instruction='Clean up the extracted text to remove any navigation or non-essential content.', task_type='text extractor', code='', result='', is_success=False, is_finished=False), Task(task_id='5', dependent_task_ids=['4'], instruction='Organize the extracted content into a structured format suitable for Markdown.', task_type='text extractor', code='', result='', is_success=False, is_finished=False), Task(task_id='6', dependent_task_ids=['5'], instruction=\"Save the structured content into a Markdown file named '快讯.md'.\", task_type='web scraping', code='', result='', is_success=False, is_finished=False)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Plan(goal=\"抓取 https://pitchhub.36kr.com/financing-flash 中'快讯'的内容，并整理成markdown存档\", context='', tasks=[Task(task_id='1', dependent_task_ids=[], instruction='Scrape the HTML and inner text content of the webpage.', task_type='web scraping', code='', result='', is_success=False, is_finished=False), Task(task_id='2', dependent_task_ids=['1'], instruction='Extract the visible text content from the scraped HTML.', task_type='web scraping', code='', result='', is_success=False, is_finished=False), Task(task_id='3', dependent_task_ids=['2'], instruction=\"Identify and extract sections of the text containing the keyword '快讯'.\", task_type='text extractor', code='', result='', is_success=False, is_finished=False), Task(task_id='4', dependent_task_ids=['3'], instruction='Clean up the extracted text to remove any navigation or non-essential content.', task_type='text extractor', code='', result='', is_success=False, is_finished=False), Task(task_id='5', dependent_task_ids=['4'], instruction='Organize the extracted content into a structured format suitable for Markdown.', task_type='text extractor', code='', result='', is_success=False, is_finished=False), Task(task_id='6', dependent_task_ids=['5'], instruction=\"Save the structured content into a Markdown file named '快讯.md'.\", task_type='web scraping', code='', result='', is_success=False, is_finished=False)], task_map={'1': Task(task_id='1', dependent_task_ids=[], instruction='Scrape the HTML and inner text content of the webpage.', task_type='web scraping', code='', result='', is_success=False, is_finished=False), '2': Task(task_id='2', dependent_task_ids=['1'], instruction='Extract the visible text content from the scraped HTML.', task_type='web scraping', code='', result='', is_success=False, is_finished=False), '3': Task(task_id='3', dependent_task_ids=['2'], instruction=\"Identify and extract sections of the text containing the keyword '快讯'.\", task_type='text extractor', code='', result='', is_success=False, is_finished=False), '4': Task(task_id='4', dependent_task_ids=['3'], instruction='Clean up the extracted text to remove any navigation or non-essential content.', task_type='text extractor', code='', result='', is_success=False, is_finished=False), '5': Task(task_id='5', dependent_task_ids=['4'], instruction='Organize the extracted content into a structured format suitable for Markdown.', task_type='text extractor', code='', result='', is_success=False, is_finished=False), '6': Task(task_id='6', dependent_task_ids=['5'], instruction=\"Save the structured content into a Markdown file named '快讯.md'.\", task_type='web scraping', code='', result='', is_success=False, is_finished=False)}, current_task_id='1')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain.prompts import PromptTemplate\n",
    "from metagpt.schema import Plan, Task\n",
    "from metagpt.utils.common import OutputParser\n",
    "\n",
    "\n",
    "def load_prompts(path: str) -> PromptTemplate:\n",
    "    base_path = os.path.join(\"prompts\", path)\n",
    "    output_format = load_plaintext(base_path, \"output.md\")\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[],\n",
    "        template=load_plaintext(base_path, \"template.yaml\"),\n",
    "    )\n",
    "    return prompt.partial(output=output_format)\n",
    "\n",
    "def parse_json(rsp):\n",
    "    try:\n",
    "        objs = json.loads(rsp)\n",
    "    except:\n",
    "        code_block = OutputParser.parse_code(rsp, \"json\")\n",
    "        objs = json.loads(code_block)\n",
    "    return objs\n",
    "\n",
    "def create_plan(goal, guidance):\n",
    "    plan_prompt = load_prompts(\"planning\")\n",
    "    template = plan_prompt.format(\n",
    "        goal=goal,\n",
    "        user_guidance=guidance,\n",
    "        task_types=task_types,\n",
    "        max_tasks=20,\n",
    "    )\n",
    "    logger.debug(template)\n",
    "\n",
    "    plan = Plan(goal=goal)\n",
    "    rsp = llm_aask(msg=template)\n",
    "\n",
    "    tasks_json = parse_json(rsp)\n",
    "    tasks = [Task(**task_config) for task_config in tasks_json]\n",
    "    logger.debug(tasks)\n",
    "\n",
    "    plan.add_tasks(tasks)\n",
    "    return plan\n",
    "\n",
    "\n",
    "user_goal = \"抓取 https://pitchhub.36kr.com/financing-flash 中'快讯'的内容，并整理成markdown存档\"\n",
    "user_guidance = \"\"\"大概的流程：\n",
    "- 使用工具抓取网页中的可见文本\n",
    "- 提取网页文本中'快讯'相关的内容。注意网页中可能包含导航，只需要抽取'快讯'的具体内容\n",
    "- 对抽取结果进行归类，并保存成markdown表格: 快讯.md\"\"\"\n",
    "\n",
    "plan = create_plan(goal=user_goal, guidance=user_guidance)\n",
    "plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a73f9842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task(task_id='1', dependent_task_ids=[], instruction='Scrape the HTML and inner text content of the webpage.', task_type='web scraping', code='', result='', is_success=False, is_finished=False),\n",
      " Task(task_id='2', dependent_task_ids=['1'], instruction='Extract the visible text content from the scraped HTML.', task_type='web scraping', code='', result='', is_success=False, is_finished=False),\n",
      " Task(task_id='3', dependent_task_ids=['2'], instruction=\"Identify and extract sections of the text containing the keyword '快讯'.\", task_type='text extractor', code='', result='', is_success=False, is_finished=False),\n",
      " Task(task_id='4', dependent_task_ids=['3'], instruction='Clean up the extracted text to remove any navigation or non-essential content.', task_type='text extractor', code='', result='', is_success=False, is_finished=False),\n",
      " Task(task_id='5', dependent_task_ids=['4'], instruction='Organize the extracted content into a structured format suitable for Markdown.', task_type='text extractor', code='', result='', is_success=False, is_finished=False),\n",
      " Task(task_id='6', dependent_task_ids=['5'], instruction=\"Save the structured content into a Markdown file named '快讯.md'.\", task_type='web scraping', code='', result='', is_success=False, is_finished=False)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(plan.tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669eb4e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
