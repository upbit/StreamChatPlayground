{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba7766cf-75a4-4370-ba70-573e88e5d7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 19:52:21.098 | INFO     | metagpt.const:get_metagpt_package_root:29 - Package root set to /Users/deryzhou/Downloads/StreamChatPlayground/notebooks\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from utils.common import load_plaintext\n",
    "\n",
    "# debug level\n",
    "from metagpt.logs import logger\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"DEBUG\")\n",
    "\n",
    "# make asyncio.run() works in notebook\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51f34c9",
   "metadata": {},
   "source": [
    "## LLM 与 tools 准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f19600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 智普AI\n",
    "# ~/.metagpt/config2.yaml\n",
    "from metagpt.config2 import config\n",
    "from metagpt.provider.zhipuai_api import ZhiPuAILLM\n",
    "# from metagpt.utils.cost_manager import CostManager\n",
    "\n",
    "llm = ZhiPuAILLM(config.llm)\n",
    "llm.use_system_prompt = False # Disable default system message\n",
    "# llm.cost_manager = CostManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01ead1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. mistral-7b-instruct-v0.2 (llamacpp)\n"
     ]
    }
   ],
   "source": [
    "# 本地 OpenAI like (vllm/llama.cpp)\n",
    "import yaml\n",
    "from metagpt.configs.llm_config import LLMConfig\n",
    "from metagpt.provider.openai_api import OpenAILLM\n",
    "# from metagpt.utils.cost_manager import CostManager\n",
    "\n",
    "llm_configs = yaml.safe_load(load_plaintext(\"../\", \"vllm_local.yaml\"))\n",
    "llm_config = LLMConfig.model_validate(llm_configs['llm'])\n",
    "llm = OpenAILLM(llm_config)\n",
    "llm.use_system_prompt = False # Disable default system message\n",
    "# llm.cost_manager = CostManager()\n",
    "\n",
    "models = await llm.aclient.models.list()\n",
    "for idx, mod in enumerate(models.data):\n",
    "    print(f\"{idx+1}. {mod.id} ({mod.owned_by})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b375fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混元\n",
    "import yaml\n",
    "from metagpt.configs.llm_config import LLMConfig\n",
    "from provider.hunyuan_api import HunyuanAPI\n",
    "\n",
    "llm_configs = yaml.safe_load(load_plaintext(\"../\", \"hunyuan.yaml\"))\n",
    "llm_config = LLMConfig.model_validate(llm_configs['hunyuan'])\n",
    "llm = HunyuanAPI(llm_config, model=\"7b-code-sft-deryzhou\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06449968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 19:52:25.628 | DEBUG    | metagpt.provider.base_llm:aask:149 - [{'role': 'user', 'content': '你好，介绍下你自己'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 你好！我是一名英语语言模型，我可以帮您生成和翻译文本。我可以回答问题，写短文本，翻译单词或短句，甚至是整个文章。我可以学习新知识和词汇，以适应您的需求。请告诉我如何帮您，我一定会尽力满足您的期望。\n",
      "\n",
      "Hello! I am an English language model. I can help you generate and translate text. I can answer questions, write short texts, translate single words or short sentences, even whole articles. I can learn new knowledge and vocabulary to suit your needs. Please tell me how I can help you, I will do my best to meet your expectations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 你好！我是一名英语语言模型，我可以帮您生成和翻译文本。我可以回答问题，写短文本，翻译单词或短句，甚至是整个文章。我可以学习新知识和词汇，以适应您的需求。请告诉我如何帮您，我一定会尽力满足您的期望。\\n\\nHello! I am an English language model. I can help you generate and translate text. I can answer questions, write short texts, translate single words or short sentences, even whole articles. I can learn new knowledge and vocabulary to suit your needs. Please tell me how I can help you, I will do my best to meet your expectations.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache\n",
    "def llm_aask(msg, seed=None):\n",
    "    return asyncio.run(llm.aask(msg=msg))\n",
    "\n",
    "llm_aask(\"你好，介绍下你自己\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bec60a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**web scraping**: Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \n",
      "**text extractor**: Perform extraction on the 'content' text using a large language model. \n",
      "\n",
      "{\"web scraping\": {\"type\": \"async_function\", \"description\": \"Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \", \"signature\": \"(url)\", \"parameters\": \"Args: url (str): The main URL to fetch inner text from. Returns: dict: The inner text content and html structure of the web page, keys are 'inner_text', 'html'.\", \"import\": \"from metagpt.tools.libs.web_scraping import scrape_web_playwright\"}}\n",
      "{\"text extractor\": {\"type\": \"async_function\", \"description\": \"Perform extraction on the 'content' text using a large language model. \", \"signature\": \"(guidance: str, content: str, format: str) -> str\", \"parameters\": \"Args: guidance (str): Guide the extraction process. content (str): The text content that needs to be extracted. format (str): The output format, can be \\\"json\\\" or \\\"markdown\\\". Returns: str: text extracted from content.\", \"import\": \"from tools.text_extractor.llm_extractor import llm_extractor\"}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import inspect\n",
    "from metagpt.tools.tool_convert import function_docstring_to_schema\n",
    "from metagpt.tools.libs.web_scraping import scrape_web_playwright\n",
    "from tools.text_extractor.llm_extractor import llm_extractor\n",
    "\n",
    "def function_to_schema(func):\n",
    "    docstring = inspect.getdoc(func)\n",
    "    schema = function_docstring_to_schema(func, docstring)\n",
    "    schema[\"import\"] = f\"from {func.__module__} import {func.__name__}\"\n",
    "    return schema\n",
    "\n",
    "DEF_TOOLS = [\n",
    "    (\"web scraping\", scrape_web_playwright),\n",
    "    (\"text extractor\", llm_extractor),\n",
    "]\n",
    "tools = {}\n",
    "for name, func in DEF_TOOLS:\n",
    "    schema = function_to_schema(func)\n",
    "    tools[name] = schema\n",
    "tools_list = \"\\n\".join([ json.dumps({k:v}) for k,v in tools.items() ])\n",
    "\n",
    "task_types = \"\\n\".join([\n",
    "    f\"**{k}**: {v['description']}\" for k,v in tools.items()\n",
    "])\n",
    "print(task_types + \"\\n\")\n",
    "print(tools_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7def00e",
   "metadata": {},
   "source": [
    "## Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23786658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 19:53:27.586 | DEBUG    | __main__:create_plan:33 - Respond to the human as helpfully and accurately as possible.\n",
      "\n",
      "# User goal\n",
      "抓取 https://pitchhub.36kr.com/financing-flash 中'快讯'的内容，并整理成markdown存档\n",
      "\n",
      "# 可能的流程\n",
      "- 使用工具抓取网页中的可见文本\n",
      "- 提取网页文本中'快讯'相关的内容。注意网页中可能包含导航，只需要抽取'快讯'的具体内容\n",
      "- 对抽取结果进行归类，并保存成markdown表格: 快讯.md\n",
      "\n",
      "\n",
      "\n",
      "# Available Task Types\n",
      "**web scraping**: Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \n",
      "**text extractor**: Perform extraction on the 'content' text using a large language model. \n",
      "\n",
      "# Task\n",
      "Based on the user goal, write a plan or modify an existing plan of what you should do to achieve the goal. A plan consists of one to 20 tasks.\n",
      "If you are modifying an existing plan, carefully follow the instruction, don't make unnecessary changes. Give the whole plan unless instructed to modify only one task of the plan.\n",
      "If you encounter errors on the current task, revise and output the current single task only.\n",
      "Output a list of jsons following the format:\n",
      "```json\n",
      "[\n",
      "    {{\n",
      "        \"task_id\": str = \"unique identifier for a task in plan, can be an ordinal\",\n",
      "        \"dependent_task_ids\": list[str] = \"ids of tasks prerequisite to this task\",\n",
      "        \"instruction\": \"what you should do in this task, one short phrase or sentence\",\n",
      "        \"task_type\": \"type of this task, should be one of Available Task Types\",\n",
      "    }},\n",
      "    ...\n",
      "]\n",
      "```\n",
      "2024-04-10 19:53:27.587 | DEBUG    | metagpt.provider.base_llm:aask:149 - [{'role': 'user', 'content': 'Respond to the human as helpfully and accurately as possible.\\n\\n# User goal\\n抓取 https://pitchhub.36kr.com/financing-flash 中\\'快讯\\'的内容，并整理成markdown存档\\n\\n# 可能的流程\\n- 使用工具抓取网页中的可见文本\\n- 提取网页文本中\\'快讯\\'相关的内容。注意网页中可能包含导航，只需要抽取\\'快讯\\'的具体内容\\n- 对抽取结果进行归类，并保存成markdown表格: 快讯.md\\n\\n\\n\\n# Available Task Types\\n**web scraping**: Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \\n**text extractor**: Perform extraction on the \\'content\\' text using a large language model. \\n\\n# Task\\nBased on the user goal, write a plan or modify an existing plan of what you should do to achieve the goal. A plan consists of one to 20 tasks.\\nIf you are modifying an existing plan, carefully follow the instruction, don\\'t make unnecessary changes. Give the whole plan unless instructed to modify only one task of the plan.\\nIf you encounter errors on the current task, revise and output the current single task only.\\nOutput a list of jsons following the format:\\n```json\\n[\\n    {{\\n        \"task_id\": str = \"unique identifier for a task in plan, can be an ordinal\",\\n        \"dependent_task_ids\": list[str] = \"ids of tasks prerequisite to this task\",\\n        \"instruction\": \"what you should do in this task, one short phrase or sentence\",\\n        \"task_type\": \"type of this task, should be one of Available Task Types\",\\n    }},\\n    ...\\n]\\n```'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\n",
      "  {\n",
      "    \"task_id\": \"1\",\n",
      "    \"dependent_task_ids\": [],\n",
      "    \"instruction\": \"Use Playwright to scrape the HTML content of the webpage https://pitchhub.36kr.com/financing-flash\",\n",
      "    \"task_type\": \"Scrape_HTML\"\n",
      "  },\n",
      "  {\n",
      "    \"task_id\": \"2\",\n",
      "    \"dependent_task_ids\": [\"1\"],\n",
      "    \"instruction\": \"Extract the text content of the webpage using extracted HTML\",\n",
      "    \"task_type\": \"Extract_Text\"\n",
      "  },\n",
      "  {\n",
      "    \"task_id\": \"3\",\n",
      "    \"dependent_task_ids\": [\"2\"],\n",
      "    \"instruction\": \"Parse the extracted text to identify the desired information\",\n",
      "    \"task_type\": \"Parse_Text\"\n",
      "  },\n",
      "  {\n",
      "    \"task_id\": \"4\",\n",
      "    \"dependent_task_ids\": [\"3\"],\n",
      "    \"instruction\": \"Store the identified information in a variable for further use\",\n",
      "    \"task_type\": \"Store_Information\"\n",
      "  }"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 19:53:39.134 | DEBUG    | __main__:create_plan:41 - [Task(task_id='1', dependent_task_ids=[], instruction='Use Playwright to scrape the HTML content of the webpage https://pitchhub.36kr.com/financing-flash', task_type='Scrape_HTML', code='', result='', is_success=False, is_finished=False), Task(task_id='2', dependent_task_ids=['1'], instruction='Extract the text content of the webpage using extracted HTML', task_type='Extract_Text', code='', result='', is_success=False, is_finished=False), Task(task_id='3', dependent_task_ids=['2'], instruction='Parse the extracted text to identify the desired information', task_type='Parse_Text', code='', result='', is_success=False, is_finished=False), Task(task_id='4', dependent_task_ids=['3'], instruction='Store the identified information in a variable for further use', task_type='Store_Information', code='', result='', is_success=False, is_finished=False)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "]\n",
      "[Task(task_id='1', dependent_task_ids=[], instruction='Use Playwright to scrape the HTML content of the webpage https://pitchhub.36kr.com/financing-flash', task_type='Scrape_HTML', code='', result='', is_success=False, is_finished=False),\n",
      " Task(task_id='2', dependent_task_ids=['1'], instruction='Extract the text content of the webpage using extracted HTML', task_type='Extract_Text', code='', result='', is_success=False, is_finished=False),\n",
      " Task(task_id='3', dependent_task_ids=['2'], instruction='Parse the extracted text to identify the desired information', task_type='Parse_Text', code='', result='', is_success=False, is_finished=False),\n",
      " Task(task_id='4', dependent_task_ids=['3'], instruction='Store the identified information in a variable for further use', task_type='Store_Information', code='', result='', is_success=False, is_finished=False)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "from metagpt.schema import Plan, Task\n",
    "from metagpt.utils.common import OutputParser\n",
    "from pprint import pprint # debug\n",
    "\n",
    "def load_prompts(path: str, filename: str) -> PromptTemplate:\n",
    "    base_path = os.path.join(\"prompts\", path)\n",
    "    output_format = load_plaintext(base_path, \"output.md\")\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[],\n",
    "        template=load_plaintext(base_path, filename),\n",
    "    )\n",
    "    return prompt.partial(output=output_format)\n",
    "\n",
    "def parse_json(rsp):\n",
    "    try:\n",
    "        objs = json.loads(rsp)\n",
    "    except:\n",
    "        code_block = OutputParser.parse_code(rsp, \"json\")\n",
    "        objs = json.loads(code_block)\n",
    "    return objs\n",
    "\n",
    "def create_plan(goal, guidance, last_plan=\"\"):\n",
    "    plan_prompt = load_prompts(\"planning\", \"planning.yaml\")\n",
    "    template = plan_prompt.format(\n",
    "        goal=goal,\n",
    "        user_guidance=guidance,\n",
    "        last_plan=last_plan,\n",
    "        task_types=task_types,\n",
    "        max_tasks=20,\n",
    "    )\n",
    "    logger.debug(template)\n",
    "\n",
    "    plan = Plan(goal=goal)\n",
    "    plan.context = guidance\n",
    "    rsp = llm_aask(msg=template)\n",
    "\n",
    "    tasks_json = parse_json(rsp)\n",
    "    tasks = [Task(**task_config) for task_config in tasks_json]\n",
    "    logger.debug(tasks)\n",
    "\n",
    "    plan.add_tasks(tasks)\n",
    "    return plan, tasks_json\n",
    "\n",
    "\n",
    "user_goal = \"抓取 https://pitchhub.36kr.com/financing-flash 中'快讯'的内容，并整理成markdown存档\"\n",
    "user_guidance = \"\"\"# 可能的流程\n",
    "- 使用工具抓取网页中的可见文本\n",
    "- 提取网页文本中'快讯'相关的内容。注意网页中可能包含导航，只需要抽取'快讯'的具体内容\n",
    "- 对抽取结果进行归类，并保存成markdown表格: 快讯.md\"\"\"\n",
    "\n",
    "plan, raw_json = create_plan(goal=user_goal, guidance=user_guidance)\n",
    "pprint(plan.tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a7cfcc",
   "metadata": {},
   "source": [
    "### Plan Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "669eb4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 21:48:46.003 | DEBUG    | metagpt.provider.base_llm:aask:149 - [{'role': 'user', 'content': 'Review the plan and determine if the plan can achieve the user goal.\\n\\n# User Goal\\n抓取 https://pitchhub.36kr.com/financing-flash 中\\'快讯\\'的内容，并整理成markdown存档\\n\\n# Plan\\n```json\\n[{\\'task_id\\': \\'1\\', \\'dependent_task_ids\\': [], \\'instruction\\': \\'Choose a programming language and set up the required tools for web scraping and text extraction.\\', \\'task_type\\': \\'setup\\'}, {\\'task_id\\': \\'2\\', \\'dependent_task_ids\\': [\\'1\\'], \\'instruction\\': \\'Clone the repository or create a new project directory for the web scraping and text extraction tasks.\\', \\'task_type\\': \\'file_management\\'}, {\\'task_id\\': \\'3\\', \\'dependent_task_ids\\': [\\'2\\'], \\'instruction\\': \\'Write a script using Playwright to scrape the HTML structure and inner text content of the specified URL (https://pitchhub.36kr.com/financing-flash).\\', \\'task_type\\': \\'web_scraping\\'}, {\\'task_id\\': \\'4\\', \\'dependent_task_ids\\': [\\'3\\'], \\'instruction\\': \"Parse the obtained HTML structure to extract the content within the \\'quick-news\\' section.\", \\'task_type\\': \\'html_parsing\\'}, {\\'task_id\\': \\'5\\', \\'dependent_task_ids\\': [\\'4\\'], \\'instruction\\': \"Use a large language model to extract the \\'快讯\\' content from the parsed content.\", \\'task_type\\': \\'text_extractor\\'}, {\\'task_id\\': \\'6\\', \\'dependent_task_ids\\': [\\'5\\'], \\'instruction\\': \"Organize the extracted \\'快讯\\' content into a well-structured markdown table.\", \\'task_type\\': \\'text_processing\\'}, {\\'task_id\\': \\'7\\', \\'dependent_task_ids\\': [\\'6\\'], \\'instruction\\': \"Save the markdown table as a separate file named \\'快讯.md\\' in the project directory.\", \\'task_type\\': \\'file_management\\'}]\\n```\\n\\n# Output\\nOutput a list of jsons following the format:\\n```json\\n[\\n    {\\n        \"task_id\": str = \"unique identifier for a task in plan, can be an ordinal\",\\n        \"dependent_task_ids\": list[str] = \"ids of tasks prerequisite to this task\",\\n        \"instruction\": \"what you should do in this task, one short phrase or sentence\",\\n        \"task_type\": \"type of this task, should be one of Available Task Types\",\\n    },\\n    ...\\n]\\n```\\n\\n# Constraint\\n- The breakdown of the plan is clear enough, and each task has single goal and easy to complete.\\n- Use the provided task types whenever possible to avoid unnecessary steps.\\n- Check whether the plan format meets the output requirements.\\n\\n**REMEMBER**: Only output the modifiy suggestions in plain text, for instruction on how to adjust the plan.\\n'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The plan seems to be properly set up for achieving the user goal. The tasks are well-defined, and each step logically follows the previous one. The use of the provided task types also makes it easier to understand the purpose of each task. The plan format meets the output requirements.\n",
      "\n",
      "However, there's one suggestion to make the plan more efficient:\n",
      "\n",
      "Instruction for task 4 could be combined with task 3 to save time and resources:\n",
      "\n",
      "```json\n",
      "- Replace task 4 with:\n",
      "  {\"task_id\": \"3\", \"dependent_task_ids\": [\"2\"], \"instruction\": \"Write a script using Playwright to scrape the HTML structure and inner text content of the specified URL (https://pitchhub.36kr.com/financing-flash), and parse the obtained HTML structure to extract the content within the 'quick-news' section.\", \"task_type\": \"web_scraping, html_parsing\"}\n",
      "```\n",
      "\n",
      "This way, the scraping and parsing will be done in a single task, reducing the need to repeat the web scraping process.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The plan seems to be properly set up for achieving the user goal. The tasks are well-defined, and each step logically follows the previous one. The use of the provided task types also makes it easier to understand the purpose of each task. The plan format meets the output requirements.\\n\\nHowever, there\\'s one suggestion to make the plan more efficient:\\n\\nInstruction for task 4 could be combined with task 3 to save time and resources:\\n\\n```json\\n- Replace task 4 with:\\n  {\"task_id\": \"3\", \"dependent_task_ids\": [\"2\"], \"instruction\": \"Write a script using Playwright to scrape the HTML structure and inner text content of the specified URL (https://pitchhub.36kr.com/financing-flash), and parse the obtained HTML structure to extract the content within the \\'quick-news\\' section.\", \"task_type\": \"web_scraping, html_parsing\"}\\n```\\n\\nThis way, the scraping and parsing will be done in a single task, reducing the need to repeat the web scraping process.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_prompt = load_prompts(\"planning\", \"review.yaml\")\n",
    "template = review_prompt.format(\n",
    "    goal=user_goal,\n",
    "    user_guidance=user_guidance,\n",
    "    task_types=task_types,\n",
    "    content=raw_json,\n",
    ")\n",
    "# logger.debug(template)\n",
    "\n",
    "rsp = llm_aask(msg=template)\n",
    "rsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0af7fd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 21:48:52.022 | DEBUG    | __main__:create_plan:33 - Respond to the human as helpfully and accurately as possible.\n",
      "\n",
      "# User goal\n",
      "抓取 https://pitchhub.36kr.com/financing-flash 中'快讯'的内容，并整理成markdown存档\n",
      "\n",
      "大概的流程：\n",
      "- 使用工具抓取网页中的可见文本\n",
      "- 提取网页文本中'快讯'相关的内容。注意网页中可能包含导航，只需要抽取'快讯'的具体内容\n",
      "- 对抽取结果进行归类，并保存成markdown表格: 快讯.md\n",
      "\n",
      "# Last plan\n",
      "## Plan\n",
      "```json\n",
      "[{'task_id': '1', 'dependent_task_ids': [], 'instruction': 'Choose a programming language and set up the required tools for web scraping and text extraction.', 'task_type': 'setup'}, {'task_id': '2', 'dependent_task_ids': ['1'], 'instruction': 'Clone the repository or create a new project directory for the web scraping and text extraction tasks.', 'task_type': 'file_management'}, {'task_id': '3', 'dependent_task_ids': ['2'], 'instruction': 'Write a script using Playwright to scrape the HTML structure and inner text content of the specified URL (https://pitchhub.36kr.com/financing-flash).', 'task_type': 'web_scraping'}, {'task_id': '4', 'dependent_task_ids': ['3'], 'instruction': \"Parse the obtained HTML structure to extract the content within the 'quick-news' section.\", 'task_type': 'html_parsing'}, {'task_id': '5', 'dependent_task_ids': ['4'], 'instruction': \"Use a large language model to extract the '快讯' content from the parsed content.\", 'task_type': 'text_extractor'}, {'task_id': '6', 'dependent_task_ids': ['5'], 'instruction': \"Organize the extracted '快讯' content into a well-structured markdown table.\", 'task_type': 'text_processing'}, {'task_id': '7', 'dependent_task_ids': ['6'], 'instruction': \"Save the markdown table as a separate file named '快讯.md' in the project directory.\", 'task_type': 'file_management'}]\n",
      "```\n",
      "\n",
      "## Review\n",
      "The plan seems to be properly set up for achieving the user goal. The tasks are well-defined, and each step logically follows the previous one. The use of the provided task types also makes it easier to understand the purpose of each task. The plan format meets the output requirements.\n",
      "\n",
      "However, there's one suggestion to make the plan more efficient:\n",
      "\n",
      "Instruction for task 4 could be combined with task 3 to save time and resources:\n",
      "\n",
      "```json\n",
      "- Replace task 4 with:\n",
      "  {\"task_id\": \"3\", \"dependent_task_ids\": [\"2\"], \"instruction\": \"Write a script using Playwright to scrape the HTML structure and inner text content of the specified URL (https://pitchhub.36kr.com/financing-flash), and parse the obtained HTML structure to extract the content within the 'quick-news' section.\", \"task_type\": \"web_scraping, html_parsing\"}\n",
      "```\n",
      "\n",
      "This way, the scraping and parsing will be done in a single task, reducing the need to repeat the web scraping process.\n",
      "\n",
      "\n",
      "# Available Task Types\n",
      "**web scraping**: Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \n",
      "**text extractor**: Perform extraction on the 'content' text using a large language model. \n",
      "\n",
      "# Task\n",
      "Based on the user goal, write a plan or modify an existing plan of what you should do to achieve the goal. A plan consists of one to 20 tasks.\n",
      "If you are modifying an existing plan, carefully follow the instruction, don't make unnecessary changes. Give the whole plan unless instructed to modify only one task of the plan.\n",
      "If you encounter errors on the current task, revise and output the current single task only.\n",
      "Output a list of jsons following the format:\n",
      "```json\n",
      "[\n",
      "    {{\n",
      "        \"task_id\": str = \"unique identifier for a task in plan, can be an ordinal\",\n",
      "        \"dependent_task_ids\": list[str] = \"ids of tasks prerequisite to this task\",\n",
      "        \"instruction\": \"what you should do in this task, one short phrase or sentence\",\n",
      "        \"task_type\": \"type of this task, should be one of Available Task Types\",\n",
      "    }},\n",
      "    ...\n",
      "]\n",
      "```\n",
      "2024-04-08 21:48:52.023 | DEBUG    | metagpt.provider.base_llm:aask:149 - [{'role': 'user', 'content': 'Respond to the human as helpfully and accurately as possible.\\n\\n# User goal\\n抓取 https://pitchhub.36kr.com/financing-flash 中\\'快讯\\'的内容，并整理成markdown存档\\n\\n大概的流程：\\n- 使用工具抓取网页中的可见文本\\n- 提取网页文本中\\'快讯\\'相关的内容。注意网页中可能包含导航，只需要抽取\\'快讯\\'的具体内容\\n- 对抽取结果进行归类，并保存成markdown表格: 快讯.md\\n\\n# Last plan\\n## Plan\\n```json\\n[{\\'task_id\\': \\'1\\', \\'dependent_task_ids\\': [], \\'instruction\\': \\'Choose a programming language and set up the required tools for web scraping and text extraction.\\', \\'task_type\\': \\'setup\\'}, {\\'task_id\\': \\'2\\', \\'dependent_task_ids\\': [\\'1\\'], \\'instruction\\': \\'Clone the repository or create a new project directory for the web scraping and text extraction tasks.\\', \\'task_type\\': \\'file_management\\'}, {\\'task_id\\': \\'3\\', \\'dependent_task_ids\\': [\\'2\\'], \\'instruction\\': \\'Write a script using Playwright to scrape the HTML structure and inner text content of the specified URL (https://pitchhub.36kr.com/financing-flash).\\', \\'task_type\\': \\'web_scraping\\'}, {\\'task_id\\': \\'4\\', \\'dependent_task_ids\\': [\\'3\\'], \\'instruction\\': \"Parse the obtained HTML structure to extract the content within the \\'quick-news\\' section.\", \\'task_type\\': \\'html_parsing\\'}, {\\'task_id\\': \\'5\\', \\'dependent_task_ids\\': [\\'4\\'], \\'instruction\\': \"Use a large language model to extract the \\'快讯\\' content from the parsed content.\", \\'task_type\\': \\'text_extractor\\'}, {\\'task_id\\': \\'6\\', \\'dependent_task_ids\\': [\\'5\\'], \\'instruction\\': \"Organize the extracted \\'快讯\\' content into a well-structured markdown table.\", \\'task_type\\': \\'text_processing\\'}, {\\'task_id\\': \\'7\\', \\'dependent_task_ids\\': [\\'6\\'], \\'instruction\\': \"Save the markdown table as a separate file named \\'快讯.md\\' in the project directory.\", \\'task_type\\': \\'file_management\\'}]\\n```\\n\\n## Review\\nThe plan seems to be properly set up for achieving the user goal. The tasks are well-defined, and each step logically follows the previous one. The use of the provided task types also makes it easier to understand the purpose of each task. The plan format meets the output requirements.\\n\\nHowever, there\\'s one suggestion to make the plan more efficient:\\n\\nInstruction for task 4 could be combined with task 3 to save time and resources:\\n\\n```json\\n- Replace task 4 with:\\n  {\"task_id\": \"3\", \"dependent_task_ids\": [\"2\"], \"instruction\": \"Write a script using Playwright to scrape the HTML structure and inner text content of the specified URL (https://pitchhub.36kr.com/financing-flash), and parse the obtained HTML structure to extract the content within the \\'quick-news\\' section.\", \"task_type\": \"web_scraping, html_parsing\"}\\n```\\n\\nThis way, the scraping and parsing will be done in a single task, reducing the need to repeat the web scraping process.\\n\\n\\n# Available Task Types\\n**web scraping**: Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \\n**text extractor**: Perform extraction on the \\'content\\' text using a large language model. \\n\\n# Task\\nBased on the user goal, write a plan or modify an existing plan of what you should do to achieve the goal. A plan consists of one to 20 tasks.\\nIf you are modifying an existing plan, carefully follow the instruction, don\\'t make unnecessary changes. Give the whole plan unless instructed to modify only one task of the plan.\\nIf you encounter errors on the current task, revise and output the current single task only.\\nOutput a list of jsons following the format:\\n```json\\n[\\n    {{\\n        \"task_id\": str = \"unique identifier for a task in plan, can be an ordinal\",\\n        \"dependent_task_ids\": list[str] = \"ids of tasks prerequisite to this task\",\\n        \"instruction\": \"what you should do in this task, one short phrase or sentence\",\\n        \"task_type\": \"type of this task, should be one of Available Task Types\",\\n    }},\\n    ...\\n]\\n```'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the user goal and the suggestion to combine the parsing step with the web scraping step, the updated plan is as follows:\n",
      "\n",
      "```json\n",
      "[{'task_id': '1', 'dependent_task_ids': [], 'instruction': 'Choose a programming language and set up the required tools for web scraping and text extraction.', 'task_type': 'setup'}, {'task_id': '2', 'dependent_task_ids': ['1'], 'instruction': 'Clone the repository or create a new project directory for the web scraping and text extraction tasks.', 'task_type': 'file_management'}, {'task_id': '3', 'dependent_task_ids': ['2'], 'instruction': 'Write a script using Playwright to scrape the HTML structure and inner text content of the specified URL (https://pitchhub.36kr.com/financing-flash) and parse the obtained HTML structure to extract the content within the \\'quick-news\\' section.', 'task_type': 'web_scraping, html_parsing'}, {'task_id': '4', 'dependent_task_ids': ['3'], 'instruction': \"Use a large language model to extract the '快讯' content from the parsed content.\", 'task_type': 'text_extractor'}, {'task_id': '5', 'dependent_task_ids': ['4'], 'instruction': \"Organize the extracted '快讯' content into a well-structured markdown table.\", 'task_type': 'text_processing'}, {'task_id': '6', 'dependent_task_ids': ['5'], 'instruction': \"Save the markdown table as a separate file named '快讯.md' in the project directory.\", 'task_type': 'file_management'}]\n",
      "```\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting property name enclosed in double quotes: line 1 column 3 (char 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 18\u001b[0m, in \u001b[0;36mparse_json\u001b[0;34m(rsp)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrsp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO: 修改成 plan_review.yaml 模板\u001b[39;00m\n\u001b[1;32m      2\u001b[0m last_plan \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m# Last plan\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m## Plan\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m```json\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mrsp\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m plan, raw_json \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_plan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgoal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_goal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguidance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_guidance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_plan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_plan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m pprint(plan\u001b[38;5;241m.\u001b[39mtasks)\n",
      "Cell \u001b[0;32mIn[22], line 39\u001b[0m, in \u001b[0;36mcreate_plan\u001b[0;34m(goal, guidance, last_plan)\u001b[0m\n\u001b[1;32m     36\u001b[0m plan\u001b[38;5;241m.\u001b[39mcontext \u001b[38;5;241m=\u001b[39m guidance\n\u001b[1;32m     37\u001b[0m rsp \u001b[38;5;241m=\u001b[39m llm_aask(msg\u001b[38;5;241m=\u001b[39mtemplate)\n\u001b[0;32m---> 39\u001b[0m tasks_json \u001b[38;5;241m=\u001b[39m \u001b[43mparse_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrsp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m tasks \u001b[38;5;241m=\u001b[39m [Task(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtask_config) \u001b[38;5;28;01mfor\u001b[39;00m task_config \u001b[38;5;129;01min\u001b[39;00m tasks_json]\n\u001b[1;32m     41\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(tasks)\n",
      "Cell \u001b[0;32mIn[22], line 21\u001b[0m, in \u001b[0;36mparse_json\u001b[0;34m(rsp)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     code_block \u001b[38;5;241m=\u001b[39m OutputParser\u001b[38;5;241m.\u001b[39mparse_code(rsp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode_block\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m objs\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 1 column 3 (char 2)"
     ]
    }
   ],
   "source": [
    "# TODO: 修改成 plan_review.yaml 模板\n",
    "last_plan = f\"\"\"# Last plan\n",
    "## Plan\n",
    "```json\n",
    "{raw_json}\n",
    "```\n",
    "\n",
    "## Review\n",
    "{rsp}\n",
    "\"\"\"\n",
    "\n",
    "plan, raw_json = create_plan(goal=user_goal, guidance=user_guidance, last_plan=last_plan)\n",
    "pprint(plan.tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff3222",
   "metadata": {},
   "source": [
    "## Tasks execute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a377910",
   "metadata": {},
   "source": [
    "### playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da32d977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">1 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">asyncio</span><span style=\"background-color: #2f1e2e\">                                                                                                 </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">2 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">nest_asyncio</span><span style=\"background-color: #2f1e2e\">                                                                                            </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">3 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">nest_asyncio</span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">.</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">apply()</span><span style=\"background-color: #2f1e2e\">                                                                                           </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">4 </span><span style=\"background-color: #2f1e2e\">                                                                                                               </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">5 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">from</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">metagpt.tools.libs.web_scraping</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> scrape_web_playwright</span><span style=\"background-color: #2f1e2e\">                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">6 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">from</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">tools.text_extractor.llm_extractor</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> llm_extractor</span><span style=\"background-color: #2f1e2e\">                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m1 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46masyncio\u001b[0m\u001b[48;2;47;30;46m                                                                                                 \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m2 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mnest_asyncio\u001b[0m\u001b[48;2;47;30;46m                                                                                            \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m3 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mnest_asyncio\u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m.\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mapply\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                                           \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m4 \u001b[0m\u001b[48;2;47;30;46m                                                                                                               \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m5 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mfrom\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mmetagpt\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mtools\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mlibs\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mweb_scraping\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mscrape_web_playwright\u001b[0m\u001b[48;2;47;30;46m                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m6 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mfrom\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mtools\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mtext_extractor\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mllm_extractor\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mllm_extractor\u001b[0m\u001b[48;2;47;30;46m                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from metagpt.actions.di.execute_nb_code import ExecuteNbCode\n",
    "\n",
    "pre_execute = \"\"\"import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\"\"\"\n",
    "\n",
    "# append imports\n",
    "for _, t in tools.items():\n",
    "    pre_execute += \"\\n\" + t[\"import\"]\n",
    "\n",
    "execute_code = ExecuteNbCode()\n",
    "result, success = await execute_code.run(pre_execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "819293bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 19:54:23.775 | DEBUG    | __main__:execute_task:9 - As an AI Engineer, you need to help user to achieve their goal step by step in a continuous Jupyter notebook.\n",
      "\n",
      "## Current Task\n",
      "Use Playwright to scrape the HTML content of the webpage https://pitchhub.36kr.com/financing-flash\n",
      "\n",
      "## Task Guidance\n",
      "Write complete code for 'Current Task'. And avoid duplicating code from 'Finished Tasks', such as repeated import of packages, reading data, etc.\n",
      "Specifically, 所有依赖均已经导入，无需提供pip或者环境相关内容\n",
      "\n",
      "\n",
      "\n",
      "# Tool Info\n",
      "\n",
      "## Capabilities\n",
      "- You can utilize pre-defined tools in any code lines from 'Available Tools' in the form of Python class or function.\n",
      "- You can freely combine the use of any other public packages, like sklearn, numpy, pandas, etc..\n",
      "\n",
      "## Available Tools:\n",
      "Each tool is described in JSON format. All tools was import by default.\n",
      "{\"web scraping\": {\"type\": \"async_function\", \"description\": \"Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \", \"signature\": \"(url)\", \"parameters\": \"Args: url (str): The main URL to fetch inner text from. Returns: dict: The inner text content and html structure of the web page, keys are 'inner_text', 'html'.\", \"import\": \"from metagpt.tools.libs.web_scraping import scrape_web_playwright\"}}\n",
      "{\"text extractor\": {\"type\": \"async_function\", \"description\": \"Perform extraction on the 'content' text using a large language model. \", \"signature\": \"(guidance: str, content: str, format: str) -> str\", \"parameters\": \"Args: guidance (str): Guide the extraction process. content (str): The text content that needs to be extracted. format (str): The output format, can be \\\"json\\\" or \\\"markdown\\\". Returns: str: text extracted from content.\", \"import\": \"from tools.text_extractor.llm_extractor import llm_extractor\"}}\n",
      "\n",
      "# Constraints\n",
      "- Take on Current Task if it is in Plan Status, otherwise, tackle User Requirement directly.\n",
      "- Ensure the output new code is executable in the same Jupyter notebook as the previous executed code.\n",
      "- Always prioritize using pre-defined tools for the same functionality.\n",
      "\n",
      "# Output\n",
      "While some concise thoughts are helpful, code is absolutely required. Always output one and only one code block in your response. Output code in the following format:\n",
      "```python\n",
      "your code\n",
      "```\n",
      "\n",
      "Remember!! Since it is a notebook environment, don't use asyncio.run. Instead, use await if you need to call an async function.\n",
      "2024-04-10 19:54:23.776 | DEBUG    | metagpt.provider.base_llm:aask:149 - [{'role': 'user', 'content': 'As an AI Engineer, you need to help user to achieve their goal step by step in a continuous Jupyter notebook.\\n\\n## Current Task\\nUse Playwright to scrape the HTML content of the webpage https://pitchhub.36kr.com/financing-flash\\n\\n## Task Guidance\\nWrite complete code for \\'Current Task\\'. And avoid duplicating code from \\'Finished Tasks\\', such as repeated import of packages, reading data, etc.\\nSpecifically, 所有依赖均已经导入，无需提供pip或者环境相关内容\\n\\n\\n\\n# Tool Info\\n\\n## Capabilities\\n- You can utilize pre-defined tools in any code lines from \\'Available Tools\\' in the form of Python class or function.\\n- You can freely combine the use of any other public packages, like sklearn, numpy, pandas, etc..\\n\\n## Available Tools:\\nEach tool is described in JSON format. All tools was import by default.\\n{\"web scraping\": {\"type\": \"async_function\", \"description\": \"Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \", \"signature\": \"(url)\", \"parameters\": \"Args: url (str): The main URL to fetch inner text from. Returns: dict: The inner text content and html structure of the web page, keys are \\'inner_text\\', \\'html\\'.\", \"import\": \"from metagpt.tools.libs.web_scraping import scrape_web_playwright\"}}\\n{\"text extractor\": {\"type\": \"async_function\", \"description\": \"Perform extraction on the \\'content\\' text using a large language model. \", \"signature\": \"(guidance: str, content: str, format: str) -> str\", \"parameters\": \"Args: guidance (str): Guide the extraction process. content (str): The text content that needs to be extracted. format (str): The output format, can be \\\\\"json\\\\\" or \\\\\"markdown\\\\\". Returns: str: text extracted from content.\", \"import\": \"from tools.text_extractor.llm_extractor import llm_extractor\"}}\\n\\n# Constraints\\n- Take on Current Task if it is in Plan Status, otherwise, tackle User Requirement directly.\\n- Ensure the output new code is executable in the same Jupyter notebook as the previous executed code.\\n- Always prioritize using pre-defined tools for the same functionality.\\n\\n# Output\\nWhile some concise thoughts are helpful, code is absolutely required. Always output one and only one code block in your response. Output code in the following format:\\n```python\\nyour code\\n```\\n\\nRemember!! Since it is a notebook environment, don\\'t use asyncio.run. Instead, use await if you need to call an async function.'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the given instructions, here's an example of how you might implement the `scrape_web_playwright` function using Playwright:\n",
      "\n",
      "```python\n",
      "import asyncio\n",
      "from playwright.async_api import Playwright, async_playwright\n",
      "\n",
      "async def scrape_web_playwright(url):\n",
      "    async with async_playwright() as p:\n",
      "        context = await p.chromium.launch()\n",
      "        page = await context.new_page()\n",
      "        await page.goto(url)\n",
      "        inner_text = await page.innerText()\n",
      "        html = await page.content()\n",
      "        await context.close()\n",
      "        return inner_text, html\n",
      "\n",
      "async def main():\n",
      "    url = \"https://example.com\"\n",
      "    inner_text, html = await scrape_web_playwright(url)\n",
      "    print(f\"Inner text: {inner_text}\")\n",
      "    print(f\"HTML: {html}\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    asyncio.run(main())\n",
      "```\n",
      "\n",
      "This code uses the `async_playwright` library to launch a Chromium browser, navigate to a URL, extract the inner text and HTML content, and then close the browser. The `main` function is defined as an async function and uses `asyncio.run` to execute it.\n",
      "\n",
      "You can use this `scrape_web_playwright` function in your previous code as follows:\n",
      "\n",
      "```python\n",
      "import re\n",
      "import json\n",
      "import asyncio\n",
      "from scrape_web_playwright import scrape_web_playwright\n",
      "\n",
      "async def main():\n",
      "    url = \"https://example.com\"\n",
      "    inner_text, html = await scrape_web_playwright(url)\n",
      "    pattern = r\"<h1>(.*?)</h1>\"\n",
      "    match = re.search(pattern, html)\n",
      "    if match:\n",
      "        title = match.group(1)\n",
      "        print(f\"Title: {title}\")\n",
      "        data = {\"title\": title}\n",
      "        print(json.dumps(data))\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    asyncio.run(main())\n",
      "```\n",
      "\n",
      "This code uses the `scrape_web_playwright` function to extract the HTML content and then searches for the title using a regular expression. If a match is found, it prints the title and serializes it as a JSON"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 19:54:45.388 | DEBUG    | __main__:execute_task:12 -  Based on the given instructions, here's an example of how you might implement the `scrape_web_playwright` function using Playwright:\n",
      "\n",
      "```python\n",
      "import asyncio\n",
      "from playwright.async_api import Playwright, async_playwright\n",
      "\n",
      "async def scrape_web_playwright(url):\n",
      "    async with async_playwright() as p:\n",
      "        context = await p.chromium.launch()\n",
      "        page = await context.new_page()\n",
      "        await page.goto(url)\n",
      "        inner_text = await page.innerText()\n",
      "        html = await page.content()\n",
      "        await context.close()\n",
      "        return inner_text, html\n",
      "\n",
      "async def main():\n",
      "    url = \"https://example.com\"\n",
      "    inner_text, html = await scrape_web_playwright(url)\n",
      "    print(f\"Inner text: {inner_text}\")\n",
      "    print(f\"HTML: {html}\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    asyncio.run(main())\n",
      "```\n",
      "\n",
      "This code uses the `async_playwright` library to launch a Chromium browser, navigate to a URL, extract the inner text and HTML content, and then close the browser. The `main` function is defined as an async function and uses `asyncio.run` to execute it.\n",
      "\n",
      "You can use this `scrape_web_playwright` function in your previous code as follows:\n",
      "\n",
      "```python\n",
      "import re\n",
      "import json\n",
      "import asyncio\n",
      "from scrape_web_playwright import scrape_web_playwright\n",
      "\n",
      "async def main():\n",
      "    url = \"https://example.com\"\n",
      "    inner_text, html = await scrape_web_playwright(url)\n",
      "    pattern = r\"<h1>(.*?)</h1>\"\n",
      "    match = re.search(pattern, html)\n",
      "    if match:\n",
      "        title = match.group(1)\n",
      "        print(f\"Title: {title}\")\n",
      "        data = {\"title\": title}\n",
      "        print(json.dumps(data))\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    asyncio.run(main())\n",
      "```\n",
      "\n",
      "This code uses the `scrape_web_playwright` function to extract the HTML content and then searches for the title using a regular expression. If a match is found, it prints the title and serializes it as a JSON object.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " object.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 1 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">asyncio</span><span style=\"background-color: #2f1e2e\">                                                                                                </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 2 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">from</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">playwright.async_api</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> Playwright, async_playwright</span><span style=\"background-color: #2f1e2e\">                                                 </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 3 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 4 </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">async</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">def</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #06b6ef; text-decoration-color: #06b6ef; background-color: #2f1e2e\">scrape_web_playwright</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">(url):</span><span style=\"background-color: #2f1e2e\">                                                                         </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 5 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">    </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">async</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">with</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> async_playwright() </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">as</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> p:</span><span style=\"background-color: #2f1e2e\">                                                                       </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 6 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">        context </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">await</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> p</span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">.</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">chromium</span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">.</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">launch()</span><span style=\"background-color: #2f1e2e\">                                                                   </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 7 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">        page </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">await</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> context</span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">.</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">new_page()</span><span style=\"background-color: #2f1e2e\">                                                                       </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 8 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">        </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">await</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> page</span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">.</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">goto(url)</span><span style=\"background-color: #2f1e2e\">                                                                                  </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 9 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">        inner_text </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">await</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> page</span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">.</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">innerText()</span><span style=\"background-color: #2f1e2e\">                                                                   </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">10 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">        html </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">await</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> page</span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">.</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">content()</span><span style=\"background-color: #2f1e2e\">                                                                           </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">11 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">        </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">await</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> context</span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">.</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">close()</span><span style=\"background-color: #2f1e2e\">                                                                                 </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">12 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">        </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">return</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> inner_text, html</span><span style=\"background-color: #2f1e2e\">                                                                               </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">13 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">14 </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">async</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">def</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #06b6ef; text-decoration-color: #06b6ef; background-color: #2f1e2e\">main</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">():</span><span style=\"background-color: #2f1e2e\">                                                                                             </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">15 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">    url </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">\"https://example.com\"</span><span style=\"background-color: #2f1e2e\">                                                                               </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">16 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">    inner_text, html </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">await</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> scrape_web_playwright(url)</span><span style=\"background-color: #2f1e2e\">                                                       </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">17 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">    print(</span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">f\"Inner text: </span><span style=\"color: #f99b15; text-decoration-color: #f99b15; background-color: #2f1e2e\">{</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">inner_text</span><span style=\"color: #f99b15; text-decoration-color: #f99b15; background-color: #2f1e2e\">}</span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">\"</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">)</span><span style=\"background-color: #2f1e2e\">                                                                        </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">18 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">    print(</span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">f\"HTML: </span><span style=\"color: #f99b15; text-decoration-color: #f99b15; background-color: #2f1e2e\">{</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">html</span><span style=\"color: #f99b15; text-decoration-color: #f99b15; background-color: #2f1e2e\">}</span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">\"</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">)</span><span style=\"background-color: #2f1e2e\">                                                                                    </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">19 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">20 </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">if</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #ef6155; text-decoration-color: #ef6155; background-color: #2f1e2e\">__name__</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">==</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">\"__main__\"</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">:</span><span style=\"background-color: #2f1e2e\">                                                                                    </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">21 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">    asyncio</span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">.</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">run(main())</span><span style=\"background-color: #2f1e2e\">                                                                                       </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">22 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 1 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46masyncio\u001b[0m\u001b[48;2;47;30;46m                                                                                                \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 2 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mfrom\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mplaywright\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46masync_api\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mPlaywright\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m,\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46masync_playwright\u001b[0m\u001b[48;2;47;30;46m                                                 \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 3 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 4 \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46masync\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46mdef\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;6;182;239;48;2;47;30;46mscrape_web_playwright\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m:\u001b[0m\u001b[48;2;47;30;46m                                                                         \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 5 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m    \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46masync\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46mwith\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46masync_playwright\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46mas\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mp\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m:\u001b[0m\u001b[48;2;47;30;46m                                                                       \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 6 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m        \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mcontext\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46mawait\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mp\u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m.\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mchromium\u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m.\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mlaunch\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                   \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 7 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m        \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mpage\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46mawait\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mcontext\u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m.\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mnew_page\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                       \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 8 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m        \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46mawait\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mpage\u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m.\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mgoto\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                                  \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 9 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m        \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46minner_text\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46mawait\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mpage\u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m.\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46minnerText\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                   \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m10 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m        \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mhtml\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46mawait\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mpage\u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m.\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mcontent\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                           \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m11 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m        \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46mawait\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mcontext\u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m.\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mclose\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                                 \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m12 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m        \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46mreturn\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46minner_text\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m,\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mhtml\u001b[0m\u001b[48;2;47;30;46m                                                                               \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m13 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m14 \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46masync\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46mdef\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;6;182;239;48;2;47;30;46mmain\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m:\u001b[0m\u001b[48;2;47;30;46m                                                                                             \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m15 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m    \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46mhttps://example.com\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[48;2;47;30;46m                                                                               \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m16 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m    \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46minner_text\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m,\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mhtml\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46mawait\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mscrape_web_playwright\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                       \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m17 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m    \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mprint\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46mf\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46mInner text: \u001b[0m\u001b[38;2;249;155;21;48;2;47;30;46m{\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46minner_text\u001b[0m\u001b[38;2;249;155;21;48;2;47;30;46m}\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                        \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m18 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m    \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mprint\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46mf\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46mHTML: \u001b[0m\u001b[38;2;249;155;21;48;2;47;30;46m{\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mhtml\u001b[0m\u001b[38;2;249;155;21;48;2;47;30;46m}\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                                    \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m19 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m20 \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46mif\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;239;97;85;48;2;47;30;46m__name__\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m==\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m__main__\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m:\u001b[0m\u001b[48;2;47;30;46m                                                                                    \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m21 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m    \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46masyncio\u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m.\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mrun\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mmain\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                                       \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m22 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'import asyncio\\nfrom playwright.async_api import Playwright, async_playwright\\n\\nasync def scrape_web_playwright(url):\\n    async with async_playwright() as p:\\n        context = await p.chromium.launch()\\n        page = await context.new_page()\\n        await page.goto(url)\\n        inner_text = await page.innerText()\\n        html = await page.content()\\n        await context.close()\\n        return inner_text, html\\n\\nasync def main():\\n    url = \"https://example.com\"\\n    inner_text, html = await scrape_web_playwright(url)\\n    print(f\"Inner text: {inner_text}\")\\n    print(f\"HTML: {html}\")\\n\\nif __name__ == \"__main__\":\\n    asyncio.run(main())\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def execute_task(plan: Plan, plan_status=\"\", task_guidance=\"\"):\n",
    "    codegen_prompt = load_prompts(\"task_codegen\", \"task_codegen.yaml\")\n",
    "    template = codegen_prompt.format(\n",
    "        plan_status=plan_status,\n",
    "        current_task=plan.current_task.instruction,\n",
    "        task_guidance=task_guidance,\n",
    "        tools=tools_list,\n",
    "    )\n",
    "    logger.debug(template)\n",
    "\n",
    "    rsp = llm_aask(msg=template, seed=123)\n",
    "    logger.debug(rsp)\n",
    "\n",
    "    code_block = OutputParser.parse_code(rsp, \"python\")\n",
    "    execute_code._display(code_block, language=\"python\")\n",
    "    return code_block\n",
    "\n",
    "code = execute_task(plan, task_guidance=\"所有依赖均已经导入，无需提供pip或者环境相关内容\")\n",
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cc75ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 1 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">from</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">metagpt.tools.libs.web_scraping</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> scrape_web_playwright</span><span style=\"background-color: #2f1e2e\">                                             </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 2 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 3 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">url </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">\"https://pitchhub.36kr.com/financing-flash\"</span><span style=\"background-color: #2f1e2e\">                                                             </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 4 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 5 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Scrape the web page using Playwright</span><span style=\"background-color: #2f1e2e\">                                                                        </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 6 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">result </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">await</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> scrape_web_playwright(url)</span><span style=\"background-color: #2f1e2e\">                                                                     </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 7 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 8 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Extract text from the scraped HTML</span><span style=\"background-color: #2f1e2e\">                                                                          </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 9 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">text </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> result[</span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">'inner_text'</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">]</span><span style=\"background-color: #2f1e2e\">                                                                                   </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">10 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">11 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Output the extracted text</span><span style=\"background-color: #2f1e2e\">                                                                                   </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">12 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">print(text)</span><span style=\"background-color: #2f1e2e\">                                                                                                   </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">13 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 1 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mfrom\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mmetagpt\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mtools\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mlibs\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mweb_scraping\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mscrape_web_playwright\u001b[0m\u001b[48;2;47;30;46m                                             \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 2 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 3 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46mhttps://pitchhub.36kr.com/financing-flash\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[48;2;47;30;46m                                                             \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 4 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 5 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Scrape the web page using Playwright\u001b[0m\u001b[48;2;47;30;46m                                                                        \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 6 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mresult\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46mawait\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mscrape_web_playwright\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                     \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 7 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 8 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Extract text from the scraped HTML\u001b[0m\u001b[48;2;47;30;46m                                                                          \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 9 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mtext\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mresult\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m[\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m'\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46minner_text\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m'\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m]\u001b[0m\u001b[48;2;47;30;46m                                                                                   \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m10 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m11 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Output the extracted text\u001b[0m\u001b[48;2;47;30;46m                                                                                   \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m12 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mprint\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mtext\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                                                   \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m13 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " '首页\\n融资快报\\n融资事件\\n项目库\\n机构库\\n项目集\\n定向对接\\n融通创新\\n公司/项目名/投资机构/赛道\\n\\xa0\\n返回36氪\\n登录\\n融资快报\\n快讯\\n“特斯联”完成20亿元D轮融资\\n36氪获悉，人工智能物联网（AIoT）企业“特斯联”近日完成D轮20亿元融资。本轮融资由AL Capital、阳明股权投资基金共同领投，福田资本、金地集团、重科控股、数字重庆、南昌政府平台公司、徐州产业基金、北科建集团、光大控股、商汤科技等新老股东跟投。本轮融资后，特斯联将进一步夯实数智化基础设施，深化“模型+系统”的比特大模型开放平台。原文链接\\n特斯联\\n战略融资\\n重庆市\\n2015年成立\\n城市级智能物联网服务提供商\\n5小时前\\n快讯\\n无人机研发及赛事运营商“美冠挚友”获天使+轮融资\\n36氪获悉，无人机研发及赛事运营商“美冠挚友（星奇世界HISINGY）”宣布完成天使+轮融资，本轮融资由中科优势旗下探方资本领投，老股东火凤资本、真成投资持续跟投，探究资本担任独家财务顾问。本轮融资资金将用于“低价格高品质”的全新娱乐穿越机套装产品的研发及投产、新消费场景的布局及各传统玩具线下渠道推进。原文链接\\n6小时前\\n文章\\n\\u200b36氪广东首发｜AI Infra供应商「星凡科技」获近亿元Pre-A轮融资，赋能算力中心建设运营\\n2023年公司全年预计实现过亿元营收。\\n星凡科技\\n天使轮\\n四川省\\n2021年成立\\n元宇宙基础数字平台\\n6小时前\\n许璧端@36氪广东\\n文章\\n「特斯联」完成20亿元D轮融资，加速模型+系统在AIoT领域落地｜硬氪独家\\n特斯联初探领域大模型应用，加速企业数智化转型。\\n特斯联\\n战略融资\\n重庆市\\n2015年成立\\n城市级智能物联网服务提供商\\n6小时前\\n黄 楠\\n文章\\n「美冠挚友（星奇世界HISINGY）」获天使+轮融资，持续推动娱乐无人机与低空经济发展\\n这是星奇世界自2022年完成天使轮和战略轮后的第三轮融资，团队正在拓展无人机与潮玩的边界。\\n星奇世界HISINGY\\n战略融资\\n北京市\\n2018年成立\\n潮玩品牌\\n6小时前\\n刘士武\\n文章\\n云澎科技完成数千万元A轮融资\\n由亚投资本投资。\\n21小时前\\n时氪分享\\n文章\\n世纪云安完成数亿元A+轮融资、华发集团与华为在全屋智能领域合作、长租公寓公司BLUEGROUND融资4500万｜ PropTech周刊\\n聚焦改变地产行业的科技及创新力量\\n21小时前\\n宋虹姗\\n文章\\n「德沪涂膜」完成数千万元A轮融资，专注钙钛矿精密溶液涂布解决方案 | 36氪首发\\n截至目前，德沪涂膜已经打入了协鑫光电等近20家钙钛矿、光伏企业的供应链。\\n德沪涂膜\\n未披露\\n上海市\\n2016年成立\\n高端装备制造企业\\n21小时前\\n王方玉\\n快讯\\n深圳跨境电商资讯平台“亿恩网”完成天使轮融资\\n36氪广东获悉，据“亿恩”微信公众号消息，近日，深圳亿恩网广告传媒有限公司（下沉“亿恩网”）完成天使轮融资，本次融资金额和投资方未披露。“亿恩网”成立于2016年，是一家跨境电商资讯平台，2023年以来，主要面向有全球化电商布局需求的国内企业，如消费品牌、科技品牌、时尚品牌、产业品牌等，提供品牌出海孵化服务。原文链接\\n22小时前\\n快讯\\nAI角色创作平台“捏Ta”完成超千万元融资\\n36氪获悉，近日AI角色创作平台“捏Ta”（公司名为“看见概念”）完成了天使轮和Pre-A轮融资，总额超千万元。投资方包括源码资本和奇绩创坛等，所得资金将主要用于模型的进一步优化、多模态功能的增强以及社交玩法的开发。原文链接\\n昨天\\n快讯\\n“天地和兴”获约8亿元融资\\n36氪获悉，“天地和兴”近日宣布完成E轮融资，融资资金约8亿元。本轮由北京国管控股下属京国瑞、联通中金、蜂云资本联合领投，北京信息产业发展投资基金、中关村科学城公司、中核新兴产业基金、网宿科技、松禾资本等新老股东跟投，深蓝资本为牵头财务顾问。本轮融资后国资占比进一步增加。原文链接\\n天地和兴\\nD轮\\n北京市\\n2007年成立\\n工控安全整体解决方案提供商\\n昨天\\n文章\\n照着剧本一键创作同人漫，AI角色创作平台「捏Ta」完成超千万元融资 | 36氪首发\\n在技术发展早期，产品需要考虑的是如何在有限的技术成熟度内，满足用户更完整、丰富的创作需求。\\n看见概念\\nPre-A轮\\n上海市\\n2022年成立\\nAI 驱动的次文化内容共创社区\\n昨天\\n周鑫雨\\n文章\\n工控安全厂商「天地和兴」获约8亿元融资，北京市国资、联通中金、蜂云资本领投｜硬氪首发\\n工业网络安全需求呈爆发性增长。\\n天地和兴\\nD轮\\n北京市\\n2007年成立\\n工控安全整体解决方案提供商\\n昨天\\n黄 楠\\n快讯\\n“中科新生命”完成数亿元C轮融资\\n36氪获悉，生物技术企业“中科新生命”完成数亿元C轮融资，由广发信德领投，盛石资本跟投，浩悦资本继续担任本轮融资的独家财务顾问。本轮融资主要用于开发基于多组学应用场景的新产品；继续拓展科技服务、生物医')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result, success = await execute_code.run(code)\n",
    "success, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c9d0cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 15:25:04.613 | DEBUG    | __main__:execute_task:9 - As an AI Engineer, you need to help user to achieve their goal step by step in a continuous Jupyter notebook.\n",
      "\n",
      "## Current Task\n",
      "Navigate to the Pitchhub.36kr.com website\n",
      "\n",
      "## Task Guidance\n",
      "Write complete code for 'Current Task'. And avoid duplicating code from 'Finished Tasks', such as repeated import of packages, reading data, etc.\n",
      "Specifically, \n",
      "\n",
      "### Error\n",
      "抓取内容不符合预期。请检查输入的URL，需要抓取的是 https://pitchhub.36kr.com/financing-flash\n",
      "\n",
      "```\n",
      "首页\n",
      "融资快报\n",
      "融资事件\n",
      "项目库\n",
      "机构库\n",
      "项目集\n",
      "定向对接\n",
      "融通创新\n",
      "公司/项目名/投资机构/赛道\n",
      " \n",
      "返回36氪\n",
      "登录\n",
      "认证投资人专享-助力对接优质项目\n",
      "为投资人对接全市场创业项目——媒体独家资源，快速链接创始团队，深挖水下项目，助力高效Deal Sourcing\n",
      "36氪创投平台正式推出企业融资一站式解决方案——36氪创投平台融资加速服务\n",
      "36氪创投平台基于媒体业务十余年积累，整合内部投资人资源、王牌路演活动、企业号产品和财务顾问服务为创业企业提供全方位的融资加速服务，目前已与数百家头部投资机构建立深度合作，67%的企业被36氪曝光或服务后成功获得了融资。\n",
      "认证投资人专享-助力对接优质项目\n",
      "为投资人对接全市场创业项目——媒体独家资源，快速链接创始团队，深挖水下项目，助力高效Deal Sourcing\n",
      "36氪创投平台正式推出企业融资一站式解决方案——36氪创投平台融资加速服务\n",
      "36氪创投平台基于媒体业务十余年积累，整合内部投资人资源、王牌路演活动、企业号产品和财务顾问服务为创业企业提供全方位的融资加速服务，目前已与数百家头部投资机构建立深度合作，67%的企业被36氪曝光或服务后成功获得了融资。\n",
      "36氪创投平台\n",
      "认证身份，享专属特权\n",
      "让一部分人先看到未来\n",
      "创业者认证\n",
      "投资人认证\n",
      "\n",
      "寻求报道\n",
      "\n",
      "Seek-report\n",
      "\n",
      "创投平台小程序\n",
      "\n",
      "MiniProgram\n",
      "\n",
      "FA服务\n",
      "\n",
      "Financial Advisor\n",
      "\n",
      "研究院\n",
      "\n",
      "36Kr Research\n",
      "\n",
      "资情留言板\n",
      "\n",
      "Secondary Fund\n",
      "\n",
      "融资快报\n",
      "快讯\n",
      "“特斯联”完成20亿元D轮融资\n",
      "36氪获悉，人工智能物联网（AIoT）企业“特斯联”近日完成D轮20亿元融资。本轮融资由AL Capital、阳明股权投资基金共同领投，福田资本、金地集团、重科控股、数字重庆、南昌政府平台公司、徐州产业基金、北科建集团、光大控股、商汤科技等新老股东跟投。本轮融资后，特斯联将进一步夯实数智化基础设施，深化“模型+系统”的比特大模型开放平台。\n",
      "5小时前\n",
      "特斯联\n",
      "快讯\n",
      "无人机研发及赛事运营商“美冠挚友”获天使+轮融资\n",
      "36氪获悉，无人机研发及赛事运营商“美冠挚友（星奇世界HISINGY）”宣布完成天使+轮融资，本轮融资由中科优势旗下探方资本领投，老股东火凤资本、真成投资持续跟投，探究资本担任独家财务顾问。本轮融资资金将用于“低价格高品质”的全新娱乐穿越机套装产品的研发及投产、新消费场景的布局及各传统玩具线下渠道推进。\n",
      "6小时前\n",
      "文章\n",
      "​36氪广东首发｜AI Infra供应商「星凡科技」获近亿元Pre-A轮融资，赋能算力中心建设运营\n",
      "2023年公司全年预计实现过亿元营收。\n",
      "6小时前\n",
      "星凡科技\n",
      "文章\n",
      "「特斯联」完成20亿元D轮融资，加速模型+系统在AIoT领域落地｜硬氪独家\n",
      "特斯联初探领域大模型应用，加速企业数智化转型。\n",
      "6小时前\n",
      "特斯联\n",
      "查看更多\n",
      "融资事件\n",
      "本月投资事件数1起\n",
      "项目\n",
      "轮次/时间/金额\n",
      "投资方\n",
      "君跻基因\n",
      "生命科学机器人研发商\n",
      "Pre-A轮\n",
      " · \n",
      "2天前\n",
      "未透露\n",
      "明熙资本\n",
      "新能源重卡一体化生态\n",
      "新能源商用车车路云一体化生态\n",
      "A轮\n",
      " · \n",
      "12天前\n",
      "30000000人民币\n",
      "未透露\n",
      "津合生物\n",
      "酶电级联科技应用于生物合成与生物传感\n",
      "Pre-A轮\n",
      " · \n",
      "15天前\n",
      "未透露\n",
      "鼎晖投资、天择资本\n",
      "杭州光研科技\n",
      "专注于半导体晶圆检测设备的高新科技企业\n",
      "Pre-A轮\n",
      " · \n",
      "22天前\n",
      "数千万人民币\n",
      "深高新投、海富产业基金\n",
      "绿森工业品\n",
      "专注于一带一路市场的工业品跨境电商平台\n",
      "Pre-A轮\n",
      " · \n",
      "25天前\n",
      "数千万人民币\n",
      "未透露\n",
      "查看更多\n",
      "在融项目\n",
      "我要融资\n",
      "常岳新能源\n",
      "A轮\n",
      "新能源电池流通解决方案提供商\n",
      "四海万联\n",
      "A+轮\n",
      "物联网和车联网技术研发商\n",
      "心岛日记\n",
      "Pre-A轮\n",
      "基于AIGC的Z世代心理疗愈服务平台\n",
      "家厨\n",
      "天使轮\n",
      "本地生活上门厨师餐娱服务提供商\n",
      "聚变方成AI计算平台\n",
      "Pre-A轮\n",
      "做领先的算力运营商，中国的CoreWeave.\n",
      "查看更多\n",
      "每日路演\n",
      "我要路演\n",
      "专注呋喃类生物基材料的设计开发与产业化中科国生路演\n",
      "专注水系有机液流电池技术宿迁时代储能路演\n",
      "专注钠离子电池硬炭负极材料研发生产容钠新能源路演\n",
      "专注碳资产交易与开发的低碳服务运营商金诺碳投路演\n",
      "查看更多\n",
      "企业号\n",
      "我要入驻\n",
      "再惠网络科技\n",
      "再惠重庆光之国团队-在路上\n",
      "深圳市万邑通\n",
      "36氪\n",
      "查看更多\n",
      "项目集\n",
      "查看更多\n",
      "「十二月最受投资人关注」的项目合集\n",
      "93个项目\n",
      "2024-01-09\n",
      "井英科技\n",
      "Pre-A轮\n",
      "新一代AIGC超级内容平台\n",
      "光鉴科技\n",
      "B轮\n",
      "3D视觉解决方案服务商\n",
      "奥创光年\n",
      "A+轮\n",
      "用技术和创意重塑数字营销\n",
      "新顿科技\n",
      "A+轮\n",
      "高性能复合材料研发生产商\n",
      "方石科技\n",
      "Pre-A轮\n",
      "建筑机器人开发商及智能建造解决方案提供商\n",
      "查看更多\n",
      "「一月最受\n",
      "```\n",
      "\n",
      "### Previous code\n",
      "```python\n",
      "from metagpt.tools.libs.web_scraping import scrape_web_playwright\n",
      "\n",
      "url = \"https://pitchhub.36kr.com/\"\n",
      "\n",
      "# Scrape the web page using Playwright\n",
      "result = await scrape_web_playwright(url)\n",
      "\n",
      "# Extract text from the scraped HTML\n",
      "text = result['inner_text']\n",
      "\n",
      "# Output the extracted text\n",
      "print(text)\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "# Tool Info\n",
      "\n",
      "## Capabilities\n",
      "- You can utilize pre-defined tools in any code lines from 'Available Tools' in the form of Python class or function.\n",
      "- You can freely combine the use of any other public packages, like sklearn, numpy, pandas, etc..\n",
      "\n",
      "## Available Tools:\n",
      "Each tool is described in JSON format. All tools was import by default.\n",
      "{\"web scraping\": {\"type\": \"async_function\", \"description\": \"Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \", \"signature\": \"(url)\", \"parameters\": \"Args: url (str): The main URL to fetch inner text from. Returns: dict: The inner text content and html structure of the web page, keys are 'inner_text', 'html'.\", \"import\": \"from metagpt.tools.libs.web_scraping import scrape_web_playwright\"}}\n",
      "{\"text extractor\": {\"type\": \"async_function\", \"description\": \"Perform extraction on the 'content' text using a large language model. \", \"signature\": \"(guidance: str, content: str, format: str) -> str\", \"parameters\": \"Args: guidance (str): Guide the extraction process. content (str): The text content that needs to be extracted. format (str): The output format, can be \\\"json\\\" or \\\"markdown\\\". Returns: str: text extracted from content.\", \"import\": \"from tools.text_extractor.llm_extractor import llm_extractor\"}}\n",
      "\n",
      "# Constraints\n",
      "- Take on Current Task if it is in Plan Status, otherwise, tackle User Requirement directly.\n",
      "- Ensure the output new code is executable in the same Jupyter notebook as the previous executed code.\n",
      "- Always prioritize using pre-defined tools for the same functionality.\n",
      "\n",
      "# Output\n",
      "While some concise thoughts are helpful, code is absolutely required. Always output one and only one code block in your response. Output code in the following format:\n",
      "```python\n",
      "your code\n",
      "```\n",
      "\n",
      "Remember!! Since it is a notebook environment, don't use asyncio.run. Instead, use await if you need to call an async function.\n",
      "2024-04-09 15:25:04.614 | DEBUG    | metagpt.provider.base_llm:aask:149 - [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'As an AI Engineer, you need to help user to achieve their goal step by step in a continuous Jupyter notebook.\\n\\n## Current Task\\nNavigate to the Pitchhub.36kr.com website\\n\\n## Task Guidance\\nWrite complete code for \\'Current Task\\'. And avoid duplicating code from \\'Finished Tasks\\', such as repeated import of packages, reading data, etc.\\nSpecifically, \\n\\n### Error\\n抓取内容不符合预期。请检查输入的URL，需要抓取的是 https://pitchhub.36kr.com/financing-flash\\n\\n```\\n首页\\n融资快报\\n融资事件\\n项目库\\n机构库\\n项目集\\n定向对接\\n融通创新\\n公司/项目名/投资机构/赛道\\n\\xa0\\n返回36氪\\n登录\\n认证投资人专享-助力对接优质项目\\n为投资人对接全市场创业项目——媒体独家资源，快速链接创始团队，深挖水下项目，助力高效Deal Sourcing\\n36氪创投平台正式推出企业融资一站式解决方案——36氪创投平台融资加速服务\\n36氪创投平台基于媒体业务十余年积累，整合内部投资人资源、王牌路演活动、企业号产品和财务顾问服务为创业企业提供全方位的融资加速服务，目前已与数百家头部投资机构建立深度合作，67%的企业被36氪曝光或服务后成功获得了融资。\\n认证投资人专享-助力对接优质项目\\n为投资人对接全市场创业项目——媒体独家资源，快速链接创始团队，深挖水下项目，助力高效Deal Sourcing\\n36氪创投平台正式推出企业融资一站式解决方案——36氪创投平台融资加速服务\\n36氪创投平台基于媒体业务十余年积累，整合内部投资人资源、王牌路演活动、企业号产品和财务顾问服务为创业企业提供全方位的融资加速服务，目前已与数百家头部投资机构建立深度合作，67%的企业被36氪曝光或服务后成功获得了融资。\\n36氪创投平台\\n认证身份，享专属特权\\n让一部分人先看到未来\\n创业者认证\\n投资人认证\\n\\n寻求报道\\n\\nSeek-report\\n\\n创投平台小程序\\n\\nMiniProgram\\n\\nFA服务\\n\\nFinancial Advisor\\n\\n研究院\\n\\n36Kr Research\\n\\n资情留言板\\n\\nSecondary Fund\\n\\n融资快报\\n快讯\\n“特斯联”完成20亿元D轮融资\\n36氪获悉，人工智能物联网（AIoT）企业“特斯联”近日完成D轮20亿元融资。本轮融资由AL Capital、阳明股权投资基金共同领投，福田资本、金地集团、重科控股、数字重庆、南昌政府平台公司、徐州产业基金、北科建集团、光大控股、商汤科技等新老股东跟投。本轮融资后，特斯联将进一步夯实数智化基础设施，深化“模型+系统”的比特大模型开放平台。\\n5小时前\\n特斯联\\n快讯\\n无人机研发及赛事运营商“美冠挚友”获天使+轮融资\\n36氪获悉，无人机研发及赛事运营商“美冠挚友（星奇世界HISINGY）”宣布完成天使+轮融资，本轮融资由中科优势旗下探方资本领投，老股东火凤资本、真成投资持续跟投，探究资本担任独家财务顾问。本轮融资资金将用于“低价格高品质”的全新娱乐穿越机套装产品的研发及投产、新消费场景的布局及各传统玩具线下渠道推进。\\n6小时前\\n文章\\n\\u200b36氪广东首发｜AI Infra供应商「星凡科技」获近亿元Pre-A轮融资，赋能算力中心建设运营\\n2023年公司全年预计实现过亿元营收。\\n6小时前\\n星凡科技\\n文章\\n「特斯联」完成20亿元D轮融资，加速模型+系统在AIoT领域落地｜硬氪独家\\n特斯联初探领域大模型应用，加速企业数智化转型。\\n6小时前\\n特斯联\\n查看更多\\n融资事件\\n本月投资事件数1起\\n项目\\n轮次/时间/金额\\n投资方\\n君跻基因\\n生命科学机器人研发商\\nPre-A轮\\n\\xa0·\\xa0\\n2天前\\n未透露\\n明熙资本\\n新能源重卡一体化生态\\n新能源商用车车路云一体化生态\\nA轮\\n\\xa0·\\xa0\\n12天前\\n30000000人民币\\n未透露\\n津合生物\\n酶电级联科技应用于生物合成与生物传感\\nPre-A轮\\n\\xa0·\\xa0\\n15天前\\n未透露\\n鼎晖投资、天择资本\\n杭州光研科技\\n专注于半导体晶圆检测设备的高新科技企业\\nPre-A轮\\n\\xa0·\\xa0\\n22天前\\n数千万人民币\\n深高新投、海富产业基金\\n绿森工业品\\n专注于一带一路市场的工业品跨境电商平台\\nPre-A轮\\n\\xa0·\\xa0\\n25天前\\n数千万人民币\\n未透露\\n查看更多\\n在融项目\\n我要融资\\n常岳新能源\\nA轮\\n新能源电池流通解决方案提供商\\n四海万联\\nA+轮\\n物联网和车联网技术研发商\\n心岛日记\\nPre-A轮\\n基于AIGC的Z世代心理疗愈服务平台\\n家厨\\n天使轮\\n本地生活上门厨师餐娱服务提供商\\n聚变方成AI计算平台\\nPre-A轮\\n做领先的算力运营商，中国的CoreWeave.\\n查看更多\\n每日路演\\n我要路演\\n专注呋喃类生物基材料的设计开发与产业化中科国生路演\\n专注水系有机液流电池技术宿迁时代储能路演\\n专注钠离子电池硬炭负极材料研发生产容钠新能源路演\\n专注碳资产交易与开发的低碳服务运营商金诺碳投路演\\n查看更多\\n企业号\\n我要入驻\\n再惠网络科技\\n再惠重庆光之国团队-在路上\\n深圳市万邑通\\n36氪\\n查看更多\\n项目集\\n查看更多\\n「十二月最受投资人关注」的项目合集\\n93个项目\\n2024-01-09\\n井英科技\\nPre-A轮\\n新一代AIGC超级内容平台\\n光鉴科技\\nB轮\\n3D视觉解决方案服务商\\n奥创光年\\nA+轮\\n用技术和创意重塑数字营销\\n新顿科技\\nA+轮\\n高性能复合材料研发生产商\\n方石科技\\nPre-A轮\\n建筑机器人开发商及智能建造解决方案提供商\\n查看更多\\n「一月最受\\n```\\n\\n### Previous code\\n```python\\nfrom metagpt.tools.libs.web_scraping import scrape_web_playwright\\n\\nurl = \"https://pitchhub.36kr.com/\"\\n\\n# Scrape the web page using Playwright\\nresult = await scrape_web_playwright(url)\\n\\n# Extract text from the scraped HTML\\ntext = result[\\'inner_text\\']\\n\\n# Output the extracted text\\nprint(text)\\n\\n```\\n\\n\\n# Tool Info\\n\\n## Capabilities\\n- You can utilize pre-defined tools in any code lines from \\'Available Tools\\' in the form of Python class or function.\\n- You can freely combine the use of any other public packages, like sklearn, numpy, pandas, etc..\\n\\n## Available Tools:\\nEach tool is described in JSON format. All tools was import by default.\\n{\"web scraping\": {\"type\": \"async_function\", \"description\": \"Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \", \"signature\": \"(url)\", \"parameters\": \"Args: url (str): The main URL to fetch inner text from. Returns: dict: The inner text content and html structure of the web page, keys are \\'inner_text\\', \\'html\\'.\", \"import\": \"from metagpt.tools.libs.web_scraping import scrape_web_playwright\"}}\\n{\"text extractor\": {\"type\": \"async_function\", \"description\": \"Perform extraction on the \\'content\\' text using a large language model. \", \"signature\": \"(guidance: str, content: str, format: str) -> str\", \"parameters\": \"Args: guidance (str): Guide the extraction process. content (str): The text content that needs to be extracted. format (str): The output format, can be \\\\\"json\\\\\" or \\\\\"markdown\\\\\". Returns: str: text extracted from content.\", \"import\": \"from tools.text_extractor.llm_extractor import llm_extractor\"}}\\n\\n# Constraints\\n- Take on Current Task if it is in Plan Status, otherwise, tackle User Requirement directly.\\n- Ensure the output new code is executable in the same Jupyter notebook as the previous executed code.\\n- Always prioritize using pre-defined tools for the same functionality.\\n\\n# Output\\nWhile some concise thoughts are helpful, code is absolutely required. Always output one and only one code block in your response. Output code in the following format:\\n```python\\nyour code\\n```\\n\\nRemember!! Since it is a notebook environment, don\\'t use asyncio.run. Instead, use await if you need to call an async function.'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "from metagpt.tools.libs.web_scraping import scrape_web_playwright\n",
      "\n",
      "url = \"https://pitchhub.36kr.com/financing-flash\"\n",
      "\n",
      "# Scrape the web page using Playwright\n",
      "result = await scrape_web_playwright(url)\n",
      "\n",
      "# Extract text from the scraped HTML\n",
      "text = result['inner_text']\n",
      "\n",
      "# Output the extracted text"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 15:25:08.321 | INFO     | metagpt.utils.cost_manager:update_cost:108 - prompt_tokens: 1883, completion_tokens: 100\n",
      "2024-04-09 15:25:08.323 | DEBUG    | __main__:execute_task:12 - ```python\n",
      "from metagpt.tools.libs.web_scraping import scrape_web_playwright\n",
      "\n",
      "url = \"https://pitchhub.36kr.com/financing-flash\"\n",
      "\n",
      "# Scrape the web page using Playwright\n",
      "result = await scrape_web_playwright(url)\n",
      "\n",
      "# Extract text from the scraped HTML\n",
      "text = result['inner_text']\n",
      "\n",
      "# Output the extracted text\n",
      "print(text)\n",
      "```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "print(text)\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 1 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">from</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">metagpt.tools.libs.web_scraping</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> scrape_web_playwright</span><span style=\"background-color: #2f1e2e\">                                             </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 2 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 3 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">url </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">\"https://pitchhub.36kr.com/financing-flash\"</span><span style=\"background-color: #2f1e2e\">                                                             </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 4 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 5 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Scrape the web page using Playwright</span><span style=\"background-color: #2f1e2e\">                                                                        </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 6 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">result </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">await</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> scrape_web_playwright(url)</span><span style=\"background-color: #2f1e2e\">                                                                     </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 7 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 8 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Extract text from the scraped HTML</span><span style=\"background-color: #2f1e2e\">                                                                          </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 9 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">text </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> result[</span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">'inner_text'</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">]</span><span style=\"background-color: #2f1e2e\">                                                                                   </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">10 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">11 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Output the extracted text</span><span style=\"background-color: #2f1e2e\">                                                                                   </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">12 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">print(text)</span><span style=\"background-color: #2f1e2e\">                                                                                                   </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">13 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 1 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mfrom\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mmetagpt\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mtools\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mlibs\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mweb_scraping\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mscrape_web_playwright\u001b[0m\u001b[48;2;47;30;46m                                             \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 2 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 3 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46mhttps://pitchhub.36kr.com/financing-flash\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[48;2;47;30;46m                                                             \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 4 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 5 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Scrape the web page using Playwright\u001b[0m\u001b[48;2;47;30;46m                                                                        \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 6 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mresult\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46mawait\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mscrape_web_playwright\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                     \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 7 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 8 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Extract text from the scraped HTML\u001b[0m\u001b[48;2;47;30;46m                                                                          \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 9 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mtext\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mresult\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m[\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m'\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46minner_text\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m'\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m]\u001b[0m\u001b[48;2;47;30;46m                                                                                   \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m10 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m11 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Output the extracted text\u001b[0m\u001b[48;2;47;30;46m                                                                                   \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m12 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mprint\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mtext\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                                                   \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m13 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'from metagpt.tools.libs.web_scraping import scrape_web_playwright\\n\\nurl = \"https://pitchhub.36kr.com/financing-flash\"\\n\\n# Scrape the web page using Playwright\\nresult = await scrape_web_playwright(url)\\n\\n# Extract text from the scraped HTML\\ntext = result[\\'inner_text\\']\\n\\n# Output the extracted text\\nprint(text)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status = f\"\"\"### Error\n",
    "抓取内容不符合预期。请检查输入的URL，需要抓取的是 https://pitchhub.36kr.com/financing-flash\n",
    "\n",
    "```\n",
    "{result}\n",
    "```\n",
    "\n",
    "### Previous code\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "code = execute_task(plan, plan_status=status)\n",
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51d38cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_id='1' dependent_task_ids=[] instruction='Navigate to the Pitchhub.36kr.com website' task_type='web scraping' code='from metagpt.tools.libs.web_scraping import scrape_web_playwright\\n\\nurl = \"https://pitchhub.36kr.com/financing-flash\"\\n\\n# Scrape the web page using Playwright\\nresult = await scrape_web_playwright(url)\\n\\n# Extract text from the scraped HTML\\ntext = result[\\'inner_text\\']\\n\\n# Output the extracted text\\nprint(text)\\n' result='首页\\n融资快报\\n融资事件\\n项目库\\n机构库\\n项目集\\n定向对接\\n融通创新\\n公司/项目名/投资机构/赛道\\n\\xa0\\n返回36氪\\n登录\\n融资快报\\n快讯\\n“特斯联”完成20亿元D轮融资\\n36氪获悉，人工智能物联网（AIoT）企业“特斯联”近日完成D轮20亿元融资。本轮融资由AL Capital、阳明股权投资基金共同领投，福田资本、金地集团、重科控股、数字重庆、南昌政府平台公司、徐州产业基金、北科建集团、光大控股、商汤科技等新老股东跟投。本轮融资后，特斯联将进一步夯实数智化基础设施，深化“模型+系统”的比特大模型开放平台。原文链接\\n特斯联\\n战略融资\\n重庆市\\n2015年成立\\n城市级智能物联网服务提供商\\n5小时前\\n快讯\\n无人机研发及赛事运营商“美冠挚友”获天使+轮融资\\n36氪获悉，无人机研发及赛事运营商“美冠挚友（星奇世界HISINGY）”宣布完成天使+轮融资，本轮融资由中科优势旗下探方资本领投，老股东火凤资本、真成投资持续跟投，探究资本担任独家财务顾问。本轮融资资金将用于“低价格高品质”的全新娱乐穿越机套装产品的研发及投产、新消费场景的布局及各传统玩具线下渠道推进。原文链接\\n6小时前\\n文章\\n\\u200b36氪广东首发｜AI Infra供应商「星凡科技」获近亿元Pre-A轮融资，赋能算力中心建设运营\\n2023年公司全年预计实现过亿元营收。\\n星凡科技\\n天使轮\\n四川省\\n2021年成立\\n元宇宙基础数字平台\\n6小时前\\n许璧端@36氪广东\\n文章\\n「特斯联」完成20亿元D轮融资，加速模型+系统在AIoT领域落地｜硬氪独家\\n特斯联初探领域大模型应用，加速企业数智化转型。\\n特斯联\\n战略融资\\n重庆市\\n2015年成立\\n城市级智能物联网服务提供商\\n6小时前\\n黄 楠\\n文章\\n「美冠挚友（星奇世界HISINGY）」获天使+轮融资，持续推动娱乐无人机与低空经济发展\\n这是星奇世界自2022年完成天使轮和战略轮后的第三轮融资，团队正在拓展无人机与潮玩的边界。\\n星奇世界HISINGY\\n战略融资\\n北京市\\n2018年成立\\n潮玩品牌\\n6小时前\\n刘士武\\n文章\\n云澎科技完成数千万元A轮融资\\n由亚投资本投资。\\n21小时前\\n时氪分享\\n文章\\n世纪云安完成数亿元A+轮融资、华发集团与华为在全屋智能领域合作、长租公寓公司BLUEGROUND融资4500万｜ PropTech周刊\\n聚焦改变地产行业的科技及创新力量\\n21小时前\\n宋虹姗\\n文章\\n「德沪涂膜」完成数千万元A轮融资，专注钙钛矿精密溶液涂布解决方案 | 36氪首发\\n截至目前，德沪涂膜已经打入了协鑫光电等近20家钙钛矿、光伏企业的供应链。\\n德沪涂膜\\n未披露\\n上海市\\n2016年成立\\n高端装备制造企业\\n21小时前\\n王方玉\\n快讯\\n深圳跨境电商资讯平台“亿恩网”完成天使轮融资\\n36氪广东获悉，据“亿恩”微信公众号消息，近日，深圳亿恩网广告传媒有限公司（下沉“亿恩网”）完成天使轮融资，本次融资金额和投资方未披露。“亿恩网”成立于2016年，是一家跨境电商资讯平台，2023年以来，主要面向有全球化电商布局需求的国内企业，如消费品牌、科技品牌、时尚品牌、产业品牌等，提供品牌出海孵化服务。原文链接\\n22小时前\\n快讯\\nAI角色创作平台“捏Ta”完成超千万元融资\\n36氪获悉，近日AI角色创作平台“捏Ta”（公司名为“看见概念”）完成了天使轮和Pre-A轮融资，总额超千万元。投资方包括源码资本和奇绩创坛等，所得资金将主要用于模型的进一步优化、多模态功能的增强以及社交玩法的开发。原文链接\\n昨天\\n快讯\\n“天地和兴”获约8亿元融资\\n36氪获悉，“天地和兴”近日宣布完成E轮融资，融资资金约8亿元。本轮由北京国管控股下属京国瑞、联通中金、蜂云资本联合领投，北京信息产业发展投资基金、中关村科学城公司、中核新兴产业基金、网宿科技、松禾资本等新老股东跟投，深蓝资本为牵头财务顾问。本轮融资后国资占比进一步增加。原文链接\\n天地和兴\\nD轮\\n北京市\\n2007年成立\\n工控安全整体解决方案提供商\\n昨天\\n文章\\n照着剧本一键创作同人漫，AI角色创作平台「捏Ta」完成超千万元融资 | 36氪首发\\n在技术发展早期，产品需要考虑的是如何在有限的技术成熟度内，满足用户更完整、丰富的创作需求。\\n看见概念\\nPre-A轮\\n上海市\\n2022年成立\\nAI 驱动的次文化内容共创社区\\n昨天\\n周鑫雨\\n文章\\n工控安全厂商「天地和兴」获约8亿元融资，北京市国资、联通中金、蜂云资本领投｜硬氪首发\\n工业网络安全需求呈爆发性增长。\\n天地和兴\\nD轮\\n北京市\\n2007年成立\\n工控安全整体解决方案提供商\\n昨天\\n黄 楠\\n快讯\\n“中科新生命”完成数亿元C轮融资\\n36氪获悉，生物技术企业“中科新生命”完成数亿元C轮融资，由广发信德领投，盛石资本跟投，浩悦资本继续担任本轮融资的独家财务顾问。本轮融资主要用于开发基于多组学应用场景的新产品；继续拓展科技服务、生物医' is_success=True is_finished=False\n"
     ]
    }
   ],
   "source": [
    "from metagpt.schema import TaskResult\n",
    "\n",
    "plan.current_task.update_task_result(task_result=TaskResult(code=code, result=result, is_success=success))\n",
    "plan.finish_current_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e05a20",
   "metadata": {},
   "source": [
    "### llm extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5aa3a8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 15:43:27.916 | DEBUG    | __main__:execute_task:9 - As an AI Engineer, you need to help user to achieve their goal step by step in a continuous Jupyter notebook.\n",
      "\n",
      "## Current Task\n",
      "Extract the HTML structure and inner text content of the 'financing-flash' page\n",
      "\n",
      "## Task Guidance\n",
      "Write complete code for 'Current Task'. And avoid duplicating code from 'Finished Tasks', such as repeated import of packages, reading data, etc.\n",
      "Specifically, 仅专注于当前任务，上一步执行的变量可以直接使用\n",
      "\n",
      "\n",
      "\n",
      "# Tool Info\n",
      "\n",
      "## Capabilities\n",
      "- You can utilize pre-defined tools in any code lines from 'Available Tools' in the form of Python class or function.\n",
      "- You can freely combine the use of any other public packages, like sklearn, numpy, pandas, etc..\n",
      "\n",
      "## Available Tools:\n",
      "Each tool is described in JSON format. All tools was import by default.\n",
      "{\"web scraping\": {\"type\": \"async_function\", \"description\": \"Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \", \"signature\": \"(url)\", \"parameters\": \"Args: url (str): The main URL to fetch inner text from. Returns: dict: The inner text content and html structure of the web page, keys are 'inner_text', 'html'.\", \"import\": \"from metagpt.tools.libs.web_scraping import scrape_web_playwright\"}}\n",
      "{\"text extractor\": {\"type\": \"async_function\", \"description\": \"Perform extraction on the 'content' text using a large language model. \", \"signature\": \"(guidance: str, content: str, format: str) -> str\", \"parameters\": \"Args: guidance (str): Guide the extraction process. content (str): The text content that needs to be extracted. format (str): The output format, can be \\\"json\\\" or \\\"markdown\\\". Returns: str: text extracted from content.\", \"import\": \"from tools.text_extractor.llm_extractor import llm_extractor\"}}\n",
      "\n",
      "# Constraints\n",
      "- Take on Current Task if it is in Plan Status, otherwise, tackle User Requirement directly.\n",
      "- Ensure the output new code is executable in the same Jupyter notebook as the previous executed code.\n",
      "- Always prioritize using pre-defined tools for the same functionality.\n",
      "\n",
      "# Output\n",
      "While some concise thoughts are helpful, code is absolutely required. Always output one and only one code block in your response. Output code in the following format:\n",
      "```python\n",
      "your code\n",
      "```\n",
      "\n",
      "Remember!! Since it is a notebook environment, don't use asyncio.run. Instead, use await if you need to call an async function.\n",
      "2024-04-09 15:43:27.917 | DEBUG    | metagpt.provider.base_llm:aask:149 - [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'As an AI Engineer, you need to help user to achieve their goal step by step in a continuous Jupyter notebook.\\n\\n## Current Task\\nExtract the HTML structure and inner text content of the \\'financing-flash\\' page\\n\\n## Task Guidance\\nWrite complete code for \\'Current Task\\'. And avoid duplicating code from \\'Finished Tasks\\', such as repeated import of packages, reading data, etc.\\nSpecifically, 仅专注于当前任务，上一步执行的变量可以直接使用\\n\\n\\n\\n# Tool Info\\n\\n## Capabilities\\n- You can utilize pre-defined tools in any code lines from \\'Available Tools\\' in the form of Python class or function.\\n- You can freely combine the use of any other public packages, like sklearn, numpy, pandas, etc..\\n\\n## Available Tools:\\nEach tool is described in JSON format. All tools was import by default.\\n{\"web scraping\": {\"type\": \"async_function\", \"description\": \"Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \", \"signature\": \"(url)\", \"parameters\": \"Args: url (str): The main URL to fetch inner text from. Returns: dict: The inner text content and html structure of the web page, keys are \\'inner_text\\', \\'html\\'.\", \"import\": \"from metagpt.tools.libs.web_scraping import scrape_web_playwright\"}}\\n{\"text extractor\": {\"type\": \"async_function\", \"description\": \"Perform extraction on the \\'content\\' text using a large language model. \", \"signature\": \"(guidance: str, content: str, format: str) -> str\", \"parameters\": \"Args: guidance (str): Guide the extraction process. content (str): The text content that needs to be extracted. format (str): The output format, can be \\\\\"json\\\\\" or \\\\\"markdown\\\\\". Returns: str: text extracted from content.\", \"import\": \"from tools.text_extractor.llm_extractor import llm_extractor\"}}\\n\\n# Constraints\\n- Take on Current Task if it is in Plan Status, otherwise, tackle User Requirement directly.\\n- Ensure the output new code is executable in the same Jupyter notebook as the previous executed code.\\n- Always prioritize using pre-defined tools for the same functionality.\\n\\n# Output\\nWhile some concise thoughts are helpful, code is absolutely required. Always output one and only one code block in your response. Output code in the following format:\\n```python\\nyour code\\n```\\n\\nRemember!! Since it is a notebook environment, don\\'t use asyncio.run. Instead, use await if you need to call an async function.'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "from metagpt.tools.libs.web_scraping import scrape_web_playwright\n",
      "from tools.text_extractor.llm_extractor import llm_extractor\n",
      "\n",
      "# Scrape the HTML structure and inner text content of the 'financing-flash' page\n",
      "url = \"https://finance.sina.com.cn/flash/hkhc/2023-03-24/doc-imukvuxu9025020.d.html\"\n",
      "result = await scrape_web_playwright(url)\n",
      "\n",
      "# Extract the inner text content from the scraped HTML\n",
      "inner_text = result['inner_text']\n",
      "\n",
      "# Extract specific information from the inner text using the text extractor\n",
      "extraction_guidance = \"Extract the title, time, and content from the article.\"\n",
      "extracted_info = llm_extractor(extraction_guidance, inner_text, format=\"json\")\n",
      "\n",
      "# Print the extracted information"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 15:43:32.921 | INFO     | metagpt.utils.cost_manager:update_cost:108 - prompt_tokens: 567, completion_tokens: 219\n",
      "2024-04-09 15:43:32.927 | DEBUG    | __main__:execute_task:12 - ```python\n",
      "from metagpt.tools.libs.web_scraping import scrape_web_playwright\n",
      "from tools.text_extractor.llm_extractor import llm_extractor\n",
      "\n",
      "# Scrape the HTML structure and inner text content of the 'financing-flash' page\n",
      "url = \"https://finance.sina.com.cn/flash/hkhc/2023-03-24/doc-imukvuxu9025020.d.html\"\n",
      "result = await scrape_web_playwright(url)\n",
      "\n",
      "# Extract the inner text content from the scraped HTML\n",
      "inner_text = result['inner_text']\n",
      "\n",
      "# Extract specific information from the inner text using the text extractor\n",
      "extraction_guidance = \"Extract the title, time, and content from the article.\"\n",
      "extracted_info = llm_extractor(extraction_guidance, inner_text, format=\"json\")\n",
      "\n",
      "# Print the extracted information\n",
      "print(extracted_info)\n",
      "```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "print(extracted_info)\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 1 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">from</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">metagpt.tools.libs.web_scraping</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> scrape_web_playwright</span><span style=\"background-color: #2f1e2e\">                                             </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 2 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">from</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">tools.text_extractor.llm_extractor</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> llm_extractor</span><span style=\"background-color: #2f1e2e\">                                                  </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 3 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 4 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Scrape the HTML structure and inner text content of the 'financing-flash' page</span><span style=\"background-color: #2f1e2e\">                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 5 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">url </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">\"https://finance.sina.com.cn/flash/hkhc/2023-03-24/doc-imukvuxu9025020.d.html\"</span><span style=\"background-color: #2f1e2e\">                          </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 6 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">result </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">await</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> scrape_web_playwright(url)</span><span style=\"background-color: #2f1e2e\">                                                                     </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 7 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 8 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Extract the inner text content from the scraped HTML</span><span style=\"background-color: #2f1e2e\">                                                        </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 9 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">inner_text </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> result[</span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">'inner_text'</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">]</span><span style=\"background-color: #2f1e2e\">                                                                             </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">10 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">11 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Extract specific information from the inner text using the text extractor</span><span style=\"background-color: #2f1e2e\">                                   </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">12 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">extraction_guidance </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">\"Extract the title, time, and content from the article.\"</span><span style=\"background-color: #2f1e2e\">                                </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">13 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">extracted_info </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> llm_extractor(extraction_guidance, inner_text, format</span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">\"json\"</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">)</span><span style=\"background-color: #2f1e2e\">                                </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">14 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">15 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Print the extracted information</span><span style=\"background-color: #2f1e2e\">                                                                             </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">16 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">print(extracted_info)</span><span style=\"background-color: #2f1e2e\">                                                                                         </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">17 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 1 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mfrom\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mmetagpt\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mtools\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mlibs\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mweb_scraping\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mscrape_web_playwright\u001b[0m\u001b[48;2;47;30;46m                                             \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 2 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mfrom\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mtools\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mtext_extractor\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mllm_extractor\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mllm_extractor\u001b[0m\u001b[48;2;47;30;46m                                                  \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 3 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 4 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Scrape the HTML structure and inner text content of the 'financing-flash' page\u001b[0m\u001b[48;2;47;30;46m                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 5 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46mhttps://finance.sina.com.cn/flash/hkhc/2023-03-24/doc-imukvuxu9025020.d.html\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[48;2;47;30;46m                          \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 6 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mresult\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46mawait\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mscrape_web_playwright\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                     \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 7 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 8 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Extract the inner text content from the scraped HTML\u001b[0m\u001b[48;2;47;30;46m                                                        \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 9 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46minner_text\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mresult\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m[\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m'\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46minner_text\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m'\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m]\u001b[0m\u001b[48;2;47;30;46m                                                                             \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m10 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m11 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Extract specific information from the inner text using the text extractor\u001b[0m\u001b[48;2;47;30;46m                                   \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m12 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mextraction_guidance\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46mExtract the title, time, and content from the article.\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[48;2;47;30;46m                                \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m13 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mextracted_info\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mllm_extractor\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mextraction_guidance\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m,\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46minner_text\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m,\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mformat\u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46mjson\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m\"\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m14 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m15 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Print the extracted information\u001b[0m\u001b[48;2;47;30;46m                                                                             \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m16 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mprint\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mextracted_info\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                                         \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m17 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'from metagpt.tools.libs.web_scraping import scrape_web_playwright\\nfrom tools.text_extractor.llm_extractor import llm_extractor\\n\\n# Scrape the HTML structure and inner text content of the \\'financing-flash\\' page\\nurl = \"https://finance.sina.com.cn/flash/hkhc/2023-03-24/doc-imukvuxu9025020.d.html\"\\nresult = await scrape_web_playwright(url)\\n\\n# Extract the inner text content from the scraped HTML\\ninner_text = result[\\'inner_text\\']\\n\\n# Extract specific information from the inner text using the text extractor\\nextraction_guidance = \"Extract the title, time, and content from the article.\"\\nextracted_info = llm_extractor(extraction_guidance, inner_text, format=\"json\")\\n\\n# Print the extracted information\\nprint(extracted_info)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan_status = \"\"\"## Finished Tasks\n",
    "\"\"\"\n",
    "\n",
    "code = execute_task(plan, plan_status=plan_status, task_guidance=\"仅专注于当前任务，上一步执行的变量可以直接使用\")\n",
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6b5cca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7112cae0254b4c66b604e188f04b989a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "execute_code._display(result, language=\"markdown\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
