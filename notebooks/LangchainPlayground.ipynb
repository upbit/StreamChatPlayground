{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd2ba955-b229-4237-8a9c-ce319ccff44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 18:01:30.536 | INFO     | metagpt.const:get_metagpt_package_root:29 - Package root set to /root/workspace/StreamChatPlayground/notebooks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLMConfig(extra_fields=None, api_key='2089197edd83eddf547f600665da02e9.ohYNy9csNMqduxID', api_type=<LLMType.ZHIPUAI: 'zhipuai'>, base_url='https://api.openai.com/v1', api_version=None, model='glm-4', pricing_plan=None, access_key=None, secret_key=None, endpoint=None, app_id=None, api_secret=None, domain=None, max_token=4096, temperature=0.0, top_p=1.0, top_k=0, repetition_penalty=1.0, stop=None, presence_penalty=0.0, frequency_penalty=0.0, best_of=None, n=None, stream=False, logprobs=None, top_logprobs=None, timeout=60, proxy=None, calc_usage=True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# make asyncio.run() works in notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from metagpt.config2 import Config\n",
    "\n",
    "cfg = Config.default()\n",
    "cfg.llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b291e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 18:02:59.269 | DEBUG    | metagpt.provider.base_llm:aask:126 - [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '你好'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course, I'm here to help. How can I assist you today?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Of course, I'm here to help. How can I assist you today?\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from metagpt.logs import logger\n",
    "from metagpt.provider.zhipuai_api import ZhiPuAILLM\n",
    "\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"DEBUG\")\n",
    "llm = ZhiPuAILLM(cfg.llm)\n",
    "\n",
    "asyncio.run(llm.aask(\"你好\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa32b8",
   "metadata": {},
   "source": [
    "## 任务拆解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bdc99519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def load_plaintext(filename):\n",
    "    with open(filename, 'r', encoding=\"UTF-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "path = \"prompts/planning\"\n",
    "\n",
    "task_types = \"\"\"**web scraping**: For scraping data from web pages.\"\"\"\n",
    "\n",
    "output_format = load_plaintext(os.path.join(path, \"output.md\"))\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[],\n",
    "    template=load_plaintext(os.path.join(path, \"template.yaml\")),\n",
    ")\n",
    "prompt = prompt.partial(\n",
    "    task_types=task_types,\n",
    "    max_tasks=20,\n",
    "    output=output_format,\n",
    ")\n",
    "# print(prompt.format(context=\"这里是指令\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4636e9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 20:00:17.622 | DEBUG    | metagpt.provider.base_llm:aask:126 - [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'You are a helpful AI assistant.\\n\\n# User goal\\n抓取 https://pitchhub.36kr.com/financing-flash 中\\'快讯\\'的内容，并整理成markdown存档。\\n\\n大概的流程：\\n- 使用工具抓取网页中的可见文本\\n- 提取网页文本中\\'快讯\\'相关的内容。注意网页中可能包含导航，只需要抽取\\'快讯\\'的具体内容\\n- 对抽取结果进行归类，并保存成markdown表格: 快讯.md\\n\\n# Available Task Types:\\n**web scraping**: For scraping data from web pages.\\n\\n# Task:\\nBased on the user goal, write a plan or modify an existing plan of what you should do to achieve the goal. A plan consists of one to 20 tasks.\\nIf you are modifying an existing plan, carefully follow the instruction, don\\'t make unnecessary changes. Give the whole plan unless instructed to modify only one task of the plan.\\nIf you encounter errors on the current task, revise and output the current single task only.\\nOutput a list of jsons following the format:\\n```json\\n[\\n    {{\\n        \"task_id\": str = \"unique identifier for a task in plan, can be an ordinal\",\\n        \"dependent_task_ids\": list[str] = \"ids of tasks prerequisite to this task\",\\n        \"instruction\": \"what you should do in this task, one short phrase or sentence\",\\n        \"task_type\": \"type of this task, should be one of Available Task Types\",\\n    }},\\n    ...\\n]\\n```'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"task_id\": \"1\",\n",
      "        \"dependent_task_ids\": [],\n",
      "        \"instruction\": \"Use a web scraping tool to fetch the visible text from the given URL.\",\n",
      "        \"task_type\": \"web scraping\"\n",
      "    },\n",
      "    {\n",
      "        \"task_id\": \"2\",\n",
      "        \"dependent_task_ids\": [\"1\"],\n",
      "        \"instruction\": \"Extract the content related to '快讯' from the fetched text, excluding navigation elements.\",\n",
      "        \"task_type\": \"web scraping\"\n",
      "    },\n",
      "    {\n",
      "        \"task_id\": \"3\",\n",
      "        \"dependent_task_ids\": [\"2\"],\n",
      "        \"instruction\": \"Categorize the extracted content and format it into a markdown table.\",\n",
      "        \"task_type\": \"data processing\"\n",
      "    },\n",
      "    {\n",
      "        \"task_id\": \"4\",\n",
      "        \"dependent_task_ids\": [\"3\"],\n",
      "        \"instruction\": \"Save the formatted markdown table into a file named '快讯.md'.\",\n",
      "        \"task_type\": \"file operation\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "goal = \"\"\"抓取 https://pitchhub.36kr.com/financing-flash 中'快讯'的内容，并整理成markdown存档。\n",
    "\n",
    "大概的流程：\n",
    "- 使用工具抓取网页中的可见文本\n",
    "- 提取网页文本中'快讯'相关的内容。注意网页中可能包含导航，只需要抽取'快讯'的具体内容\n",
    "- 对抽取结果进行归类，并保存成markdown表格: 快讯.md\"\"\"\n",
    "\n",
    "planning_prompt = prompt.format(goal=goal)\n",
    "# print(planning_prompt + \"\\n\\n\")\n",
    "\n",
    "rsp = asyncio.run(llm.aask(msg=planning_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "872a3dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'task_id': '1',\n",
       "  'dependent_task_ids': [],\n",
       "  'instruction': 'Use a web scraping tool to fetch the visible text from the given URL.',\n",
       "  'task_type': 'web scraping'},\n",
       " {'task_id': '2',\n",
       "  'dependent_task_ids': ['1'],\n",
       "  'instruction': \"Extract the content related to '快讯' from the fetched text, excluding navigation elements.\",\n",
       "  'task_type': 'web scraping'},\n",
       " {'task_id': '3',\n",
       "  'dependent_task_ids': ['2'],\n",
       "  'instruction': 'Categorize the extracted content and format it into a markdown table.',\n",
       "  'task_type': 'data processing'},\n",
       " {'task_id': '4',\n",
       "  'dependent_task_ids': ['3'],\n",
       "  'instruction': \"Save the formatted markdown table into a file named '快讯.md'.\",\n",
       "  'task_type': 'file operation'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from metagpt.utils.common import OutputParser\n",
    "\n",
    "try:\n",
    "    tasks = json.loads(rsp)\n",
    "except:\n",
    "    code_block = OutputParser.parse_code(rsp, \"json\")\n",
    "    tasks = json.loads(code_block)\n",
    "\n",
    "tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f8e4f",
   "metadata": {},
   "source": [
    "## 任务执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "93db0356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI Engineer, you need to help user to achieve their goal step by step in a continuous Jupyter notebook. Since it is a notebook environment, don't use asyncio.run. Instead, use await if you need to call an async function.\n",
      "\n",
      "# User goal\n",
      "这里是指令\n",
      "\n",
      "\n",
      "\n",
      "## Current Task\n",
      "Use a web scraping tool to fetch the visible text from the given URL.\n",
      "\n",
      "## Task Guidance\n",
      "Write complete code for 'Current Task'. And avoid duplicating code from 'Finished Tasks', such as repeated import of packages, reading data, etc.\n",
      "Specifically, \n",
      "\n",
      "# Tool Info\n",
      "\n",
      "## Capabilities\n",
      "- You can utilize pre-defined tools in any code lines from 'Available Tools' in the form of Python class or function.\n",
      "- You can freely combine the use of any other public packages, like sklearn, numpy, pandas, etc..\n",
      "\n",
      "## Available Tools:\n",
      "Each tool is described in JSON format. When you call a tool, import the tool from its path first.\n",
      "{'scrape_web_playwright': {'type': 'async_function', 'description': 'Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. ', 'signature': '(url)', 'parameters': \"Args: url (str): The main URL to fetch inner text from. Returns: dict: The inner text content and html structure of the web page, keys are 'inner_text', 'html'.\", 'tool_path': 'metagpt/tools/libs/web_scraping.py'}}\n",
      "\n",
      "# Constraints\n",
      "- Take on Current Task if it is in Plan Status, otherwise, tackle User Requirement directly.\n",
      "- Ensure the output new code is executable in the same Jupyter notebook as the previous executed code.\n",
      "- Always prioritize using pre-defined tools for the same functionality.\n",
      "\n",
      "# Output\n",
      "While some concise thoughts are helpful, code is absolutely required. Always output one and only one code block in your response. Output code in the following format:\n",
      "```python\n",
      "your code\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "path = \"prompts/task_plan\"\n",
    "\n",
    "task_types = \"\"\"**web scraping**: For scraping data from web pages.\"\"\"\n",
    "\n",
    "output_format = load_plaintext(os.path.join(path, \"output.md\"))\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[],\n",
    "    template=load_plaintext(os.path.join(path, \"template.yaml\")),\n",
    ")\n",
    "prompt = prompt.partial(\n",
    "    plan_status=\"\",\n",
    "    current_task=tasks[0]['instruction'],\n",
    "    task_guidance=\"\",\n",
    "    tools=\"\"\"{'scrape_web_playwright': {'type': 'async_function', 'description': 'Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. ', 'signature': '(url)', 'parameters': \"Args: url (str): The main URL to fetch inner text from. Returns: dict: The inner text content and html structure of the web page, keys are 'inner_text', 'html'.\", 'tool_path': 'metagpt/tools/libs/web_scraping.py'}}\"\"\",\n",
    "    output=output_format,\n",
    ")\n",
    "# print(prompt.format(goal=\"这里是指令\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d3debb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 20:26:32.943 | DEBUG    | metagpt.provider.base_llm:aask:126 - [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'As an AI Engineer, you need to help user to achieve their goal step by step in a continuous Jupyter notebook. Since it is a notebook environment, don\\'t use asyncio.run. Instead, use await if you need to call an async function.\\n\\n# User goal\\n抓取 https://pitchhub.36kr.com/financing-flash 中\\'快讯\\'的内容，并整理成markdown存档。\\n\\n大概的流程：\\n- 使用工具抓取网页中的可见文本\\n- 提取网页文本中\\'快讯\\'相关的内容。注意网页中可能包含导航，只需要抽取\\'快讯\\'的具体内容\\n- 对抽取结果进行归类，并保存成markdown表格: 快讯.md\\n\\n\\n\\n## Current Task\\nUse a web scraping tool to fetch the visible text from the given URL.\\n\\n## Task Guidance\\nWrite complete code for \\'Current Task\\'. And avoid duplicating code from \\'Finished Tasks\\', such as repeated import of packages, reading data, etc.\\nSpecifically, \\n\\n# Tool Info\\n\\n## Capabilities\\n- You can utilize pre-defined tools in any code lines from \\'Available Tools\\' in the form of Python class or function.\\n- You can freely combine the use of any other public packages, like sklearn, numpy, pandas, etc..\\n\\n## Available Tools:\\nEach tool is described in JSON format. When you call a tool, import the tool from its path first.\\n{\\'scrape_web_playwright\\': {\\'type\\': \\'async_function\\', \\'description\\': \\'Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \\', \\'signature\\': \\'(url)\\', \\'parameters\\': \"Args: url (str): The main URL to fetch inner text from. Returns: dict: The inner text content and html structure of the web page, keys are \\'inner_text\\', \\'html\\'.\", \\'tool_path\\': \\'metagpt/tools/libs/web_scraping.py\\'}}\\n\\n# Constraints\\n- Take on Current Task if it is in Plan Status, otherwise, tackle User Requirement directly.\\n- Ensure the output new code is executable in the same Jupyter notebook as the previous executed code.\\n- Always prioritize using pre-defined tools for the same functionality.\\n\\n# Output\\nWhile some concise thoughts are helpful, code is absolutely required. Always output one and only one code block in your response. Output code in the following format:\\n```python\\nyour code\\n```'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI Engineer, you need to help user to achieve their goal step by step in a continuous Jupyter notebook. Since it is a notebook environment, don't use asyncio.run. Instead, use await if you need to call an async function.\n",
      "\n",
      "# User goal\n",
      "抓取 https://pitchhub.36kr.com/financing-flash 中'快讯'的内容，并整理成markdown存档。\n",
      "\n",
      "大概的流程：\n",
      "- 使用工具抓取网页中的可见文本\n",
      "- 提取网页文本中'快讯'相关的内容。注意网页中可能包含导航，只需要抽取'快讯'的具体内容\n",
      "- 对抽取结果进行归类，并保存成markdown表格: 快讯.md\n",
      "\n",
      "\n",
      "\n",
      "## Current Task\n",
      "Use a web scraping tool to fetch the visible text from the given URL.\n",
      "\n",
      "## Task Guidance\n",
      "Write complete code for 'Current Task'. And avoid duplicating code from 'Finished Tasks', such as repeated import of packages, reading data, etc.\n",
      "Specifically, \n",
      "\n",
      "# Tool Info\n",
      "\n",
      "## Capabilities\n",
      "- You can utilize pre-defined tools in any code lines from 'Available Tools' in the form of Python class or function.\n",
      "- You can freely combine the use of any other public packages, like sklearn, numpy, pandas, etc..\n",
      "\n",
      "## Available Tools:\n",
      "Each tool is described in JSON format. When you call a tool, import the tool from its path first.\n",
      "{'scrape_web_playwright': {'type': 'async_function', 'description': 'Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. ', 'signature': '(url)', 'parameters': \"Args: url (str): The main URL to fetch inner text from. Returns: dict: The inner text content and html structure of the web page, keys are 'inner_text', 'html'.\", 'tool_path': 'metagpt/tools/libs/web_scraping.py'}}\n",
      "\n",
      "# Constraints\n",
      "- Take on Current Task if it is in Plan Status, otherwise, tackle User Requirement directly.\n",
      "- Ensure the output new code is executable in the same Jupyter notebook as the previous executed code.\n",
      "- Always prioritize using pre-defined tools for the same functionality.\n",
      "\n",
      "# Output\n",
      "While some concise thoughts are helpful, code is absolutely required. Always output one and only one code block in your response. Output code in the following format:\n",
      "```python\n",
      "your code\n",
      "```\n",
      "\n",
      "\n",
      "To achieve the user's goal of scraping the '快讯' content from the provided URL and saving it in a Markdown file, we will follow the steps outlined in the 'Current Task'. We will use the `scrape_web_playwright` function from the available tools to fetch the visible text from the web page. Here is the code to do that:\n",
      "\n",
      "```python\n",
      "# Import the tool for web scraping\n",
      "from metagpt.tools.libs.web_scraping import scrape_web_playwright\n",
      "\n",
      "# Define the URL to be scraped\n",
      "url = 'https://pitchhub.36kr.com/financing-flash'\n",
      "\n",
      "# Use the tool to scrape the web page for its inner text\n",
      "async def fetch_pageInnerText(url):\n",
      "    return await scrape_web_playwright(url)\n",
      "\n",
      "# Execute the async function in a Jupyter notebook\n",
      "import nest_asyncio\n",
      "nest_asyncio.apply()\n",
      "\n",
      "# This will store the result of the scraping\n",
      "result = await fetch_pageInnerText(url)\n",
      "\n",
      "# Extract the inner text from the result\n",
      "pageInnerText = result['inner_text']\n",
      "\n",
      "# Now, further processing will be needed to filter out the '快讯' content.\n",
      "# However, since the current task is only to fetch the inner text, \n",
      "# we will stop here and the next steps will be handled in the subsequent tasks.\n",
      "```\n",
      "\n",
      "Please note that the above code only fetches the inner text of the web page. The next steps would involve parsing this text to extract the '快讯' content and organizing it into a Markdown table, which would be done in subsequent tasks according to the plan.\n"
     ]
    }
   ],
   "source": [
    "task_prompt = prompt.format(goal=goal)\n",
    "print(task_prompt + \"\\n\\n\")\n",
    "\n",
    "rsp = asyncio.run(llm.aask(msg=task_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c7bc3a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Import the tool for web scraping\n",
      "from metagpt.tools.libs.web_scraping import scrape_web_playwright\n",
      "\n",
      "# Define the URL to be scraped\n",
      "url = 'https://pitchhub.36kr.com/financing-flash'\n",
      "\n",
      "# Use the tool to scrape the web page for its inner text\n",
      "async def fetch_pageInnerText(url):\n",
      "    return await scrape_web_playwright(url)\n",
      "\n",
      "# Execute the async function in a Jupyter notebook\n",
      "import nest_asyncio\n",
      "nest_asyncio.apply()\n",
      "\n",
      "# This will store the result of the scraping\n",
      "result = await fetch_pageInnerText(url)\n",
      "\n",
      "# Extract the inner text from the result\n",
      "pageInnerText = result['inner_text']\n",
      "\n",
      "# Now, further processing will be needed to filter out the '快讯' content.\n",
      "# However, since the current task is only to fetch the inner text, \n",
      "# we will stop here and the next steps will be handled in the subsequent tasks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "code = OutputParser.parse_code(rsp, \"python\")\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "57962d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 1 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Import the tool for web scraping</span><span style=\"background-color: #2f1e2e\">                                                                            </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 2 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">from</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">metagpt.tools.libs.web_scraping</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> scrape_web_playwright</span><span style=\"background-color: #2f1e2e\">                                             </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 3 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 4 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Define the URL to be scraped</span><span style=\"background-color: #2f1e2e\">                                                                                </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 5 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">url </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">'https://pitchhub.36kr.com/financing-flash'</span><span style=\"background-color: #2f1e2e\">                                                             </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 6 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 7 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Use the tool to scrape the web page for its inner text</span><span style=\"background-color: #2f1e2e\">                                                      </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 8 </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">async</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">def</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #06b6ef; text-decoration-color: #06b6ef; background-color: #2f1e2e\">fetch_pageInnerText</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">(url):</span><span style=\"background-color: #2f1e2e\">                                                                           </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\"> 9 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">    </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">return</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">await</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> scrape_web_playwright(url)</span><span style=\"background-color: #2f1e2e\">                                                                   </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">10 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">11 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Execute the async function in a Jupyter notebook</span><span style=\"background-color: #2f1e2e\">                                                            </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">12 </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">import</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #fec418; text-decoration-color: #fec418; background-color: #2f1e2e\">nest_asyncio</span><span style=\"background-color: #2f1e2e\">                                                                                           </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">13 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">nest_asyncio</span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">.</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">apply()</span><span style=\"background-color: #2f1e2e\">                                                                                          </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">14 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">15 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># This will store the result of the scraping</span><span style=\"background-color: #2f1e2e\">                                                                  </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">16 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">result </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> </span><span style=\"color: #815ba4; text-decoration-color: #815ba4; background-color: #2f1e2e\">await</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> fetch_pageInnerText(url)</span><span style=\"background-color: #2f1e2e\">                                                                       </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">17 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">18 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Extract the inner text from the result</span><span style=\"background-color: #2f1e2e\">                                                                      </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">19 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">pageInnerText </span><span style=\"color: #5bc4bf; text-decoration-color: #5bc4bf; background-color: #2f1e2e\">=</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\"> result[</span><span style=\"color: #48b685; text-decoration-color: #48b685; background-color: #2f1e2e\">'inner_text'</span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">]</span><span style=\"background-color: #2f1e2e\">                                                                          </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">20 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">21 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># Now, further processing will be needed to filter out the '快讯' content.</span><span style=\"background-color: #2f1e2e\">                                    </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">22 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># However, since the current task is only to fetch the inner text, </span><span style=\"background-color: #2f1e2e\">                                           </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">23 </span><span style=\"color: #776e71; text-decoration-color: #776e71; background-color: #2f1e2e\"># we will stop here and the next steps will be handled in the subsequent tasks.</span><span style=\"background-color: #2f1e2e\">                               </span>\n",
       "<span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">24 </span><span style=\"background-color: #2f1e2e\">                                                                                                              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 1 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Import the tool for web scraping\u001b[0m\u001b[48;2;47;30;46m                                                                            \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 2 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mfrom\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mmetagpt\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mtools\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mlibs\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46m.\u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mweb_scraping\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mscrape_web_playwright\u001b[0m\u001b[48;2;47;30;46m                                             \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 3 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 4 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Define the URL to be scraped\u001b[0m\u001b[48;2;47;30;46m                                                                                \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 5 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m'\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46mhttps://pitchhub.36kr.com/financing-flash\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m'\u001b[0m\u001b[48;2;47;30;46m                                                             \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 6 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 7 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Use the tool to scrape the web page for its inner text\u001b[0m\u001b[48;2;47;30;46m                                                      \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 8 \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46masync\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46mdef\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;6;182;239;48;2;47;30;46mfetch_pageInnerText\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m:\u001b[0m\u001b[48;2;47;30;46m                                                                           \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m 9 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m    \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46mreturn\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46mawait\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mscrape_web_playwright\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                   \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m10 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m11 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Execute the async function in a Jupyter notebook\u001b[0m\u001b[48;2;47;30;46m                                                            \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m12 \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46mimport\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;254;196;24;48;2;47;30;46mnest_asyncio\u001b[0m\u001b[48;2;47;30;46m                                                                                           \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m13 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mnest_asyncio\u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m.\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mapply\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                                          \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m14 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m15 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# This will store the result of the scraping\u001b[0m\u001b[48;2;47;30;46m                                                                  \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m16 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mresult\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;129;91;164;48;2;47;30;46mawait\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mfetch_pageInnerText\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46murl\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                       \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m17 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m18 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Extract the inner text from the result\u001b[0m\u001b[48;2;47;30;46m                                                                      \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m19 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mpageInnerText\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;91;196;191;48;2;47;30;46m=\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mresult\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m[\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m'\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46minner_text\u001b[0m\u001b[38;2;72;182;133;48;2;47;30;46m'\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m]\u001b[0m\u001b[48;2;47;30;46m                                                                          \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m20 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m21 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# Now, further processing will be needed to filter out the '快讯' content.\u001b[0m\u001b[48;2;47;30;46m                                    \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m22 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# However, since the current task is only to fetch the inner text, \u001b[0m\u001b[48;2;47;30;46m                                           \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m23 \u001b[0m\u001b[38;2;119;110;113;48;2;47;30;46m# we will stop here and the next steps will be handled in the subsequent tasks.\u001b[0m\u001b[48;2;47;30;46m                               \u001b[0m\n",
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m24 \u001b[0m\u001b[48;2;47;30;46m                                                                                                              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from metagpt.actions.di.execute_nb_code import ExecuteNbCode\n",
    "\n",
    "execute_code = ExecuteNbCode()\n",
    "\n",
    "result, success = await execute_code.run(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "532ce3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4d4c9; text-decoration-color: #d4d4c9; background-color: #2f1e2e; font-weight: bold\">  </span><span style=\"color: #665a61; text-decoration-color: #665a61; background-color: #2f1e2e\">1 </span><span style=\"color: #e7e9db; text-decoration-color: #e7e9db; background-color: #2f1e2e\">print(pageInnerText)</span><span style=\"background-color: #2f1e2e\">                                                                                           </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;212;201;48;2;47;30;46m  \u001b[0m\u001b[38;2;102;90;97;48;2;47;30;46m1 \u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mprint\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m(\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46mpageInnerText\u001b[0m\u001b[38;2;231;233;219;48;2;47;30;46m)\u001b[0m\u001b[48;2;47;30;46m                                                                                           \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "首页\n",
      "融资快报\n",
      "融资事件\n",
      "项目库\n",
      "机构库\n",
      "项目集\n",
      "定向对接\n",
      "融通创新\n",
      "公司/项目名/投资机构/赛道\n",
      " \n",
      "返回36氪\n",
      "登录\n",
      "融资快报\n",
      "文章\n",
      "Alithea Genomics获320万美元种子轮融资，提供RNA测序服务 | 海外New Things\n",
      "该公司研发的产品适用于大型转录学研究，如生物标志物发现、抗体开发和药物筛查。\n",
      "12小时前\n",
      "海若镜\n",
      "文章\n",
      "Tubulis获1.388亿美元B轮融资，实体瘤ADC管线再得资金加持 | 海外New Things\n",
      "Tubulis将新型专有技术与对特定疾病的生物洞察力相结合，生产出独特匹配的蛋白质药物偶联物。\n",
      "12小时前\n",
      "海若镜\n",
      "文章\n",
      "Asgard Therapeutics获3000万欧元融资，强生、诺和控股等支持其基因治疗 | 海外New Things\n",
      "支持其主打项目AT-108在2026年前达到支持递交IND申请的状态。\n",
      "12小时前\n",
      "海若镜\n",
      "文章\n",
      "「锐正基因」专注非病毒载体体内基因编辑药物开发；先博生物完成2亿元A+轮融资\n",
      "36氪医疗健康行业周报\n",
      "锐正基因\n",
      "Pre-A轮\n",
      "江苏省\n",
      "2021年成立\n",
      "端到端体内基因编辑技术与药物开发与生产商\n",
      "12小时前\n",
      "胡安2022\n",
      "文章\n",
      "Zephyr AI完成1.11亿美元A轮融资，AI+精准医疗技术获礼来支持 | 海外 New Things\n",
      "用AI加速临床试验的速度和成功率。\n",
      "昨天\n",
      "海若镜\n",
      "文章\n",
      "X-Therma获2240万美元B轮融资，研发细胞及器官低温保存技术 | 海外New Things\n",
      "X-Therma通过融合生物保存平台开创突破性冷链技术，推动再生医学发展，为有需要的患者提供安全、按需的器官、工程组织、细胞和基因疗法、疫苗等。\n",
      "2024-03-15\n",
      "海若镜\n",
      "快讯\n",
      "“卓宇智能”获近亿元A轮融资\n",
      "36氪获悉，“卓宇智能”近日完成近亿元A轮融资，由金雨茂物领投，常州拓扑投资跟投。本轮融资将用于渠道和交付能力的扩建。原文链接\n",
      "2024-03-15\n",
      "文章\n",
      "专注军民融合智能光电产品，「卓宇智能」获近亿元A轮融资｜硬氪首发\n",
      "从底层基础算法做起，逐步向产业纵深探索。\n",
      "2024-03-15\n",
      "黄 楠\n",
      "文章\n",
      "上海百奥恒完成亿元A+轮融资、黑石加码数据中心资产、深圳加大新型基础设施建设投入| PropTech周刊97期\n",
      "聚焦改变地产行业的科技及创新力量\n",
      "2024-03-15\n",
      "宋虹姗\n",
      "文章\n",
      "成立机器人事业部，「本末科技」获亿元级Pre-B轮融资\n",
      "具身智能是标准的新质生产力。\n",
      "本末科技\n",
      "A+轮\n",
      "广东省\n",
      "2020年成立\n",
      "智能机器人技术\n",
      "2024-03-15\n",
      "张卓倩\n",
      "快讯\n",
      "“果壳心理”获千万级A轮融资\n",
      "36氪获悉，近日，果壳（郑州）教育科技有限公司宣布完成千万级A轮融资，投资方为鱼和海科技。据介绍，本轮融资资金将主要用于支持心理学与AI的交叉研究及青少年心理指导体系研发、申请精神科医疗许可证以及1000平米场地建设。果壳心理成立于2011年，是专注于青少年心理研究的咨询机构。\n",
      "2024-03-14\n",
      "快讯\n",
      "「森泰英格」完成新一轮融资\n",
      "3月12日，森泰英格（成都）数控刀具股份有限公司（以下简称为“森泰英格”）完成了新一轮融资，投资方为力中投资。「森泰英格」是一家数控刀具及精密夹具研发商，专注于高端数控刀具和精密夹具的研发、生产和销售，产品广泛应用于汽车、航空航天、工程机械、能源装备、通用机械、模具制造、轨道交通、石油化工、3C等领域。（猎云网）原文链接\n",
      "森泰英格\n",
      "定向增发\n",
      "四川省\n",
      "1997年成立\n",
      "机床用切削刀具系统\n",
      "2024-03-14\n",
      "快讯\n",
      "篆芯半导体完成2亿元A2轮融资\n",
      "36氪获悉，篆芯半导体南京有限公司（以下简称“篆芯”）日前完成2亿元A2轮融资，此次融资由隆湫资本领投，睿悦投资、柠盟投资、君盛资本、卓源亚洲、华方资本等多家新老股东跟投。这也是继去年8月“篆芯”完成近3亿元A1轮融资后，半年后再次获得资本市场认同。本轮融资资金将用于技术研发和产品升级。\n",
      "篆芯半导体\n",
      "A轮\n",
      "江苏省\n",
      "2021年成立\n",
      "半导体设备制造商\n",
      "2024-03-14\n",
      "文章\n",
      "获千万级天使轮融资，华为系团队以AI大模型改造用工模式｜硬氪首发\n",
      "与比亚迪、bilibili等企业客户合作，产品也进入深圳部分政府单位。\n",
      "2024-03-14\n",
      "胡依婷\n",
      "文章\n",
      "半年后再获2亿元A2轮融资，「篆芯半导体」瞄准AI计算网络市场｜硬氪首发\n",
      "聚焦高端网络芯片及解决方案的自主研制。\n",
      "篆芯半导体\n",
      "A轮\n",
      "江苏省\n",
      "2021年成立\n",
      "半导体设备制造商\n",
      "2024-03-14\n",
      "黄 楠\n",
      "快讯\n",
      "“沐创”完成数亿元A3轮融资\n",
      "36氪获悉，近期，无锡沐创集成电路设计有限公司（以下简称沐创）完成数亿元A3轮融资，本轮融资由蚂蚁集团领投，新尚资本、一元航天、龙鼎投资、毅岭资本、光远投资跟投。本轮融资资金将用于业务爆发阶段现金流补充、产能扩大，以及新一代智能网络控制器\n"
     ]
    }
   ],
   "source": [
    "result, success = await execute_code.run(\"print(pageInnerText)\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
